{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711c22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "  \"probable_spam\",\n",
    "  \"medium_no_hashtags\",\n",
    "  \"long_with_hashtags\",\n",
    "  \"medium_with_hashtags\",\n",
    "  \"short_no_hashtag\",\n",
    "  \"short_with_hashtag\",\n",
    "  \"shotgun\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853d7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled tweets saved to out/feature-only-labelstudio-prepped.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Parameters\n",
    "INPUT_FILE = 'out/feature-only-cleaned-with-labels.json'\n",
    "OUTPUT_FILE = 'out/feature-only-labelstudio-prepped.json'\n",
    "SAMPLES_PER_BUCKET = 100  # Change this as needed\n",
    "\n",
    "# Load data\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    tweets = json.load(f)\n",
    "\n",
    "# Group tweets by bucket\n",
    "buckets = defaultdict(list)\n",
    "for tweet in tweets:\n",
    "  bucket_label = labels[tweet[\"metadata\"][\"bucket_label\"]]\n",
    "  tweet[\"bucket_label\"] = bucket_label\n",
    "  buckets[bucket_label].append(tweet)\n",
    "\n",
    "# Sample tweets\n",
    "sampled_tweets = []\n",
    "for bucket_label, tweets_in_bucket in buckets.items():\n",
    "  if len(tweets_in_bucket) < SAMPLES_PER_BUCKET:\n",
    "    print(f\"Warning: Bucket '{bucket_label}' has only {len(tweets_in_bucket)} tweets. Sampling all.\")\n",
    "    sampled = tweets_in_bucket\n",
    "  else:\n",
    "    sampled = random.sample(tweets_in_bucket, SAMPLES_PER_BUCKET)\n",
    "  sampled_tweets.extend(sampled)\n",
    "\n",
    "# Save to output JSON\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "  json.dump(sampled_tweets, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Sampled tweets saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757abc7",
   "metadata": {},
   "source": [
    "# Start parsing label studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6f2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/feature-only-relevancy-labeled.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "  documents = json.load(file)\n",
    "\n",
    "rawdata = [doc[\"data\"] for doc in documents]\n",
    "annotations = [doc[\"annotations\"][0] for doc in documents]\n",
    "\n",
    "for data, annotation in zip(rawdata, annotations):\n",
    "  data[\"relevancy_label\"] = annotation[\"result\"][0][\"value\"][\"choices\"][0]\n",
    "with open(\"out/feature-only-relevancy-labeled-cleaned.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(rawdata, file, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3953ee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 700 examples [00:00, 43416.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "ds = load_dataset(\"json\", data_files=\"out/feature-only-relevancy-labeled-cleaned.json\")[\"train\"]\n",
    "split_ds = ds.train_test_split(test_size=0.2)\n",
    "train_dataset = split_ds[\"train\"]\n",
    "test_dataset = split_ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87588a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cb569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_function(examples):\n",
    "  return tokenizer(\n",
    "    examples[\"content\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ad8082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 560/560 [00:00<00:00, 13191.93 examples/s]\n",
      "Map: 100%|██████████| 140/140 [00:00<00:00, 15411.74 examples/s]\n",
      "Map: 100%|██████████| 560/560 [00:00<00:00, 11164.93 examples/s]\n",
      "Map: 100%|██████████| 140/140 [00:00<00:00, 10963.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenizer_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "label2id = {\"irrelevant\": 0, \"relevant\": 1}  # adjust to your label names\n",
    "\n",
    "def convert_labels(example):\n",
    "  example[\"relevancy_label\"] = label2id[example[\"relevancy_label\"]]\n",
    "  return example\n",
    "\n",
    "train_dataset = train_dataset.map(convert_labels)\n",
    "test_dataset = test_dataset.map(convert_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc79277",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.rename_column(\"relevancy_label\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"relevancy_label\", \"labels\")\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"content\", \"text\")\n",
    "test_dataset = test_dataset.rename_column(\"content\", \"text\")\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d34feb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec74b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 32\n",
    "epochs=3\n",
    "train_data_size = len(train_dataset)\n",
    "steps_per_epoch = train_data_size // batch_size\n",
    "num_train_steps = steps_per_epoch * epochs \n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "  init_lr=2e-5,\n",
    "  num_train_steps= num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  weight_decay_rate=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef995197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./results\",\n",
    "  num_train_epochs=3,\n",
    "  per_device_train_batch_size=16,\n",
    "  save_strategy=\"epoch\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=100,\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"accuracy\",\n",
    "  eval_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "818118c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c36159c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, default_data_collator\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args = training_args,\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset=test_dataset,\n",
    "  compute_metrics=compute_metrics,\n",
    "  data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d64e286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = trainer.get_train_dataloader()\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad8bbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 02:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.338198</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.435860</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.383238</td>\n",
       "      <td>0.921429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=105, training_loss=0.07939920453798203, metrics={'train_runtime': 141.433, 'train_samples_per_second': 11.878, 'train_steps_per_second': 0.742, 'total_flos': 221013286502400.0, 'train_loss': 0.07939920453798203, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
