{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e6a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "OUT_DIR =\"OUT/\"\n",
    "ASSET_DIR=\"assets/\"\n",
    "with open(ASSET_DIR + \"dump-formatted.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "  RAW_DOCUMENTS = json.load(file)\n",
    "DOCUMENT_DF = pd.DataFrame.from_records(RAW_DOCUMENTS)\n",
    "DATA_LEN = len(DOCUMENT_DF)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd9c36",
   "metadata": {},
   "source": [
    "### Text Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f687730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaconv\n",
    "import re\n",
    "def cleantext(text):\n",
    "  text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "  text = text.replace('\\\\n', \" \").replace(\"\\\\r\", \" \")\n",
    "  text = re.sub(r'\\s+', ' ', text)  \n",
    "  text = text.strip()\n",
    "  text = text.lower()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8466d",
   "metadata": {},
   "source": [
    "### Apply cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9663d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_DF[\"cleaned_content\"] = DOCUMENT_DF[\"content\"].apply(cleantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a05d56",
   "metadata": {},
   "source": [
    "### Initialize all models and tokenizers from IndoBERT and IndoBERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "162726b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "indobert_model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "indobert_tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "\n",
    "tweet_model = AutoModel.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "tweet_tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "\n",
    "#Turn on evaluation mode as default\n",
    "indobert_model.eval()\n",
    "tweet_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965c6ea",
   "metadata": {},
   "source": [
    "### Create functions to get encodings for both indobert and indobertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c31a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def getEncodings(textArray):\n",
    "  indobert_inputs = indobert_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  tweet_inputs = tweet_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  with torch.no_grad():\n",
    "    indobert_outputs = indobert_model(**indobert_inputs)\n",
    "    tweet_outputs = tweet_model(**tweet_inputs)\n",
    "  indobert_embeddings = indobert_outputs.last_hidden_state[:, 0, :]\n",
    "  tweet_embeddings = tweet_outputs.last_hidden_state[:, 0, :]\n",
    "  return (indobert_embeddings.cpu().numpy(), tweet_embeddings.cpu().numpy())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148ebfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def removeFile(file_path):\n",
    "  if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f\"{file_path} deleted.\")\n",
    "  else:\n",
    "    print(f\"{file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "258959a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "INDOBERT_OUT_FILE = OUT_DIR + \"indobert_embeds.jsonl\"\n",
    "INDOBERTWEET_OUT_FILE = OUT_DIR + \"indobertweet_embeds.jsonl\"\n",
    "def get_batch_embeddings(documents, batch_size=32):\n",
    "  #Document is in the shape of a Pandas Dataframe. Convert to a list first before processing\n",
    "  removeFile(INDOBERT_OUT_FILE)\n",
    "  removeFile(INDOBERTWEET_OUT_FILE)\n",
    "  documents_list = documents.to_dict(orient=\"records\")\n",
    "  for i in tqdm(range(0, len(documents_list), batch_size), desc=\"Generating embeddings\"):\n",
    "    batched = documents_list[i:i+batch_size]\n",
    "    texts = [doc[\"cleaned_content\"] for doc in batched]\n",
    "    indobert_embedding, tweet_embedding = getEncodings(texts)\n",
    "    \n",
    "    with open(INDOBERT_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, indobert_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")\n",
    "        \n",
    "    with open(INDOBERTWEET_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, tweet_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84b2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_embeds.jsonl deleted.\n",
      "OUT/indobertweet_embeds.jsonl deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "get_batch_embeddings(DOCUMENT_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5875d73",
   "metadata": {},
   "source": [
    "### Create function to reduce embed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04096a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "def reduce_embed_size(embeds):\n",
    "  umap_model = umap.UMAP(n_components=45)\n",
    "  reduced_embedding = umap_model.fit_transform(np.array(embeds))\n",
    "  return reduced_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80dedb8",
   "metadata": {},
   "source": [
    "### Process Indobert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ae7ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "INDOBERT_REDUCED_OUT_FILE = OUT_DIR + \"indobert_reduced_embeds.jsonl\"\n",
    "\n",
    "with open(INDOBERT_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(INDOBERT_REDUCED_OUT_FILE)\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a2933",
   "metadata": {},
   "source": [
    "### Process IndoBERTweet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9aece640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "TWEET_REDUCED_OUT_FILE = OUT_DIR + \"indobertweet_reduced_embeds.jsonl\"\n",
    "\n",
    "with open(INDOBERTWEET_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(TWEET_REDUCED_OUT_FILE)\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34f4ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christianharjuno/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "try:\n",
    "  stopwords.words('english')\n",
    "except LookupError:\n",
    "  nltk.download('stopwords')\n",
    "stopwords_combined = set(stopwords.words(\"indonesian\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "def extract_structural_features(tweet):\n",
    "  words = tweet.split()\n",
    "  word_lengths = [len(w) for w in words]\n",
    "  \n",
    "  length = len(tweet)\n",
    "  num_hashtags = tweet.count(\"#\")\n",
    "  num_mentions = tweet.count(\"@\")\n",
    "  num_urls = len(re.findall(r\"http\\S+\", tweet))\n",
    "  num_emojis = len([c for c in tweet if c in emoji.EMOJI_DATA])\n",
    "  num_upper = sum(1 for c in tweet if c.isupper())\n",
    "  num_punct = len(re.findall(r\"[^\\w\\s]\", tweet))\n",
    "  avg_word_len = np.mean(word_lengths) if words else 0\n",
    "\n",
    "  # Content/structure-oriented features\n",
    "  is_question = int(tweet.strip().endswith('?'))\n",
    "  is_exclamatory = int(tweet.strip().endswith('!'))\n",
    "  contains_ellipsis = int(\"...\" in tweet)\n",
    "  contains_repeated_chars = int(bool(re.search(r\"(.)\\1{2,}\", tweet)))  # e.g., sooo, yessss\n",
    "  contains_short_link = int(bool(re.search(r\"\\b(?:https?:\\/\\/)?(?:www\\.)?(bit\\.ly|t\\.co|tinyurl\\.com|goo\\.gl|ow\\.ly|is\\.gd|buff\\.ly|adf\\.ly|bitly\\.com|cutt\\.ly|rb\\.gy|rebrand\\.ly)\\/[A-Za-z0-9]+\", tweet)))\n",
    "  contains_digit = int(bool(re.search(r\"\\d\", tweet)))\n",
    "  is_all_caps = int(tweet.isupper() and len(tweet) > 3)\n",
    "  is_emoji_only = int(all(c in emoji.EMOJI_DATA or c.isspace() for c in tweet.strip()) and tweet.strip() != \"\")\n",
    "  contains_quote_or_rt = int(bool(re.search(r\"(RT\\s@|\\\".+\\\")\", tweet)))\n",
    "  word_count = len(words)\n",
    "  stopword_ratio = np.mean([w.lower() in stopwords_combined for w in words]) if words else 0\n",
    "\n",
    "  return [\n",
    "    length, num_hashtags, num_mentions, num_urls,\n",
    "    num_emojis, num_upper, num_punct, avg_word_len,\n",
    "    is_question, is_exclamatory, contains_ellipsis,\n",
    "    contains_repeated_chars, contains_short_link,\n",
    "    contains_digit, is_all_caps, is_emoji_only,\n",
    "    contains_quote_or_rt, word_count, stopword_ratio\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9316084",
   "metadata": {},
   "source": [
    "### Generate structural features of each cleaned content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3c7ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "INDOBERT_NORMALIZED = OUT_DIR + \"indobert_normalized.jsonl\"\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"cleaned_content\"]) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props)])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(INDOBERT_NORMALIZED)\n",
    "with open(INDOBERT_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c335379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_normalized.jsonl does not exist.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "TWEET_NORMALIZED = OUT_DIR + \"indobertweet_normalized.jsonl\"\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"cleaned_content\"]) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props)])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(TWEET_NORMALIZED)\n",
    "with open(TWEET_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0dd2d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clustered_docs(docs, cluster_labels, output_path):\n",
    "    for doc, label in zip(docs, cluster_labels):\n",
    "        if \"metadata\" not in doc:\n",
    "            doc[\"metadata\"] = {}\n",
    "        doc[\"metadata\"][\"bucket_label\"] = int(label)\n",
    "\n",
    "    # Sort by label for better organization (optional)\n",
    "    docs_sorted = sorted(docs, key=lambda x: x[\"metadata\"][\"bucket_label\"])\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for doc in docs_sorted:\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d01c4",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobert\n",
    "1. KMeans + IndoBERT Embeddings\n",
    "2. KMeans + IndoBERT Embeddings + Structure Properties\n",
    "3. KMeans + Structure Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "from sre_parse import Verbose\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "INDOBERT_KMEANS_EMBED = OUT_DIR + \"indobert-kmeans-embed.json\"\n",
    "INDOBERT_KMEANS_PROPS = OUT_DIR + \"indobert-kmeans-props.json\"\n",
    "INDOBERT_KMEANS_CONCAT = OUT_DIR + \"indobert-kmeans-concat.json\"\n",
    "\n",
    "with open(INDOBERT_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=8, random_state=42).fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = KMeans(n_clusters=8, random_state=42).fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = KMeans(n_clusters=8, random_state=42).fit(properties)\n",
    "\n",
    "def save_clustered_docs(docs, cluster_labels, output_path):\n",
    "    for doc, label in zip(docs, cluster_labels):\n",
    "        doc[\"bucket_label\"] = int(label)\n",
    "    for doc in normalized_indobert_documents:\n",
    "        if \"embedding\" in doc:\n",
    "            del doc[\"embedding\"]\n",
    "        if \"structural_property\" in doc:\n",
    "            del doc[\"structural_property\"]\n",
    "        if \"concatenated_features\" in doc:\n",
    "            del doc[\"concatenated_features\"]\n",
    "            \n",
    "    # Sort by label for better organization (optional)\n",
    "    docs_sorted = sorted(docs, key=lambda x: x[\"bucket_label\"])\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "      json.dump(docs_sorted, f, ensure_ascii=False)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(normalized_indobert_documents, labels_embed, INDOBERT_KMEANS_EMBED)\n",
    "save_clustered_docs(normalized_indobert_documents, labels_concat, INDOBERT_KMEANS_CONCAT)\n",
    "save_clustered_docs(normalized_indobert_documents, labels_props, INDOBERT_KMEANS_PROPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1875ee",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobertweet\n",
    "1. KMeans + IndoBERTweet Embeddings\n",
    "2. KMeans + IndoBERTweet Embeddings + Structure Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b408120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "INDOBERTWEET_KMEANS_EMBED = OUT_DIR + \"indobertweet-kmeans-embed.json\"\n",
    "INDOBERTWEET_KMEANS_PROPS = OUT_DIR + \"indobertweet-kmeans-props.json\"\n",
    "INDOBERTWEET_KMEANS_CONCAT = OUT_DIR + \"indobertweet-kmeans-concat.json\"\n",
    "\n",
    "with open(TWEET_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=8, random_state=42).fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = KMeans(n_clusters=8, random_state=42).fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = KMeans(n_clusters=8, random_state=42).fit(properties)\n",
    "\n",
    "def save_clustered_docs(docs, cluster_labels, output_path):\n",
    "    for doc, label in zip(docs, cluster_labels):\n",
    "        doc[\"bucket_label\"] = int(label)\n",
    "    for doc in normalized_indobert_documents:\n",
    "        if \"embedding\" in doc:\n",
    "            del doc[\"embedding\"]\n",
    "        if \"structural_property\" in doc:\n",
    "            del doc[\"structural_property\"]\n",
    "        if \"concatenated_features\" in doc:\n",
    "            del doc[\"concatenated_features\"]\n",
    "            \n",
    "    # Sort by label for better organization (optional)\n",
    "    docs_sorted = sorted(docs, key=lambda x: x[\"bucket_label\"])\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "      json.dump(docs_sorted, f, ensure_ascii=False)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(normalized_indobert_documents, labels_embed, INDOBERTWEET_KMEANS_EMBED)\n",
    "save_clustered_docs(normalized_indobert_documents, labels_concat, INDOBERTWEET_KMEANS_CONCAT)\n",
    "save_clustered_docs(normalized_indobert_documents, labels_props, INDOBERTWEET_KMEANS_PROPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
