{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11887971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "OUT_DIR =\"out/\"\n",
    "ASSET_DIR=\"assets/\"\n",
    "with open(ASSET_DIR + \"dump-formatted.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "  RAW_DOCUMENTS = json.load(file)\n",
    "DOCUMENT_DF = pd.DataFrame.from_records(RAW_DOCUMENTS)\n",
    "DATA_LEN = len(DOCUMENT_DF)\n",
    "  \n",
    "INDOBERT_OUT_FILE = OUT_DIR + \"indobert_embeds.jsonl\"\n",
    "INDOBERTWEET_OUT_FILE = OUT_DIR + \"indobertweet_embeds.jsonl\"\n",
    "INDOBERT_REDUCED_OUT_FILE = OUT_DIR + \"indobert_reduced_embeds.jsonl\"\n",
    "TWEET_REDUCED_OUT_FILE = OUT_DIR + \"indobertweet_reduced_embeds.jsonl\"\n",
    "INDOBERT_NORMALIZED = OUT_DIR + \"indobert_normalized.jsonl\"\n",
    "TWEET_NORMALIZED = OUT_DIR + \"indobertweet_normalized.jsonl\"\n",
    "\n",
    "INDOBERT_KMEANS_EMBED = OUT_DIR + \"indobert/indobert-kmeans-embed.json\"\n",
    "INDOBERT_KMEANS_PROPS = OUT_DIR + \"model-agnostic/mixed-kmeans-props.json\"\n",
    "INDOBERT_KMEANS_CONCAT = OUT_DIR + \"indobert/indobert-kmeans-concat.json\"\n",
    "INDOBERTWEET_KMEANS_EMBED = OUT_DIR + \"indobertweet/indobertweet-kmeans-embed.json\"\n",
    "INDOBERTWEET_KMEANS_PROPS = OUT_DIR + \"model-agnostic/mixed-kmeans-props.json\"\n",
    "INDOBERTWEET_KMEANS_CONCAT = OUT_DIR + \"indobertweet/indobertweet-kmeans-concat.json\"\n",
    "\n",
    "INDOBERT_HDBSCAN_EMBED = OUT_DIR + \"indobert/indobert-hdbscan-embed.json\"\n",
    "INDOBERT_HDBSCAN_PROPS = OUT_DIR + \"model-agnostic/mixed-hdbscan-props.json\"\n",
    "INDOBERT_HDBSCAN_CONCAT = OUT_DIR + \"indobert/indobert-hdbscan-concat.json\"\n",
    "INDOBERTWEET_HDBSCAN_EMBED = OUT_DIR + \"indobertweet/indobertweet-hdbscan-embed.json\"\n",
    "INDOBERTWEET_HDBSCAN_PROPS = OUT_DIR + \"model-agnostic/mixed-hdbscan-props.json\"\n",
    "INDOBERTWEET_HDBSCAN_CONCAT = OUT_DIR + \"indobertweet/indobertweet-hdbscan-concat.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd9c36",
   "metadata": {},
   "source": [
    "### Text Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f687730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "# ─── pre-compiled patterns ────────────────────────────────────────────────\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_KUTI_BEF = re.compile(r'(?i)(?<!\\s)(kutipan)')\n",
    "_KUTI_AFT = re.compile(r'(?i)(kutipan)(?!\\s)')\n",
    "_REPEAT   = re.compile(r'(.)\\1{2,}')       # ≥3 of same char\n",
    "_WS       = re.compile(r'\\s+')\n",
    "\n",
    "# remove from the first token that *begins* with “kutipan” (any case) to the string-end\n",
    "_KUTI_CUT = re.compile(r'(?i)\\bkutipan\\w*.*$', re.DOTALL)   # pre-compile once\n",
    "\n",
    "def cleantext(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = _MENTION.sub(' ', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "\n",
    "    # ⇣ one liner does all the “kutipan” work; the old _KUTI_BEF/_KUTI_AFT are no longer needed\n",
    "    text = _KUTI_CUT.sub('', text)\n",
    "\n",
    "    text = emoji.demojize(text, delimiters=(' ', ' '))\n",
    "    text = _REPEAT.sub(r'\\1\\1', text)\n",
    "    text = _WS.sub(' ', text).strip().lower()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8466d",
   "metadata": {},
   "source": [
    "### Apply cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9d9663d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_DF[\"content\"] = DOCUMENT_DF[\"content\"].apply(cleantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77557b79",
   "metadata": {},
   "source": [
    "### Generate splits and golden standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "372557d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "\n",
    "DOCUMENT_DF = DOCUMENT_DF.drop_duplicates(subset=[\"content\"]).reset_index(drop=True)\n",
    "TRAIN_DF, TEST_DF = train_test_split(\n",
    "  DOCUMENT_DF,\n",
    "  test_size=0.90,\n",
    "  random_state=42,\n",
    ")\n",
    "GOLDEN_STANDARD, UNUSED = train_test_split(\n",
    "  TEST_DF,\n",
    "  test_size=0.99,\n",
    "  random_state=42\n",
    ")\n",
    "print(len(TRAIN_DF))\n",
    "with open(\"out/golden_standard.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(GOLDEN_STANDARD.to_dict(orient=\"records\"), file, ensure_ascii=False, indent=2)\n",
    "with open(\"out/training_split_general.json\", \"w\") as file:\n",
    "  json.dump(TRAIN_DF.to_dict(orient=\"records\"),file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292726f",
   "metadata": {},
   "source": [
    "### Extract hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fac8fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from typing import List, Hashable, Optional\n",
    "with open(\"out/training_split_general.json\", \"r\") as file:\n",
    "  documents = json.load(file)\n",
    "texts = [doc[\"content\"] for doc in documents]\n",
    "\n",
    "hashtags = []\n",
    "\n",
    "for text in texts:\n",
    "  text_split = text.split(\" \")\n",
    "  for token in text_split:\n",
    "    if token.startswith(\"#\"):\n",
    "      hashtags.append(token)\n",
    "\n",
    "def most_common_hashtags(\n",
    "    tags: List[Hashable],\n",
    "    *,\n",
    "    top_n: Optional[int] = None,\n",
    "    min_count: Optional[int] = None,\n",
    ") -> List[Hashable]:\n",
    "    if top_n is None and min_count is None:\n",
    "        raise ValueError(\"Specify either top_n or min_count\")\n",
    "\n",
    "    freq = Counter(tags)\n",
    "    # Sort once by (-count, tag) so result is deterministic for ties\n",
    "    ranked = sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "\n",
    "    if top_n is not None:\n",
    "        selected = ranked[:top_n]\n",
    "    else:\n",
    "        selected = [kv for kv in ranked if kv[1] >= min_count]\n",
    "\n",
    "    return [tag for tag, _ in selected]\n",
    "\n",
    "\n",
    "cleaned_hashtags = most_common_hashtags(hashtags, min_count=20)\n",
    "with open(\"out/hashtag_list.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(cleaned_hashtags, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a05d56",
   "metadata": {},
   "source": [
    "### Initialize all models and tokenizers from IndoBERT and IndoBERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "162726b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "indobert_model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "indobert_tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "\n",
    "tweet_model = AutoModel.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "tweet_tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "\n",
    "indobert_tokenizer.add_tokens(cleaned_hashtags)\n",
    "tweet_tokenizer.add_tokens(cleaned_hashtags)\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "indobert_model = indobert_model.to(device)\n",
    "tweet_model = tweet_model.to(device)\n",
    "\n",
    "#Turn on evaluation mode as default\n",
    "indobert_model.eval()\n",
    "tweet_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965c6ea",
   "metadata": {},
   "source": [
    "### Create functions to get encodings for both indobert and indobertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d2c31a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def getEncodings(textArray):\n",
    "  indobert_inputs = indobert_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  tweet_inputs = tweet_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  indobert_inputs = {k: v.to(\"mps\") for k, v in indobert_inputs.items()}\n",
    "  tweet_inputs = {k: v.to(\"mps\") for k, v in tweet_inputs.items()}\n",
    "  with torch.no_grad():\n",
    "    indobert_outputs = indobert_model(**indobert_inputs)\n",
    "    tweet_outputs = tweet_model(**tweet_inputs)\n",
    "    \n",
    "    \n",
    "  indobert_embeddings = indobert_outputs.last_hidden_state[:, 0, :]\n",
    "  tweet_embeddings = tweet_outputs.last_hidden_state[:, 0, :]\n",
    "  return (indobert_embeddings.cpu().numpy(), tweet_embeddings.cpu().numpy())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "148ebfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def removeFile(file_path):\n",
    "  if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f\"{file_path} deleted.\")\n",
    "  else:\n",
    "    print(f\"{file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "258959a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "def get_batch_embeddings(documents, batch_size=32):\n",
    "  #Document is in the shape of a Pandas Dataframe. Convert to a list first before processing\n",
    "  removeFile(INDOBERT_OUT_FILE)\n",
    "  removeFile(INDOBERTWEET_OUT_FILE)\n",
    "  documents_list = documents.to_dict(orient=\"records\")\n",
    "  for i in tqdm(range(0, len(documents_list), batch_size), desc=\"Generating embeddings\"):\n",
    "    batched = documents_list[i:i+batch_size]\n",
    "    texts = [doc[\"content\"] for doc in batched]\n",
    "    indobert_embedding, tweet_embedding = getEncodings(texts)\n",
    "    \n",
    "    with open(INDOBERT_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, indobert_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")\n",
    "        \n",
    "    with open(INDOBERTWEET_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, tweet_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7d84b2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_embeds.jsonl deleted.\n",
      "OUT/indobertweet_embeds.jsonl deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 481/481 [03:37<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "get_batch_embeddings(TRAIN_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5875d73",
   "metadata": {},
   "source": [
    "### Create function to reduce embed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "04096a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "def reduce_embed_size(embeds):\n",
    "  umap_model = umap.UMAP(n_components=45)\n",
    "  reduced_embedding = umap_model.fit_transform(np.array(embeds))\n",
    "  return reduced_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80dedb8",
   "metadata": {},
   "source": [
    "### Process Indobert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3ae7ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(INDOBERT_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(INDOBERT_REDUCED_OUT_FILE)\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a2933",
   "metadata": {},
   "source": [
    "### Process IndoBERTweet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9aece640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(INDOBERTWEET_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(TWEET_REDUCED_OUT_FILE)\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "34f4ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "try:\n",
    "  stopwords.words('english')\n",
    "except LookupError:\n",
    "  nltk.download('stopwords')\n",
    "stopwords_combined = set(stopwords.words(\"indonesian\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "def extract_structural_features(tweet):\n",
    "  words = tweet.split()\n",
    "  word_lengths = [len(w) for w in words]\n",
    "  \n",
    "  length = len(tweet)\n",
    "  num_hashtags = tweet.count(\"#\")\n",
    "  num_mentions = tweet.count(\"@\")\n",
    "  num_urls = len(re.findall(r\"http\\S+\", tweet))\n",
    "  num_emojis = len([c for c in tweet if c in emoji.EMOJI_DATA])\n",
    "  num_upper = sum(1 for c in tweet if c.isupper())\n",
    "  num_punct = len(re.findall(r\"[^\\w\\s]\", tweet))\n",
    "  avg_word_len = np.mean(word_lengths) if words else 0\n",
    "\n",
    "  # Content/structure-oriented features\n",
    "  is_question = int(tweet.strip().endswith('?'))\n",
    "  is_exclamatory = int(tweet.strip().endswith('!'))\n",
    "  contains_ellipsis = int(\"...\" in tweet)\n",
    "  contains_repeated_chars = int(bool(re.search(r\"(.)\\1{2,}\", tweet)))  # e.g., sooo, yessss\n",
    "  contains_short_link = int(bool(re.search(r\"\\b(?:https?:\\/\\/)?(?:www\\.)?(bit\\.ly|t\\.co|tinyurl\\.com|goo\\.gl|ow\\.ly|is\\.gd|buff\\.ly|adf\\.ly|bitly\\.com|cutt\\.ly|rb\\.gy|rebrand\\.ly)\\/[A-Za-z0-9]+\", tweet)))\n",
    "  contains_digit = int(bool(re.search(r\"\\d\", tweet)))\n",
    "  is_all_caps = int(tweet.isupper() and len(tweet) > 3)\n",
    "  is_emoji_only = int(all(c in emoji.EMOJI_DATA or c.isspace() for c in tweet.strip()) and tweet.strip() != \"\")\n",
    "  contains_quote_or_rt = int(bool(re.search(r\"(RT\\s@|\\\".+\\\")\", tweet)))\n",
    "  word_count = len(words)\n",
    "  stopword_ratio = np.mean([w.lower() in stopwords_combined for w in words]) if words else 0\n",
    "\n",
    "  return [\n",
    "    length, num_hashtags, num_mentions, num_urls,\n",
    "    num_emojis, num_upper, num_punct, avg_word_len,\n",
    "    is_question, is_exclamatory, contains_ellipsis,\n",
    "    contains_repeated_chars, contains_short_link,\n",
    "    contains_digit, is_all_caps, is_emoji_only,\n",
    "    contains_quote_or_rt, word_count, stopword_ratio\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9316084",
   "metadata": {},
   "source": [
    "### Generate structural features of each cleaned content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a3c7ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"content\"]) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props) * 2])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(INDOBERT_NORMALIZED)\n",
    "with open(INDOBERT_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5c335379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"content\"]) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props) * 2])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(TWEET_NORMALIZED)\n",
    "with open(TWEET_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d01c4",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobert\n",
    "1. KMeans + IndoBERT Embeddings\n",
    "2. KMeans + IndoBERT Embeddings + Structure Properties\n",
    "3. KMeans + Structure Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb6699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clustered_docs(docs, cluster_labels, output_path):\n",
    "    for doc, label in zip(docs, cluster_labels):\n",
    "        doc[\"bucket_label\"] = int(label)\n",
    "    for doc in docs:\n",
    "        del doc[\"__v\"]\n",
    "        del doc[\"_id\"]\n",
    "        if \"embedding\" in doc:\n",
    "            del doc[\"embedding\"]\n",
    "        if \"structural_property\" in doc:\n",
    "            del doc[\"structural_property\"]\n",
    "        if \"concatenated_features\" in doc:\n",
    "            del doc[\"concatenated_features\"]\n",
    "            \n",
    "    # Sort by label for better organization (optional)\n",
    "    docs_sorted = sorted(docs, key=lambda x: x[\"bucket_label\"])\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs_sorted, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import hdbscan\n",
    "import copy\n",
    "\n",
    "with open(INDOBERT_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=20, random_state=42).fit(embeds)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERT_KMEANS_EMBED)\n",
    "########################################################################################################################################\n",
    "with open(TWEET_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only #before waas  18\n",
    "kmeans_embed = hdbscan.HDBSCAN(min_cluster_size=40, metric=\"euclidean\").fit(embeds)\n",
    "\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERTWEET_HDBSCAN_EMBED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dcdecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running silhouette score and gini coefficient tests...\n",
      "\n",
      "--- IndoBERT KMeans + Embed ---\n",
      "out/indobert/indobert-kmeans-embed.json Silhouette Score: 0.3413\n",
      "out/indobert/indobert-kmeans-embed.json Gini Coefficient of cluster sizes: 0.2148\n",
      "Cluster counts: {0: 192, 1: 281, 2: 173, 3: 168, 4: 80, 5: 154, 6: 82, 7: 190, 8: 170, 9: 221, 10: 51, 11: 250, 12: 33, 13: 163, 14: 204, 15: 180, 16: 201, 17: 182, 18: 185, 19: 309, 20: 119, 21: 256, 22: 210, 23: 106, 24: 24, 25: 50, 26: 23, 27: 128, 28: 209, 29: 206, 30: 9, 31: 189, 32: 128, 33: 177, 34: 153, 35: 153, 36: 249, 37: 165, 38: 160, 39: 219, 40: 228, 41: 181, 42: 169, 43: 154, 44: 216, 45: 111, 46: 59, 47: 186, 48: 321, 49: 131, 50: 189, 51: 145, 52: 147, 53: 212, 54: 129, 55: 159, 56: 92, 57: 202, 58: 161, 59: 211, 60: 166, 61: 194, 62: 67, 63: 146, 64: 193, 65: 128, 66: 265, 67: 132, 68: 133, 69: 168, 70: 61, 71: 255, 72: 60, 73: 86, 74: 169, 75: 130, 76: 231, 77: 64, 78: 212, 79: 75, 80: 177, 81: 231, 82: 133, 83: 205, 84: 119, 85: 200, 86: 76, 87: 219, 88: 142, 89: 188, 90: 208, 91: 125, 92: 170, 93: 232, 94: 180, 95: 205, 96: 156, 97: 221, 98: 163, 99: 122}\n",
      "\n",
      "--- IndoBERTweet HDBSCAN + Embed ---\n",
      "out/indobertweet/indobertweet-hdbscan-embed.json Silhouette Score: -0.2142\n",
      "out/indobertweet/indobertweet-hdbscan-embed.json Gini Coefficient of cluster sizes: 0.5698\n",
      "Cluster counts: {0: 45, 1: 21, 2: 298, 3: 59, 4: 33, 5: 501, 6: 106, 7: 44, 8: 13, 9: 105, 10: 27, 11: 22, 12: 15, 13: 52, 14: 11, 15: 152, 16: 15, 17: 18, 18: 19, 19: 72, 20: 22, 21: 12, 22: 28, 23: 59, 24: 18, 25: 31, 26: 23, 27: 40, 28: 71, 29: 14, 30: 16, 31: 10, 32: 45, 33: 19, 34: 20, 35: 47, 36: 34, 37: 41, 38: 41, 39: 16, 40: 24, 41: 42, 42: 45, 43: 26, 44: 13, 45: 34, 46: 38, 47: 20, 48: 13, 49: 85, 50: 134, 51: 17, 52: 24, 53: 12, 54: 16, 55: 14, 56: 16, 57: 14, 58: 23, 59: 52, 60: 71, 61: 53, 62: 10, 63: 31, 64: 12, 65: 49, 66: 207, 67: 13, 68: 15, 69: 19, 70: 55, 71: 34, 72: 85, 73: 43, 74: 14, 75: 249, 76: 13, 77: 29, 78: 49, 79: 14, 80: 13, 81: 104, 82: 15, 83: 26, 84: 39, 85: 24, 86: 12, 87: 21, 88: 234, 89: 10, 90: 113, 91: 10, 92: 18, 93: 94, 94: 36, 95: 285, 96: 44, 97: 36, 98: 24, 99: 56, 100: 36, 101: 10, 102: 28, 103: 102, 104: 14, 105: 20, 106: 119, 107: 18, 108: 17, 109: 293, 110: 21, 111: 22, 112: 17, 113: 27, 114: 34, 115: 30, 116: 40, 117: 23, 118: 46, 119: 14, 120: 16, 121: 43, 122: 204, 123: 86, 124: 16, 125: 28, 126: 23, 127: 12, 128: 39, 129: 30, 130: 882}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def gini_coefficient(array):\n",
    "    \"\"\"Compute Gini coefficient of array of values.\"\"\"\n",
    "    # Based on mean absolute difference formula\n",
    "    array = np.array(array, dtype=np.float64)\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array)  # Ensure non-negative\n",
    "    array += 1e-10  # Avoid division by zero\n",
    "    array = np.sort(array)\n",
    "    n = array.size\n",
    "    cumvals = np.cumsum(array)\n",
    "    gini = (n + 1 - 2 * np.sum(cumvals) / cumvals[-1]) / n\n",
    "    return gini\n",
    "\n",
    "def evaluate_gini(bucketed):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    labels = labels_data[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    # Exclude noise points (-1)\n",
    "    valid_labels = labels[labels >= 0]\n",
    "\n",
    "    counts = pd.Series(valid_labels).value_counts().sort_index()\n",
    "    gini = gini_coefficient(counts.values)\n",
    "\n",
    "    print(f\"{bucketed} Gini Coefficient of cluster sizes: {gini:.4f}\")\n",
    "    print(f\"Cluster counts: {counts.to_dict()}\")\n",
    "\n",
    "def get_silhouette_score_embedding(bucketed, model_features):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    with open(model_features, \"r\") as file:\n",
    "        features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "    merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "    features_embedding = np.stack(merged_df[\"embedding\"].to_numpy())\n",
    "    labels = merged_df[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    embedding_score = silhouette_score(features_embedding, labels, metric='euclidean')\n",
    "    print(f\"{bucketed} Silhouette Score: {embedding_score:.4f}\")\n",
    "\n",
    "def get_silhouette_score_properties(bucketed, model_features):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    with open(model_features, \"r\") as file:\n",
    "        features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "    merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "    features_properties = np.stack(merged_df[\"structural_property\"].to_numpy())\n",
    "    labels = merged_df[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    # valid_mask = labels >= 0\n",
    "    # features_properties_valid = features_properties[valid_mask]\n",
    "    # labels_valid = labels[valid_mask]\n",
    "\n",
    "    properties_score = silhouette_score(features_properties, labels, metric='euclidean')\n",
    "    print(f\"{bucketed} Silhouette Score: {properties_score:.4f}\")\n",
    "\n",
    "def get_silhouette_score_concat(bucketed, model_features):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    with open(model_features, \"r\") as file:\n",
    "        features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "    merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "    features_concat = np.stack(merged_df[\"concatenated_features\"].to_numpy())\n",
    "    labels = merged_df[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    concat_score = silhouette_score(features_concat, labels, metric='euclidean')\n",
    "    print(f\"{bucketed} Silhouette Score: {concat_score:.4f}\")\n",
    "\n",
    "\n",
    "tests = [\n",
    "  [\"IndoBERT KMeans + Embed\",   INDOBERT_KMEANS_EMBED,  INDOBERT_NORMALIZED, get_silhouette_score_embedding],\n",
    "  [\"IndoBERTweet HDBSCAN + Embed\",  INDOBERTWEET_HDBSCAN_EMBED,  TWEET_NORMALIZED, get_silhouette_score_embedding],\n",
    "]\n",
    "\n",
    "print(\"Running silhouette score and gini coefficient tests...\\n\")\n",
    "for name, label_file, feature_file, scorer in tests:\n",
    "    print(f\"--- {name} ---\")\n",
    "    scorer(label_file, feature_file)\n",
    "    evaluate_gini(label_file)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b839114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Davies–Bouldin Index tests...\n",
      "\n",
      "out/indobert/indobert-kmeans-embed.json DBI (Embedding): 0.6192\n",
      "out/indobertweet/indobertweet-hdbscan-embed.json DBI (Embedding): 0.9380\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# DBI SCORERS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def dbi_embedding(bucketed: str, model_features: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the Davies–Bouldin Index using only the embedding vectors.\n",
    "    \"\"\"\n",
    "    labels_df   = pd.read_json(bucketed)\n",
    "    labels_df[\"tweet_id\"] = labels_df[\"tweet_id\"].astype(str)\n",
    "    \n",
    "    features_df = pd.DataFrame([json.loads(line) for line in open(model_features)])\n",
    "\n",
    "    merged_df   = pd.merge(labels_df, features_df, on=\"tweet_id\", how=\"inner\")\n",
    "    X           = np.stack(merged_df[\"embedding\"].to_numpy())\n",
    "    y           = merged_df[\"bucket_label\"]\n",
    "\n",
    "    score = davies_bouldin_score(X, y)\n",
    "    print(f\"{bucketed} DBI (Embedding): {score:.4f}\")\n",
    "\n",
    "def dbi_properties(bucketed: str, model_features: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the Davies–Bouldin Index on structural-property feature vectors.\n",
    "    \"\"\"\n",
    "    labels_df   = pd.read_json(bucketed)\n",
    "    labels_df[\"tweet_id\"] = labels_df[\"tweet_id\"].astype(str)\n",
    "    features_df = pd.DataFrame([json.loads(line) for line in open(model_features)])\n",
    "\n",
    "    merged_df   = pd.merge(labels_df, features_df, on=\"tweet_id\", how=\"inner\")\n",
    "    X           = np.stack(merged_df[\"structural_property\"].to_numpy())\n",
    "    y           = merged_df[\"bucket_label\"]\n",
    "\n",
    "    score = davies_bouldin_score(X, y)\n",
    "    print(f\"{bucketed} DBI (Properties): {score:.4f}\")\n",
    "\n",
    "def dbi_concat(bucketed: str, model_features: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the Davies–Bouldin Index on concatenated feature vectors.\n",
    "    \"\"\"\n",
    "    labels_df   = pd.read_json(bucketed)\n",
    "    labels_df[\"tweet_id\"] = labels_df[\"tweet_id\"].astype(str)\n",
    "    features_df = pd.DataFrame([json.loads(line) for line in open(model_features)])\n",
    "\n",
    "    merged_df   = pd.merge(labels_df, features_df, on=\"tweet_id\", how=\"inner\")\n",
    "    X           = np.stack(merged_df[\"concatenated_features\"].to_numpy())\n",
    "    y           = merged_df[\"bucket_label\"]\n",
    "\n",
    "    score = davies_bouldin_score(X, y)\n",
    "    print(f\"{bucketed} DBI (Concat): {score:.4f}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# TEST MATRIX\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "tests = [\n",
    "    [\"IndoBERT KMeans  + Embed\",   INDOBERT_KMEANS_EMBED,    INDOBERT_NORMALIZED, dbi_embedding],\n",
    "    [\"IndoBERTweet HDBSCAN + Embed\",  INDOBERTWEET_HDBSCAN_EMBED,  TWEET_NORMALIZED, dbi_embedding],\n",
    "]\n",
    "\n",
    "print(\"Running Davies–Bouldin Index tests...\\n\")\n",
    "for name, label_file, feature_file, scorer in tests:\n",
    "    scorer(label_file, feature_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c1606",
   "metadata": {},
   "source": [
    "### Do a bit of bucket analization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d9ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from collections import Counter\n",
    "import re\n",
    "def count_hashtags(text):\n",
    "    return len(re.findall(r\"#\\w+\", text))\n",
    "\n",
    "def hashtag_ratio(text):\n",
    "    hashtags = ''.join(re.findall(r\"#\\w+\", text))\n",
    "    return len(hashtags) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\"\n",
    "                               \"\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "def emoji_ratio(text):\n",
    "    emojis = extract_emojis(text)\n",
    "    return len(''.join(emojis)) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def url_ratio(tweets):\n",
    "    return sum(1 for t in tweets if re.search(r\"http\\S+\", t)) / len(tweets)\n",
    "\n",
    "def mention_ratio(tweets):\n",
    "    return sum(1 for t in tweets if re.search(r\"@\\w+\", t)) / len(tweets)\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "def repeated_char_abuse(text):\n",
    "    return bool(re.search(r\"(.)\\1{3,}\", text))\n",
    "\n",
    "def duplicate_ratio(tweets):\n",
    "    freq = Counter(tweets)\n",
    "    return sum(count for tweet, count in freq.items() if count > 1) / len(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c8f84332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0: generic\n",
      "  avg_hashtags: 0.2755244755244755\n",
      "  avg_length: 70.04568764568765\n",
      "  longest_tweet: sorry bgt ye gw bukannya tone deaf atau apalah cuma badan gw sakit sakitan puki. semalem sakin overwhelmed nya seharian ngikutin berita ruu tni, ditambah timnas kalah, ditambah berita yang ga udah udah gw muntah muntah tengah malem sampe ga kuat makan lagi sakin lemesnya\n",
      "  shortest_tweet: pap\n",
      "  hashtag_ratio: 0.05821706207157399\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9720739969124486\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 1: generic\n",
      "  avg_hashtags: 0.8354037267080745\n",
      "  avg_length: 188.0\n",
      "  longest_tweet: selain kempen derma darah, turut diadakan demonstrasi pertolongan cemas cardiopulmonary resuscitation (cpr) serta penggunaan automated external defibrillator (aed) dalam program pada 8 disember lalu ini.nabalunews.comkerjasama komuniti, kerajaan dan swasta: kempen derma darah berjaya dilaksanakan di kampung..11 disember 2024 kota marudu : kempen derma darah anjuran kawasan rukun tetangga (krt) dan komuniti sihat perkasa negara (kospen) kampung masolog dengan kerjasama unit tabung darah hospital kota..\n",
      "  shortest_tweet: harap pn buat demonstrasi lah dakwa tok saka\n",
      "  hashtag_ratio: 0.06348265196149985\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9342568056869182\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 2: generic\n",
      "  avg_hashtags: 2.11877903118779\n",
      "  avg_length: 141.57332448573325\n",
      "  longest_tweet: ya, ini cuma rumor. tidak ada laporan resmi dari world bank yang menyebut masa depan indonesia \"gelap\". sebaliknya, world bank dan adb memprediksi pertumbuhan ekonomi stabil sekitar 5,0-5,2% untuk 2025. istilah \"dark indonesia\" lebih terkait protes politik, bukan pernyataan worldtampilkan lebih banyakgrokproductivitypasang\n",
      "  shortest_tweet: lawan dwifungsi tni!!\n",
      "  hashtag_ratio: 0.2477753653429727\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9564934210551687\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 3: hashtag-heavy\n",
      "  avg_hashtags: 8.644770408163266\n",
      "  avg_length: 171.6549744897959\n",
      "  longest_tweet: #cabutuutni #tolakruupolri #supremasisipil #adilijokowi #makzulkanprabowogibran #usiroligarki #batalkanpsn #indonesiagelap #peringatandarurat #sos #nalar marperempuan mengalami kekerasan pada aksi demo di gedung dpr ri.. #cabutuutni #tolakruutni #tolakrevisiuutni #tolakdwifungsiabri #indonesiagelap #peringatandarurat\n",
      "  shortest_tweet: go!! dwifungsi tni go!!\n",
      "  hashtag_ratio: 0.7895076333411463\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.956657900251721\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 4: long tweets\n",
      "  avg_hashtags: 1.2464685825621042\n",
      "  avg_length: 246.55577204091574\n",
      "  longest_tweet: updates (baca di threadnya) gerakan hamas: kami menyerukan kelanjutan dan peningkatan demonstrasi jumat,sabtu,minggu sebagai penolakan terhadap kelaparan anak\" & mengecam agresi zionis terhadap jalur gaza. 09.11.23 #freepalenstine #stopgenocideingaza #sahabatpalestina_idkutipańme_ nov 2023 akun parodihamas tidak akan jatuh : demi allah, hamas tidak akan jatuh meskipun pihak-pihak yang dekat dan jauh menentangnya demi allah,hamas tidak akan jatuh meskipun setan dari barat dan arab menyatakan perang terhadapnya #freepalenstine #stopgenocideingaza #sahabatpalestina_id x.com/kheyl4_8/statu..\n",
      "  shortest_tweet: ⋆౨ৎ ̊⟡˖ ࣪ cahol dwifungsi —\n",
      "  hashtag_ratio: 0.07070696719267948\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.8738409334023365\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 5: generic\n",
      "  avg_hashtags: 0.2471422940480883\n",
      "  avg_length: 188.3728813559322\n",
      "  longest_tweet: beginilah nasib menjadi negara komedi yang dicengkeram oligarki dan dikuasai politik dinasti yang suka memberangus oposisi demi bisa mesra dengan bestie karena alergi demonstrasi parahnya persekusi sudah jadi hobi serta intimidasi jadi insting alami <url> mar kepercayaan publik dan dunia usaha makin terkikis akibat kemenkeu tak transparan dalam menyampaikan pengelolaan keuangan negara. bulan lalu, konferensi pers apbn kita yang biasanya rutin digelar, tiba-tiba tidak diadakan. bahkan, dokumen apbn kita edisi januari & februari 2025\n",
      "  shortest_tweet: hah kenapa?\n",
      "  hashtag_ratio: 0.020904024416048574\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9273427964591474\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 6: generic\n",
      "  avg_hashtags: 0.5\n",
      "  avg_length: 131.11023622047244\n",
      "  longest_tweet: nggak mau transparan dalam berproses membuat legislasi tapi mengaharapkan publik tidak berburuk sangka setelah apa yang terjadi dalam proses perubahan uu tni. anda punya otak?! #indonesiagelap aprpimpinan dpr minta publik tak berburuk sangka soal pembahasan ruu kuhap <url>\n",
      "  shortest_tweet: <url>\n",
      "  hashtag_ratio: 0.04801945478117335\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.970416505449059\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 7: generic\n",
      "  avg_hashtags: 1.9969325153374233\n",
      "  avg_length: 152.50306748466258\n",
      "  longest_tweet: ini adalah salah satu jeritan hati warga desa ria-ria yang berulang kali menggelar demonstrasi, mendesak pemerintah menyelesaikan berbagai masalah yang terjadi akibat food estate. <url> news indonesia okt 2024\"kalau kamu harus kehilangan tanah kami, lebih baik bunuh kami semua!\" ini adalah salah satu jeritan hati warga desa ria-ria yang berulang kali menggelar demonstrasi, mendesak pemerintah menyelesaikan berbagai masalah yang terjadi akibat food estate. <url>\n",
      "  shortest_tweet: pengin kuliah lg yaallah :(\n",
      "  hashtag_ratio: 0.21124532872700005\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9510482435763299\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 8: hashtag-heavy\n",
      "  avg_hashtags: 2.314263920671243\n",
      "  avg_length: 68.42944317315026\n",
      "  longest_tweet: to everyone who will take to the streets today, raising our voices against injustice, may you be protected by god, the kindest of all beings. and to those who cannot be there physically but still care, still fight in their own way—our voice matters just as much. #indonesiagelap\n",
      "  shortest_tweet: 1\n",
      "  hashtag_ratio: 0.5195099159953707\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9763561227458517\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 9: generic\n",
      "  avg_hashtags: 2.643879173290938\n",
      "  avg_length: 114.7066772655008\n",
      "  longest_tweet: nigeria: wapco trains 164 nigerian artisans, invests $750,00 annually #rajatdalal |#fenerintetikcisivincic |#skynani #skynani | |#tubabüyüküstün |#bbnaija|#tolakdwifungsiabri |#rnaq40|#volkankonak|resign|big akwes|tyla|e-levy|betting taenergynewsafrica.comnigeria: wapco trains 164 nigerian artisans, invests $750,00 annuallythe west african gas pipeline company ltd. (wapco) has committed over 750,00 dollars annually to train more than 164 artisans in nigeria, with a focus on empowering local communities. dr isaac..\n",
      "  shortest_tweet: #unjuk rasa\n",
      "  hashtag_ratio: 0.3105336236716718\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.967769575956962\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "from collections import defaultdict\n",
    "import json\n",
    "buckets = defaultdict(list)\n",
    "\n",
    "with open(INDOBERT_KMEANS_EMBED, \"r\") as file:\n",
    "  documents = json.load(file)\n",
    "\n",
    "for doc in documents:\n",
    "  buckets[doc[\"bucket_label\"]].append(doc)\n",
    "\n",
    "for label, bucket_tweets in buckets.items():\n",
    "    bucket_tweets = [doc[\"content\"] for doc in bucket_tweets]\n",
    "    \n",
    "    hashtag_counts = [count_hashtags(t) for t in bucket_tweets]\n",
    "    tweet_lengths = [len(t) for t in bucket_tweets]\n",
    "    hashtag_ratios = [hashtag_ratio(t) for t in bucket_tweets]\n",
    "    emoji_ratios = [emoji_ratio(t) for t in bucket_tweets]\n",
    "    lexical_divs = [lexical_diversity(t) for t in bucket_tweets]\n",
    "    repeated_abuse_count = sum(1 for t in bucket_tweets if repeated_char_abuse(t))\n",
    "\n",
    "    avg_hashtags = sum(hashtag_counts) / len(bucket_tweets)\n",
    "    avg_length = sum(tweet_lengths) / len(bucket_tweets)\n",
    "    avg_hashtag_ratio = sum(hashtag_ratios) / len(bucket_tweets)\n",
    "    avg_emoji_ratio = sum(emoji_ratios) / len(bucket_tweets)\n",
    "    avg_lexical_div = sum(lexical_divs) / len(bucket_tweets)\n",
    "    url_ratio_val = url_ratio(bucket_tweets)\n",
    "    mention_ratio_val = mention_ratio(bucket_tweets)\n",
    "    dup_ratio = duplicate_ratio(bucket_tweets)\n",
    "\n",
    "    # Heuristic label tagging\n",
    "    label_tags = []\n",
    "    if avg_length > 200:\n",
    "        label_tags.append(\"long tweets\")\n",
    "    if avg_hashtag_ratio > 0.4:\n",
    "        label_tags.append(\"hashtag-heavy\")\n",
    "    if avg_emoji_ratio > 0.2:\n",
    "        label_tags.append(\"emoji spam\")\n",
    "    if url_ratio_val > 0.3:\n",
    "        label_tags.append(\"link drop\")\n",
    "    if mention_ratio_val > 0.3:\n",
    "        label_tags.append(\"mention spam\")\n",
    "    if avg_lexical_div < 0.4:\n",
    "        label_tags.append(\"low diversity (copypasta)\")\n",
    "    if repeated_abuse_count / len(bucket_tweets) > 0.3:\n",
    "        label_tags.append(\"repeated char abuse\")\n",
    "    if dup_ratio > 0.3:\n",
    "        label_tags.append(\"high duplication\")\n",
    "\n",
    "    longest = max(bucket_tweets, key=len)\n",
    "    shortest = min(bucket_tweets, key=len)\n",
    "\n",
    "    results[label] = {\n",
    "        \"label\": \", \".join(label_tags) if label_tags else \"generic\",\n",
    "        \"avg_hashtags\": avg_hashtags,\n",
    "        \"avg_length\": avg_length,\n",
    "        \"longest_tweet\": longest,\n",
    "        \"shortest_tweet\": shortest,\n",
    "        \"hashtag_ratio\": avg_hashtag_ratio,\n",
    "        \"emoji_ratio\": avg_emoji_ratio,\n",
    "        \"url_ratio\": url_ratio_val,\n",
    "        \"mention_ratio\": mention_ratio_val,\n",
    "        \"lexical_diversity\": avg_lexical_div,\n",
    "        \"duplication_ratio\": dup_ratio,\n",
    "        \"repeated_char_abuse_count\": repeated_abuse_count,\n",
    "    }\n",
    "\n",
    "# Output the results\n",
    "for label, metrics in results.items():\n",
    "    print(f\"\\nCluster {label}: {metrics['label']}\")\n",
    "    for k, v in metrics.items():\n",
    "        if k != 'label':\n",
    "            print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e65716c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0 sampled with ratio 0.011843079200592153 and total sample 1\n",
      "Bucket 1 sampled with ratio 0.017332839871699975 and total sample 1\n",
      "Bucket 2 sampled with ratio 0.010671107821366889 and total sample 1\n",
      "Bucket 3 sampled with ratio 0.010362694300518135 and total sample 1\n",
      "Bucket 4 sampled with ratio 0.004934616333580064 and total sample 1\n",
      "Bucket 5 sampled with ratio 0.009499136442141624 and total sample 1\n",
      "Bucket 6 sampled with ratio 0.005057981741919566 and total sample 1\n",
      "Bucket 7 sampled with ratio 0.011719713792252652 and total sample 1\n",
      "Bucket 8 sampled with ratio 0.010486059708857636 and total sample 1\n",
      "Bucket 9 sampled with ratio 0.013631877621514927 and total sample 1\n",
      "Bucket 10 sampled with ratio 0.003145817912657291 and total sample 1\n",
      "Bucket 11 sampled with ratio 0.0154206760424377 and total sample 1\n",
      "Bucket 12 sampled with ratio 0.0020355292376017763 and total sample 1\n",
      "Bucket 13 sampled with ratio 0.01005428077966938 and total sample 1\n",
      "Bucket 14 sampled with ratio 0.012583271650629163 and total sample 1\n",
      "Bucket 15 sampled with ratio 0.011102886750555145 and total sample 1\n",
      "Bucket 16 sampled with ratio 0.012398223538119911 and total sample 1\n",
      "Bucket 17 sampled with ratio 0.011226252158894647 and total sample 1\n",
      "Bucket 18 sampled with ratio 0.011411300271403899 and total sample 1\n",
      "Bucket 19 sampled with ratio 0.019059955588452998 and total sample 1\n",
      "Bucket 20 sampled with ratio 0.007340241796200345 and total sample 1\n",
      "Bucket 21 sampled with ratio 0.015790772267456205 and total sample 1\n",
      "Bucket 22 sampled with ratio 0.012953367875647668 and total sample 1\n",
      "Bucket 23 sampled with ratio 0.006538366641993585 and total sample 1\n",
      "Bucket 24 sampled with ratio 0.0014803849000740192 and total sample 1\n",
      "Bucket 25 sampled with ratio 0.00308413520848754 and total sample 1\n",
      "Bucket 26 sampled with ratio 0.0014187021959042685 and total sample 1\n",
      "Bucket 27 sampled with ratio 0.007895386133728102 and total sample 1\n",
      "Bucket 28 sampled with ratio 0.012891685171477917 and total sample 1\n",
      "Bucket 29 sampled with ratio 0.012706637058968665 and total sample 1\n",
      "Bucket 30 sampled with ratio 0.0005551443375277572 and total sample 1\n",
      "Bucket 31 sampled with ratio 0.011658031088082901 and total sample 1\n",
      "Bucket 32 sampled with ratio 0.007895386133728102 and total sample 1\n",
      "Bucket 33 sampled with ratio 0.010917838638045891 and total sample 1\n",
      "Bucket 34 sampled with ratio 0.009437453737971873 and total sample 1\n",
      "Bucket 35 sampled with ratio 0.009437453737971873 and total sample 1\n",
      "Bucket 36 sampled with ratio 0.01535899333826795 and total sample 1\n",
      "Bucket 37 sampled with ratio 0.010177646188008883 and total sample 1\n",
      "Bucket 38 sampled with ratio 0.009869232667160128 and total sample 1\n",
      "Bucket 39 sampled with ratio 0.013508512213175426 and total sample 1\n",
      "Bucket 40 sampled with ratio 0.014063656550703183 and total sample 1\n",
      "Bucket 41 sampled with ratio 0.011164569454724896 and total sample 1\n",
      "Bucket 42 sampled with ratio 0.010424377004687886 and total sample 1\n",
      "Bucket 43 sampled with ratio 0.009499136442141624 and total sample 1\n",
      "Bucket 44 sampled with ratio 0.013323464100666173 and total sample 1\n",
      "Bucket 45 sampled with ratio 0.006846780162842339 and total sample 1\n",
      "Bucket 46 sampled with ratio 0.0036392795460152973 and total sample 1\n",
      "Bucket 47 sampled with ratio 0.01147298297557365 and total sample 1\n",
      "Bucket 48 sampled with ratio 0.019800148038490006 and total sample 1\n",
      "Bucket 49 sampled with ratio 0.008080434246237354 and total sample 1\n",
      "Bucket 50 sampled with ratio 0.011658031088082901 and total sample 1\n",
      "Bucket 51 sampled with ratio 0.008943992104613866 and total sample 1\n",
      "Bucket 52 sampled with ratio 0.009067357512953367 and total sample 1\n",
      "Bucket 53 sampled with ratio 0.01307673328398717 and total sample 1\n",
      "Bucket 54 sampled with ratio 0.007957068837897853 and total sample 1\n",
      "Bucket 55 sampled with ratio 0.009807549962990377 and total sample 1\n",
      "Bucket 56 sampled with ratio 0.005674808783617074 and total sample 1\n",
      "Bucket 57 sampled with ratio 0.012459906242289662 and total sample 1\n",
      "Bucket 58 sampled with ratio 0.009930915371329879 and total sample 1\n",
      "Bucket 59 sampled with ratio 0.01301505057981742 and total sample 1\n",
      "Bucket 60 sampled with ratio 0.010239328892178634 and total sample 1\n",
      "Bucket 61 sampled with ratio 0.011966444608931655 and total sample 1\n",
      "Bucket 62 sampled with ratio 0.004132741179373304 and total sample 1\n",
      "Bucket 63 sampled with ratio 0.009005674808783616 and total sample 1\n",
      "Bucket 64 sampled with ratio 0.011904761904761904 and total sample 1\n",
      "Bucket 65 sampled with ratio 0.007895386133728102 and total sample 1\n",
      "Bucket 66 sampled with ratio 0.01634591660498396 and total sample 1\n",
      "Bucket 67 sampled with ratio 0.008142116950407105 and total sample 1\n",
      "Bucket 68 sampled with ratio 0.008203799654576857 and total sample 1\n",
      "Bucket 69 sampled with ratio 0.010362694300518135 and total sample 1\n",
      "Bucket 70 sampled with ratio 0.003762644954354799 and total sample 1\n",
      "Bucket 71 sampled with ratio 0.015729089563286454 and total sample 1\n",
      "Bucket 72 sampled with ratio 0.003700962250185048 and total sample 1\n",
      "Bucket 73 sampled with ratio 0.005304712558598569 and total sample 1\n",
      "Bucket 74 sampled with ratio 0.010424377004687886 and total sample 1\n",
      "Bucket 75 sampled with ratio 0.008018751542067604 and total sample 1\n",
      "Bucket 76 sampled with ratio 0.014248704663212436 and total sample 1\n",
      "Bucket 77 sampled with ratio 0.003947693066864051 and total sample 1\n",
      "Bucket 78 sampled with ratio 0.01307673328398717 and total sample 1\n",
      "Bucket 79 sampled with ratio 0.0046262028127313105 and total sample 1\n",
      "Bucket 80 sampled with ratio 0.010917838638045891 and total sample 1\n",
      "Bucket 81 sampled with ratio 0.014248704663212436 and total sample 1\n",
      "Bucket 82 sampled with ratio 0.008203799654576857 and total sample 1\n",
      "Bucket 83 sampled with ratio 0.012644954354798914 and total sample 1\n",
      "Bucket 84 sampled with ratio 0.007340241796200345 and total sample 1\n",
      "Bucket 85 sampled with ratio 0.01233654083395016 and total sample 1\n",
      "Bucket 86 sampled with ratio 0.004687885516901061 and total sample 1\n",
      "Bucket 87 sampled with ratio 0.013508512213175426 and total sample 1\n",
      "Bucket 88 sampled with ratio 0.008758943992104614 and total sample 1\n",
      "Bucket 89 sampled with ratio 0.01159634838391315 and total sample 1\n",
      "Bucket 90 sampled with ratio 0.012830002467308166 and total sample 1\n",
      "Bucket 91 sampled with ratio 0.00771033802121885 and total sample 1\n",
      "Bucket 92 sampled with ratio 0.010486059708857636 and total sample 1\n",
      "Bucket 93 sampled with ratio 0.014310387367382186 and total sample 1\n",
      "Bucket 94 sampled with ratio 0.011102886750555145 and total sample 1\n",
      "Bucket 95 sampled with ratio 0.012644954354798914 and total sample 1\n",
      "Bucket 96 sampled with ratio 0.009622501850481125 and total sample 1\n",
      "Bucket 97 sampled with ratio 0.013631877621514927 and total sample 1\n",
      "Bucket 98 sampled with ratio 0.01005428077966938 and total sample 1\n",
      "Bucket 99 sampled with ratio 0.007525289908709598 and total sample 1\n",
      "Sampled tweets saved to out/bucket-count/labelstudio-training-sampled.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# Parameters\n",
    "INPUT_FILE = INDOBERT_KMEANS_EMBED\n",
    "OUTPUT_FILE = 'out/bucket-count/labelstudio-training-sampled.json'\n",
    "\n",
    "TOTAL_SAMPLE = 100  # Change this as needed\n",
    "\n",
    "# Load data\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "bucket_content_count = defaultdict(int)\n",
    "bucket_ratio_count = defaultdict(float)\n",
    "\n",
    "total_tweet = 0\n",
    "for tweet in tweets:\n",
    "  label = tweet[\"bucket_label\"]\n",
    "  bucket_content_count[label] += 1\n",
    "  total_tweet += 1\n",
    "\n",
    "for label, bucket_tweet_count in bucket_content_count.items():\n",
    "  bucket_ratio_count[label] = bucket_tweet_count / total_tweet\n",
    "\n",
    "# Group tweets by bucket\n",
    "buckets = defaultdict(list)\n",
    "for tweet in tweets:\n",
    "  bucket_label = str(tweet[\"bucket_label\"])\n",
    "  buckets[bucket_label].append(tweet)\n",
    "\n",
    "# Sample tweets\n",
    "sampled_tweets = []\n",
    "for (bucket_label, tweets_in_bucket), ratio in zip(buckets.items(), bucket_ratio_count.values()):\n",
    "  # ratiod_total = math.ceil(TOTAL_SAMPLE * ratio)\n",
    "  ratiod_total = int(TOTAL_SAMPLE / bucket_content_count.__len__())\n",
    "  print(f\"Bucket {bucket_label} sampled with ratio {ratio} and total sample {ratiod_total}\")\n",
    "  if len(tweets_in_bucket) < ratiod_total:\n",
    "    print(f\"Warning: Bucket '{bucket_label}' has only {len(tweets_in_bucket)} tweets. Sampling all.\")\n",
    "    sampled = tweets_in_bucket\n",
    "  else:\n",
    "    sampled = random.sample(tweets_in_bucket, ratiod_total)\n",
    "  sampled_tweets.extend(sampled)\n",
    "\n",
    "# Save to output JSON\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "  json.dump(sampled_tweets, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Sampled tweets saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f2fc2",
   "metadata": {},
   "source": [
    "### Check for data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4a2b90d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No data leakage detected.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_ids_from_json_or_jsonl(file_path, id_key=\"tweet_id\"):\n",
    "    ids = set()\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        if first_char == \"[\":  # JSON array\n",
    "            data = json.load(f)\n",
    "            ids = {entry[id_key] for entry in data if id_key in entry}\n",
    "        else:  # JSONL\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    if id_key in obj:\n",
    "                        ids.add(obj[id_key])\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return ids\n",
    "\n",
    "def check_data_leakage(file1, file2, id_key=\"tweet_id\"):\n",
    "    ids_1 = load_ids_from_json_or_jsonl(file1, id_key)\n",
    "    ids_2 = load_ids_from_json_or_jsonl(file2, id_key)\n",
    "\n",
    "    intersection = ids_1 & ids_2\n",
    "\n",
    "    if intersection:\n",
    "        print(f\"⚠️ Data leakage detected! {len(intersection)} shared {id_key}s.\")\n",
    "    else:\n",
    "        print(\"✅ No data leakage detected.\")\n",
    "\n",
    "    return intersection\n",
    "\n",
    "# Example usage:\n",
    "file_a = \"out/labelstudio-training-sampled.json\"\n",
    "file_b = \"out/golden_standard.json\"\n",
    "leaked_ids = check_data_leakage(file_a, file_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bfc58",
   "metadata": {},
   "source": [
    "### Convert to a Label Studio Processable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e2cf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_label_studio_format(raw_data):\n",
    "    converted = []\n",
    "    for entry in raw_data:\n",
    "        new_entry = {\n",
    "            \"data\": {\n",
    "                \"text\": entry[\"content\"],\n",
    "                \"bucket_label\" : entry[\"bucket_label\"] if entry.get(\"bucket_label\") is not None else -10\n",
    "            },\n",
    "            \"meta\": {k: v for k, v in entry.items() if k != \"content\" and k != \"bucket_label\"}\n",
    "        }\n",
    "        converted.append(new_entry)\n",
    "    return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d4a9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/bucket-count/labelstudio-training-sampled.json\", \"r\") as file:\n",
    "  training_documents = json.load(file)\n",
    "parsed_training_documents = convert_to_label_studio_format(training_documents)\n",
    "with open(\"out/bucket-count/training_prepped.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(parsed_training_documents, file, ensure_ascii=False, indent=2)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
