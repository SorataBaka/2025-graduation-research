{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11887971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "OUT_DIR =\"OUT/\"\n",
    "ASSET_DIR=\"assets/\"\n",
    "with open(ASSET_DIR + \"dump-formatted.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "  RAW_DOCUMENTS = json.load(file)\n",
    "DOCUMENT_DF = pd.DataFrame.from_records(RAW_DOCUMENTS)\n",
    "DATA_LEN = len(DOCUMENT_DF)\n",
    "  \n",
    "INDOBERT_OUT_FILE = OUT_DIR + \"indobert_embeds.jsonl\"\n",
    "INDOBERTWEET_OUT_FILE = OUT_DIR + \"indobertweet_embeds.jsonl\"\n",
    "INDOBERT_REDUCED_OUT_FILE = OUT_DIR + \"indobert_reduced_embeds.jsonl\"\n",
    "TWEET_REDUCED_OUT_FILE = OUT_DIR + \"indobertweet_reduced_embeds.jsonl\"\n",
    "INDOBERT_NORMALIZED = OUT_DIR + \"indobert_normalized.jsonl\"\n",
    "TWEET_NORMALIZED = OUT_DIR + \"indobertweet_normalized.jsonl\"\n",
    "\n",
    "INDOBERT_KMEANS_EMBED = OUT_DIR + \"indobert/indobert-kmeans-embed.json\"\n",
    "INDOBERT_KMEANS_PROPS = OUT_DIR + \"model-agnostic/mixed-kmeans-props.json\"\n",
    "INDOBERT_KMEANS_CONCAT = OUT_DIR + \"indobert/indobert-kmeans-concat.json\"\n",
    "INDOBERTWEET_KMEANS_EMBED = OUT_DIR + \"indobertweet/indobertweet-kmeans-embed.json\"\n",
    "INDOBERTWEET_KMEANS_PROPS = OUT_DIR + \"model-agnostic/mixed-kmeans-props.json\"\n",
    "INDOBERTWEET_KMEANS_CONCAT = OUT_DIR + \"indobertweet/indobertweet-kmeans-concat.json\"\n",
    "\n",
    "INDOBERT_HDBSCAN_EMBED = OUT_DIR + \"indobert/indobert-hdbscan-embed.json\"\n",
    "INDOBERT_HDBSCAN_PROPS = OUT_DIR + \"model-agnostic/mixed-hdbscan-props.json\"\n",
    "INDOBERT_HDBSCAN_CONCAT = OUT_DIR + \"indobert/indobert-hdbscan-concat.json\"\n",
    "INDOBERTWEET_HDBSCAN_EMBED = OUT_DIR + \"indobertweet/indobertweet-hdbscan-embed.json\"\n",
    "INDOBERTWEET_HDBSCAN_PROPS = OUT_DIR + \"model-agnostic/mixed-hdbscan-props.json\"\n",
    "INDOBERTWEET_HDBSCAN_CONCAT = OUT_DIR + \"indobertweet/indobertweet-hdbscan-concat.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd9c36",
   "metadata": {},
   "source": [
    "### Text Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f687730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "# ─── pre-compiled patterns ────────────────────────────────────────────────\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_KUTI_BEF = re.compile(r'(?i)(?<!\\s)(kutipan)')\n",
    "_KUTI_AFT = re.compile(r'(?i)(kutipan)(?!\\s)')\n",
    "_REPEAT   = re.compile(r'(.)\\1{2,}')       # ≥3 of same char\n",
    "_WS       = re.compile(r'\\s+')\n",
    "\n",
    "def cleantext(text: str) -> str:\n",
    "    # 1 Unicode sanity\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "\n",
    "    # 2 Twitter cruft\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = _MENTION.sub(' ', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "\n",
    "    # 3 Space around “kutipan”\n",
    "    text = _KUTI_BEF.sub(r' \\1', text)\n",
    "    text = _KUTI_AFT.sub(r'\\1 ', text)\n",
    "\n",
    "    # 4 Emoji → text (optional: or .sub('', …) to drop)\n",
    "    text = emoji.demojize(text, delimiters=(' ', ' '))\n",
    "\n",
    "    # 5 Normalize repeats & whitespace\n",
    "    text = _REPEAT.sub(r'\\1\\1', text)            # clip to 2\n",
    "    text = _WS.sub(' ', text).strip().lower()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8466d",
   "metadata": {},
   "source": [
    "### Apply cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9663d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_DF[\"content\"] = DOCUMENT_DF[\"content\"].apply(cleantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77557b79",
   "metadata": {},
   "source": [
    "### Generate splits and golden standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372557d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "\n",
    "DOCUMENT_DF = DOCUMENT_DF.drop_duplicates(subset=[\"content\"]).reset_index(drop=True)\n",
    "TRAIN_DF, TEST_DF = train_test_split(\n",
    "  DOCUMENT_DF,\n",
    "  test_size=0.90,\n",
    "  random_state=42,\n",
    ")\n",
    "GOLDEN_STANDARD, UNUSED = train_test_split(\n",
    "  TEST_DF,\n",
    "  test_size=0.99,\n",
    "  random_state=42\n",
    ")\n",
    "print(len(TRAIN_DF))\n",
    "with open(\"out/golden_standard.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(GOLDEN_STANDARD.to_dict(orient=\"records\"), file, ensure_ascii=False, indent=2)\n",
    "with open(\"out/training_split_general.json\", \"w\") as file:\n",
    "  json.dump(TRAIN_DF.to_dict(orient=\"records\"),file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a05d56",
   "metadata": {},
   "source": [
    "### Initialize all models and tokenizers from IndoBERT and IndoBERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "162726b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "indobert_model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "indobert_tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "\n",
    "tweet_model = AutoModel.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "tweet_tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "indobert_model = indobert_model.to(device)\n",
    "tweet_model = tweet_model.to(device)\n",
    "\n",
    "#Turn on evaluation mode as default\n",
    "indobert_model.eval()\n",
    "tweet_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965c6ea",
   "metadata": {},
   "source": [
    "### Create functions to get encodings for both indobert and indobertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c31a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def getEncodings(textArray):\n",
    "  indobert_inputs = indobert_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  tweet_inputs = tweet_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  indobert_inputs = {k: v.to(\"mps\") for k, v in indobert_inputs.items()}\n",
    "  tweet_inputs = {k: v.to(\"mps\") for k, v in tweet_inputs.items()}\n",
    "  with torch.no_grad():\n",
    "    indobert_outputs = indobert_model(**indobert_inputs)\n",
    "    tweet_outputs = tweet_model(**tweet_inputs)\n",
    "    \n",
    "    \n",
    "  indobert_embeddings = indobert_outputs.last_hidden_state[:, 0, :]\n",
    "  tweet_embeddings = tweet_outputs.last_hidden_state[:, 0, :]\n",
    "  return (indobert_embeddings.cpu().numpy(), tweet_embeddings.cpu().numpy())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "148ebfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def removeFile(file_path):\n",
    "  if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f\"{file_path} deleted.\")\n",
    "  else:\n",
    "    print(f\"{file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258959a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "def get_batch_embeddings(documents, batch_size=32):\n",
    "  #Document is in the shape of a Pandas Dataframe. Convert to a list first before processing\n",
    "  removeFile(INDOBERT_OUT_FILE)\n",
    "  removeFile(INDOBERTWEET_OUT_FILE)\n",
    "  documents_list = documents.to_dict(orient=\"records\")\n",
    "  for i in tqdm(range(0, len(documents_list), batch_size), desc=\"Generating embeddings\"):\n",
    "    batched = documents_list[i:i+batch_size]\n",
    "    texts = [doc[\"content\"] for doc in batched]\n",
    "    indobert_embedding, tweet_embedding = getEncodings(texts)\n",
    "    \n",
    "    with open(INDOBERT_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, indobert_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")\n",
    "        \n",
    "    with open(INDOBERTWEET_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, tweet_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d84b2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_embeds.jsonl deleted.\n",
      "OUT/indobertweet_embeds.jsonl deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 507/507 [04:46<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "get_batch_embeddings(TRAIN_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5875d73",
   "metadata": {},
   "source": [
    "### Create function to reduce embed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04096a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "def reduce_embed_size(embeds):\n",
    "  umap_model = umap.UMAP(n_components=45)\n",
    "  reduced_embedding = umap_model.fit_transform(np.array(embeds))\n",
    "  return reduced_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80dedb8",
   "metadata": {},
   "source": [
    "### Process Indobert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae7ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(INDOBERT_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(INDOBERT_REDUCED_OUT_FILE)\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a2933",
   "metadata": {},
   "source": [
    "### Process IndoBERTweet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aece640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(INDOBERTWEET_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(TWEET_REDUCED_OUT_FILE)\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f4ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "try:\n",
    "  stopwords.words('english')\n",
    "except LookupError:\n",
    "  nltk.download('stopwords')\n",
    "stopwords_combined = set(stopwords.words(\"indonesian\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "def extract_structural_features(tweet):\n",
    "  words = tweet.split()\n",
    "  word_lengths = [len(w) for w in words]\n",
    "  \n",
    "  length = len(tweet)\n",
    "  num_hashtags = tweet.count(\"#\")\n",
    "  num_mentions = tweet.count(\"@\")\n",
    "  num_urls = len(re.findall(r\"http\\S+\", tweet))\n",
    "  num_emojis = len([c for c in tweet if c in emoji.EMOJI_DATA])\n",
    "  num_upper = sum(1 for c in tweet if c.isupper())\n",
    "  num_punct = len(re.findall(r\"[^\\w\\s]\", tweet))\n",
    "  avg_word_len = np.mean(word_lengths) if words else 0\n",
    "\n",
    "  # Content/structure-oriented features\n",
    "  is_question = int(tweet.strip().endswith('?'))\n",
    "  is_exclamatory = int(tweet.strip().endswith('!'))\n",
    "  contains_ellipsis = int(\"...\" in tweet)\n",
    "  contains_repeated_chars = int(bool(re.search(r\"(.)\\1{2,}\", tweet)))  # e.g., sooo, yessss\n",
    "  contains_short_link = int(bool(re.search(r\"\\b(?:https?:\\/\\/)?(?:www\\.)?(bit\\.ly|t\\.co|tinyurl\\.com|goo\\.gl|ow\\.ly|is\\.gd|buff\\.ly|adf\\.ly|bitly\\.com|cutt\\.ly|rb\\.gy|rebrand\\.ly)\\/[A-Za-z0-9]+\", tweet)))\n",
    "  contains_digit = int(bool(re.search(r\"\\d\", tweet)))\n",
    "  is_all_caps = int(tweet.isupper() and len(tweet) > 3)\n",
    "  is_emoji_only = int(all(c in emoji.EMOJI_DATA or c.isspace() for c in tweet.strip()) and tweet.strip() != \"\")\n",
    "  contains_quote_or_rt = int(bool(re.search(r\"(RT\\s@|\\\".+\\\")\", tweet)))\n",
    "  word_count = len(words)\n",
    "  stopword_ratio = np.mean([w.lower() in stopwords_combined for w in words]) if words else 0\n",
    "\n",
    "  return [\n",
    "    length, num_hashtags, num_mentions, num_urls,\n",
    "    num_emojis, num_upper, num_punct, avg_word_len,\n",
    "    is_question, is_exclamatory, contains_ellipsis,\n",
    "    contains_repeated_chars, contains_short_link,\n",
    "    contains_digit, is_all_caps, is_emoji_only,\n",
    "    contains_quote_or_rt, word_count, stopword_ratio\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9316084",
   "metadata": {},
   "source": [
    "### Generate structural features of each cleaned content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3c7ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"content\"]) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props) * 2])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(INDOBERT_NORMALIZED)\n",
    "with open(INDOBERT_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c335379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"content\"]) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props) * 2])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(TWEET_NORMALIZED)\n",
    "with open(TWEET_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d01c4",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobert\n",
    "1. KMeans + IndoBERT Embeddings\n",
    "2. KMeans + IndoBERT Embeddings + Structure Properties\n",
    "3. KMeans + Structure Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edb6699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clustered_docs(docs, cluster_labels, output_path):\n",
    "    for doc, label in zip(docs, cluster_labels):\n",
    "        doc[\"bucket_label\"] = int(label)\n",
    "    for doc in docs:\n",
    "        del doc[\"__v\"]\n",
    "        del doc[\"_id\"]\n",
    "        if \"embedding\" in doc:\n",
    "            del doc[\"embedding\"]\n",
    "        if \"structural_property\" in doc:\n",
    "            del doc[\"structural_property\"]\n",
    "        if \"concatenated_features\" in doc:\n",
    "            del doc[\"concatenated_features\"]\n",
    "            \n",
    "    # Sort by label for better organization (optional)\n",
    "    docs_sorted = sorted(docs, key=lambda x: x[\"bucket_label\"])\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs_sorted, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "264d73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16212 16212 16212\n"
     ]
    }
   ],
   "source": [
    "from sre_parse import Verbose\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "with open(INDOBERT_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=10, random_state=42).fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = KMeans(n_clusters=10, random_state=42).fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = KMeans(n_clusters=10, random_state=42).fit(properties)\n",
    "\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERT_KMEANS_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERT_KMEANS_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERT_KMEANS_PROPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1875ee",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobertweet\n",
    "1. KMeans + IndoBERTweet Embeddings\n",
    "2. KMeans + IndoBERTweet Embeddings + Structure Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b408120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16212 16212 16212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "with open(TWEET_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=10, random_state=42).fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = KMeans(n_clusters=10, random_state=42).fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = KMeans(n_clusters=10, random_state=42).fit(properties)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "assert len(labels_embed) == len(normalized_indobert_documents)\n",
    "\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERTWEET_KMEANS_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERTWEET_KMEANS_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERTWEET_KMEANS_PROPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e489d",
   "metadata": {},
   "source": [
    "### Utilize HDBSCAN and generate buckets on IndoBERT\n",
    "1. HDBSCAN + IndoBERT Embeddings\n",
    "2. HDBSCAN + IndoBERT Embeddings + Structure Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20be0838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16212 16212 16212\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import copy\n",
    "import hdbscan\n",
    "\n",
    "with open(INDOBERT_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = hdbscan.HDBSCAN(min_cluster_size=16, metric=\"euclidean\").fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = hdbscan.HDBSCAN(min_cluster_size=18, metric=\"euclidean\").fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = hdbscan.HDBSCAN(min_cluster_size=30, metric=\"euclidean\").fit(properties)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERT_HDBSCAN_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERT_HDBSCAN_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERT_HDBSCAN_PROPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9e59b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16212 16212 16212\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import copy\n",
    "import hdbscan\n",
    "\n",
    "with open(TWEET_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only #before waas  18\n",
    "kmeans_embed = hdbscan.HDBSCAN(min_cluster_size=18, metric=\"euclidean\").fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props #before was 17\n",
    "kmeans_concat = hdbscan.HDBSCAN(min_cluster_size=16, metric=\"euclidean\").fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = hdbscan.HDBSCAN(min_cluster_size=14, metric=\"euclidean\").fit(properties)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERTWEET_HDBSCAN_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERTWEET_HDBSCAN_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERTWEET_HDBSCAN_PROPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3dcdecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running silhouette score and gini coefficient tests...\n",
      "\n",
      "--- IndoBERT HDBSCAN + Concat ---\n",
      "OUT/indobert/indobert-hdbscan-concat.json Silhouette Score: 0.1020\n",
      "OUT/indobert/indobert-hdbscan-concat.json Gini Coefficient of cluster sizes: 0.8090\n",
      "Cluster counts: {0: 33, 1: 278, 2: 221, 3: 562, 4: 893, 5: 37, 6: 599, 7: 48, 8: 13087, 9: 45}\n",
      "\n",
      "--- IndoBERT HDBSCAN + Embed ---\n",
      "OUT/indobert/indobert-hdbscan-embed.json Silhouette Score: 0.2581\n",
      "OUT/indobert/indobert-hdbscan-embed.json Gini Coefficient of cluster sizes: 0.7043\n",
      "Cluster counts: {0: 33, 1: 221, 2: 939, 3: 15010}\n",
      "\n",
      "--- IndoBERT KMeans + Concat ---\n",
      "OUT/indobert/indobert-kmeans-concat.json Silhouette Score: 0.2530\n",
      "OUT/indobert/indobert-kmeans-concat.json Gini Coefficient of cluster sizes: 0.3117\n",
      "Cluster counts: {0: 774, 1: 2981, 2: 660, 3: 1743, 4: 2046, 5: 2386, 6: 236, 7: 2721, 8: 939, 9: 1726}\n",
      "\n",
      "--- IndoBERT KMeans + Embed ---\n",
      "OUT/indobert/indobert-kmeans-embed.json Silhouette Score: 0.3504\n",
      "OUT/indobert/indobert-kmeans-embed.json Gini Coefficient of cluster sizes: 0.2994\n",
      "Cluster counts: {0: 1080, 1: 2405, 2: 2308, 3: 939, 4: 1801, 5: 2007, 6: 2452, 7: 2673, 8: 514, 9: 33}\n",
      "\n",
      "--- IndoBERTweet HDBSCAN + Concat ---\n",
      "OUT/indobertweet/indobertweet-hdbscan-concat.json Silhouette Score: 0.0030\n",
      "OUT/indobertweet/indobertweet-hdbscan-concat.json Gini Coefficient of cluster sizes: 0.8429\n",
      "Cluster counts: {0: 21, 1: 44, 2: 282, 3: 288, 4: 555, 5: 14170, 6: 55, 7: 21, 8: 505, 9: 16}\n",
      "\n",
      "--- IndoBERTweet HDBSCAN + Embed ---\n",
      "OUT/indobertweet/indobertweet-hdbscan-embed.json Silhouette Score: 0.3112\n",
      "OUT/indobertweet/indobertweet-hdbscan-embed.json Gini Coefficient of cluster sizes: 0.8033\n",
      "Cluster counts: {0: 298, 1: 45, 2: 21, 3: 15305, 4: 33, 5: 493}\n",
      "\n",
      "--- IndoBERTweet KMeans + Concat ---\n",
      "OUT/indobertweet/indobertweet-kmeans-concat.json Silhouette Score: 0.2293\n",
      "OUT/indobertweet/indobertweet-kmeans-concat.json Gini Coefficient of cluster sizes: 0.2607\n",
      "Cluster counts: {0: 354, 1: 3030, 2: 2046, 3: 2055, 4: 1243, 5: 2491, 6: 1002, 7: 1431, 8: 1009, 9: 1551}\n",
      "\n",
      "--- IndoBERTweet KMeans + Embed ---\n",
      "OUT/indobertweet/indobertweet-kmeans-embed.json Silhouette Score: 0.4033\n",
      "OUT/indobertweet/indobertweet-kmeans-embed.json Gini Coefficient of cluster sizes: 0.2693\n",
      "Cluster counts: {0: 2301, 1: 818, 2: 548, 3: 2043, 4: 2416, 5: 2636, 6: 298, 7: 1463, 8: 2034, 9: 1655}\n",
      "\n",
      "--- KMeans + Properties ---\n",
      "OUT/model-agnostic/mixed-kmeans-props.json Silhouette Score: 0.2251\n",
      "OUT/model-agnostic/mixed-kmeans-props.json Gini Coefficient of cluster sizes: 0.6186\n",
      "Cluster counts: {0: 4279, 1: 3967, 2: 4867, 3: 1, 4: 3, 5: 2172, 6: 13, 7: 9, 8: 304, 9: 597}\n",
      "\n",
      "--- HDBSCAN + Properties ---\n",
      "OUT/model-agnostic/mixed-hdbscan-props.json Silhouette Score: 0.2566\n",
      "OUT/model-agnostic/mixed-hdbscan-props.json Gini Coefficient of cluster sizes: 0.7459\n",
      "Cluster counts: {0: 22, 1: 153, 2: 406, 3: 316, 4: 358, 5: 4654, 6: 9873, 7: 82, 8: 169}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def gini_coefficient(array):\n",
    "    \"\"\"Compute Gini coefficient of array of values.\"\"\"\n",
    "    # Based on mean absolute difference formula\n",
    "    array = np.array(array, dtype=np.float64)\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array)  # Ensure non-negative\n",
    "    array += 1e-10  # Avoid division by zero\n",
    "    array = np.sort(array)\n",
    "    n = array.size\n",
    "    cumvals = np.cumsum(array)\n",
    "    gini = (n + 1 - 2 * np.sum(cumvals) / cumvals[-1]) / n\n",
    "    return gini\n",
    "\n",
    "def evaluate_gini(bucketed):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    labels = labels_data[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    # Exclude noise points (-1)\n",
    "    valid_labels = labels[labels >= 0]\n",
    "\n",
    "    counts = pd.Series(valid_labels).value_counts().sort_index()\n",
    "    gini = gini_coefficient(counts.values)\n",
    "\n",
    "    print(f\"{bucketed} Gini Coefficient of cluster sizes: {gini:.4f}\")\n",
    "    print(f\"Cluster counts: {counts.to_dict()}\")\n",
    "\n",
    "def get_silhouette_score_embedding(bucketed, model_features):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    with open(model_features, \"r\") as file:\n",
    "        features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "    merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "    features_embedding = np.stack(merged_df[\"embedding\"].to_numpy())\n",
    "    labels = merged_df[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    embedding_score = silhouette_score(features_embedding, labels, metric='euclidean')\n",
    "    print(f\"{bucketed} Silhouette Score: {embedding_score:.4f}\")\n",
    "\n",
    "def get_silhouette_score_properties(bucketed, model_features):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    with open(model_features, \"r\") as file:\n",
    "        features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "    merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "    features_properties = np.stack(merged_df[\"structural_property\"].to_numpy())\n",
    "    labels = merged_df[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    # valid_mask = labels >= 0\n",
    "    # features_properties_valid = features_properties[valid_mask]\n",
    "    # labels_valid = labels[valid_mask]\n",
    "\n",
    "    properties_score = silhouette_score(features_properties, labels, metric='euclidean')\n",
    "    print(f\"{bucketed} Silhouette Score: {properties_score:.4f}\")\n",
    "\n",
    "def get_silhouette_score_concat(bucketed, model_features):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    with open(model_features, \"r\") as file:\n",
    "        features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "    merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "    features_concat = np.stack(merged_df[\"concatenated_features\"].to_numpy())\n",
    "    labels = merged_df[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    concat_score = silhouette_score(features_concat, labels, metric='euclidean')\n",
    "    print(f\"{bucketed} Silhouette Score: {concat_score:.4f}\")\n",
    "\n",
    "\n",
    "tests = [\n",
    "  [\"IndoBERT HDBSCAN + Concat\", INDOBERT_HDBSCAN_CONCAT, INDOBERT_NORMALIZED, get_silhouette_score_concat],\n",
    "  [\"IndoBERT HDBSCAN + Embed\",  INDOBERT_HDBSCAN_EMBED, INDOBERT_NORMALIZED, get_silhouette_score_embedding],\n",
    "  [\"IndoBERT KMeans + Concat\",  INDOBERT_KMEANS_CONCAT, INDOBERT_NORMALIZED, get_silhouette_score_concat],\n",
    "  [\"IndoBERT KMeans + Embed\",   INDOBERT_KMEANS_EMBED,  INDOBERT_NORMALIZED, get_silhouette_score_embedding],\n",
    "  [\"IndoBERTweet HDBSCAN + Concat\", INDOBERTWEET_HDBSCAN_CONCAT, TWEET_NORMALIZED, get_silhouette_score_concat],\n",
    "  [\"IndoBERTweet HDBSCAN + Embed\",  INDOBERTWEET_HDBSCAN_EMBED,  TWEET_NORMALIZED, get_silhouette_score_embedding],\n",
    "  [\"IndoBERTweet KMeans + Concat\",  INDOBERTWEET_KMEANS_CONCAT, TWEET_NORMALIZED, get_silhouette_score_concat],\n",
    "  [\"IndoBERTweet KMeans + Embed\",   INDOBERTWEET_KMEANS_EMBED,  TWEET_NORMALIZED, get_silhouette_score_embedding],\n",
    "  [\"KMeans + Properties\", INDOBERTWEET_KMEANS_PROPS, TWEET_NORMALIZED, get_silhouette_score_properties],\n",
    "  [\"HDBSCAN + Properties\", INDOBERTWEET_HDBSCAN_PROPS, TWEET_NORMALIZED, get_silhouette_score_properties]\n",
    "]\n",
    "\n",
    "print(\"Running silhouette score and gini coefficient tests...\\n\")\n",
    "for name, label_file, feature_file, scorer in tests:\n",
    "    print(f\"--- {name} ---\")\n",
    "    scorer(label_file, feature_file)\n",
    "    evaluate_gini(label_file)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b839114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Davies–Bouldin Index tests...\n",
      "\n",
      "OUT/indobert/indobert-hdbscan-concat.json DBI (Concat): 1.0673\n",
      "OUT/indobert/indobert-hdbscan-embed.json DBI (Embedding): 0.4672\n",
      "OUT/indobert/indobert-kmeans-concat.json DBI (Concat): 1.1807\n",
      "OUT/indobert/indobert-kmeans-embed.json DBI (Embedding): 0.9086\n",
      "OUT/indobertweet/indobertweet-hdbscan-concat.json DBI (Concat): 1.0770\n",
      "OUT/indobertweet/indobertweet-hdbscan-embed.json DBI (Embedding): 0.6202\n",
      "OUT/indobertweet/indobertweet-kmeans-concat.json DBI (Concat): 1.2954\n",
      "OUT/indobertweet/indobertweet-kmeans-embed.json DBI (Embedding): 0.9293\n",
      "OUT/model-agnostic/mixed-kmeans-props.json DBI (Properties): 1.0608\n",
      "OUT/model-agnostic/mixed-hdbscan-props.json DBI (Properties): 0.9238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# DBI SCORERS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def dbi_embedding(bucketed: str, model_features: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the Davies–Bouldin Index using only the embedding vectors.\n",
    "    \"\"\"\n",
    "    labels_df   = pd.read_json(bucketed)\n",
    "    labels_df[\"tweet_id\"] = labels_df[\"tweet_id\"].astype(str)\n",
    "    \n",
    "    features_df = pd.DataFrame([json.loads(line) for line in open(model_features)])\n",
    "\n",
    "    merged_df   = pd.merge(labels_df, features_df, on=\"tweet_id\", how=\"inner\")\n",
    "    X           = np.stack(merged_df[\"embedding\"].to_numpy())\n",
    "    y           = merged_df[\"bucket_label\"]\n",
    "\n",
    "    score = davies_bouldin_score(X, y)\n",
    "    print(f\"{bucketed} DBI (Embedding): {score:.4f}\")\n",
    "\n",
    "def dbi_properties(bucketed: str, model_features: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the Davies–Bouldin Index on structural-property feature vectors.\n",
    "    \"\"\"\n",
    "    labels_df   = pd.read_json(bucketed)\n",
    "    labels_df[\"tweet_id\"] = labels_df[\"tweet_id\"].astype(str)\n",
    "    features_df = pd.DataFrame([json.loads(line) for line in open(model_features)])\n",
    "\n",
    "    merged_df   = pd.merge(labels_df, features_df, on=\"tweet_id\", how=\"inner\")\n",
    "    X           = np.stack(merged_df[\"structural_property\"].to_numpy())\n",
    "    y           = merged_df[\"bucket_label\"]\n",
    "\n",
    "    score = davies_bouldin_score(X, y)\n",
    "    print(f\"{bucketed} DBI (Properties): {score:.4f}\")\n",
    "\n",
    "def dbi_concat(bucketed: str, model_features: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the Davies–Bouldin Index on concatenated feature vectors.\n",
    "    \"\"\"\n",
    "    labels_df   = pd.read_json(bucketed)\n",
    "    labels_df[\"tweet_id\"] = labels_df[\"tweet_id\"].astype(str)\n",
    "    features_df = pd.DataFrame([json.loads(line) for line in open(model_features)])\n",
    "\n",
    "    merged_df   = pd.merge(labels_df, features_df, on=\"tweet_id\", how=\"inner\")\n",
    "    X           = np.stack(merged_df[\"concatenated_features\"].to_numpy())\n",
    "    y           = merged_df[\"bucket_label\"]\n",
    "\n",
    "    score = davies_bouldin_score(X, y)\n",
    "    print(f\"{bucketed} DBI (Concat): {score:.4f}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# TEST MATRIX\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "tests = [\n",
    "    [\"IndoBERT HDBSCAN + Concat\",  INDOBERT_HDBSCAN_CONCAT,  INDOBERT_NORMALIZED, dbi_concat],\n",
    "    [\"IndoBERT HDBSCAN + Embed\",   INDOBERT_HDBSCAN_EMBED,   INDOBERT_NORMALIZED, dbi_embedding],\n",
    "    [\"IndoBERT KMeans  + Concat\",  INDOBERT_KMEANS_CONCAT,   INDOBERT_NORMALIZED, dbi_concat],\n",
    "    [\"IndoBERT KMeans  + Embed\",   INDOBERT_KMEANS_EMBED,    INDOBERT_NORMALIZED, dbi_embedding],\n",
    "    [\"IndoBERTweet HDBSCAN + Concat\", INDOBERTWEET_HDBSCAN_CONCAT, TWEET_NORMALIZED, dbi_concat],\n",
    "    [\"IndoBERTweet HDBSCAN + Embed\",  INDOBERTWEET_HDBSCAN_EMBED,  TWEET_NORMALIZED, dbi_embedding],\n",
    "    [\"IndoBERTweet KMeans  + Concat\", INDOBERTWEET_KMEANS_CONCAT,  TWEET_NORMALIZED, dbi_concat],\n",
    "    [\"IndoBERTweet KMeans  + Embed\",  INDOBERTWEET_KMEANS_EMBED,   TWEET_NORMALIZED, dbi_embedding],\n",
    "    [\"KMeans  + Properties\",          INDOBERTWEET_KMEANS_PROPS,   TWEET_NORMALIZED, dbi_properties],\n",
    "    [\"HDBSCAN + Properties\",          INDOBERTWEET_HDBSCAN_PROPS,  TWEET_NORMALIZED, dbi_properties],\n",
    "]\n",
    "\n",
    "print(\"Running Davies–Bouldin Index tests...\\n\")\n",
    "for name, label_file, feature_file, scorer in tests:\n",
    "    scorer(label_file, feature_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c1606",
   "metadata": {},
   "source": [
    "### Do a bit of bucket analization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18d9ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from collections import Counter\n",
    "import re\n",
    "def count_hashtags(text):\n",
    "    return len(re.findall(r\"#\\w+\", text))\n",
    "\n",
    "def hashtag_ratio(text):\n",
    "    hashtags = ''.join(re.findall(r\"#\\w+\", text))\n",
    "    return len(hashtags) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\"\n",
    "                               \"\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "def emoji_ratio(text):\n",
    "    emojis = extract_emojis(text)\n",
    "    return len(''.join(emojis)) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def url_ratio(tweets):\n",
    "    return sum(1 for t in tweets if re.search(r\"http\\S+\", t)) / len(tweets)\n",
    "\n",
    "def mention_ratio(tweets):\n",
    "    return sum(1 for t in tweets if re.search(r\"@\\w+\", t)) / len(tweets)\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "def repeated_char_abuse(text):\n",
    "    return bool(re.search(r\"(.)\\1{3,}\", text))\n",
    "\n",
    "def duplicate_ratio(tweets):\n",
    "    freq = Counter(tweets)\n",
    "    return sum(count for tweet, count in freq.items() if count > 1) / len(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8f84332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0: long tweets\n",
      "  avg_hashtags: 3.163407214254672\n",
      "  avg_length: 223.52411994784876\n",
      "  longest_tweet: timnya pak kenapa ga nahan ambulan jalan depan iringan presiden? tapi kok pas demonstrasi malah berani ambulan diblokir jalannya? kutipan thendons marakun parodirespect moment! ketika iring-iringan ri 1 presiden prabowo subianto mendahulukan ambulans terlebih dahulu melaju sebagai prioritas utama.pembaca menambahkan konteksberdasarkan uu no 22 thn 2009 ttng lalu lintas dan angkutan jalan (llaj) dalam pasal 134. ambulans yang berisi orang sakit ada di urutan kedua sedangkan kendaraan presiden ada di urutan ke 4. jadi tindakan ini bkn suatu pencapaian tapi kewajiban. google.com/url?sa=t&sourc..apakah ini membantu?beri nilai\n",
      "  shortest_tweet: preach\n",
      "  hashtag_ratio: 0.22738051594104797\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.000434593654932638\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9382618618995452\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 1: long tweets\n",
      "  avg_hashtags: 2.667481662591687\n",
      "  avg_length: 204.47310513447434\n",
      "  longest_tweet: buzzers and government lickers ass are confused by massive protests in foreign media. they think it's about cash, but we're self-funded! we make posters/stickers, provide food & first aid. join us! online support fuels on-ground efforts. together, we fight! #indonesiagelap kutipan bareng warga - #indonesiagelap febbuzzer pada bingung, kok bisa demo serentak di berbagai daerah, kok bisa rame, kok trending terus, kok media luar bisa coverage. bingung soalnya mereka mereka ini taunya semua hal itu transaksional pake duit. mari kita buat mereka bingung terus, swadaya design poster, cetaktampilkan lebih banyak\n",
      "  shortest_tweet: omg\n",
      "  hashtag_ratio: 0.1686286060615403\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9349519289491705\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 2: generic\n",
      "  avg_hashtags: 0.9653284671532847\n",
      "  avg_length: 194.71167883211677\n",
      "  longest_tweet: lautan manusia turun ke jalan di tel aviv melakukan demonstrasi usai tiga tewasnya sandera di jalur gaza di tangan tentara idf tiga orang israel tewas itu disebut salah diidentifikasi, dirasa menjadi ancaman oleh tentara idf yang beroperasi di shujaiya, gaza utara kutipan nur sabanah des 2023 protes di tel aviv setelah israel mengaku membunuh 3 sandera israel pasukan pertahanan israel mengonfirmasi tiga sandera dibunuh oleh tentaranya setelah salah diidentifikasi sebagai ancaman.. pm netanyahu telah berbicara tentang penembakan itu pada konferensi pers x.com/nursyahbana9/s..\n",
      "  shortest_tweet: demonstrasi di new york\n",
      "  hashtag_ratio: 0.07139588709743729\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9140125371546699\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 3: long tweets\n",
      "  avg_hashtags: 0.9045521292217328\n",
      "  avg_length: 201.136563876652\n",
      "  longest_tweet: ᅠ ᅠ nathan menyantap roti lapis buatan profesor dunford dan kak cecil sambil menyimak dengan saksama demonstrasi yang diberikan sang pengajar. jujur, ada sedikit kekhawatiran—pertemuan lalu saja, ia masih kesulitan hanya untuk terbang biasa. ( <url> ᅠ ᅠ kutipan yolanne meiakun parodimembalas dan “teknik pertama yang ingin saya demonstrasikan adalah fast laps.” ia meloncat ke atas sapunya sembari beterbangan mengelilingi para murid. “fast laps pada dasarnya adalah terbang dengan kecepatan di atas rata-rata.” setelah menjelaskan, ia memacu sapunya secara cepat menjauhtampilkan lebih banyak\n",
      "  shortest_tweet: 19\n",
      "  hashtag_ratio: 0.06435416784985663\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9051360605430546\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 4: generic\n",
      "  avg_hashtags: 0.40521523178807944\n",
      "  avg_length: 136.0413907284768\n",
      "  longest_tweet: sakit hati banget gue bacanya tolol aparat tolol. jadi perempuan bisa kena double bahkan triple hits sama lu semua. ditangkepin pas lagi demonstrasi, jadi objek seksual bisa diperkosa ditelanjangin sesuka lu semua. kontol. semoga lu semua membusuk di neraka. gue benci lu semua kutipan jorgiana au. meiawalnya gue mau becanda kayak gini aja, tapi pas ngeliat video itu dan baru ngeh kalo ada yang nyuruh nelanjangin gua and actually try to do it, sakit banget rasanya. mereka gak tau siapa gua, cuman tau gue perempuan, dan itulah satu-satunya alasan kenapa mereka melakukan itu. x.com/jorgianaa/sta..\n",
      "  shortest_tweet: iya\n",
      "  hashtag_ratio: 0.03798560337447939\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.94633127244134\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 5: hashtag-heavy\n",
      "  avg_hashtags: 6.492792109256449\n",
      "  avg_length: 151.73672230652502\n",
      "  longest_tweet: guyss liat yg ini juga, kasian bgt ibuknya ampe gemeter saking takutnya, untung ibu' nya ga terlalu parah.. sehat' yaa ibukk #gagalkanuutni #cabutuutni #tolakuutni #tolakrevisiuutni #indonesiagelap #tolakdwifungsiabri #tolakruupolri #tolakruukejaksaan kutipan dipi. rest dlu markasian bgt yaallah ibunya ampe gemeter gtu ketakutan, pgn nangis gua anj.. ni skrg lgi nangis brengsek ke inget nenek gua yg umurnya segitu #gagalkanuutni #cabutuutni #tolakuutni #tolakrevisiuutni #peringatandarurat #tolakdwifungsiabri #tolakruupolri #tolakruukejaksaan x.com/qancutt/status..\n",
      "  shortest_tweet: susuu!!\n",
      "  hashtag_ratio: 0.6399984688874251\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9635790880171533\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 2\n",
      "\n",
      "Cluster 6: long tweets\n",
      "  avg_hashtags: 0.47651006711409394\n",
      "  avg_length: 212.09395973154363\n",
      "  longest_tweet: sebabtu malaysia masih di takuk yang lama. sebab ada orang mcm kau lah. tak nak adapt dengan perubahan. apa yg kau lalui tak semestinya orang lain kena ikut. yang memekak diorang bukan kau. takda paksa kau ikut demonstrasi pon. kutipan syed azharul asriq mar 2024 kenapa budak universiti banyak memekak? sebab budak universiti belum tahu realiti dunia sebenar. diorang belum tahu realiti industri. realiti masyarakat. hidup diorang belum ada lagi watak yang dipanggil \"bos\". nanti dah kena hentak dengan realiti, senyap la budak-budak ni. x.com/youngaljunied/..\n",
      "  shortest_tweet: demonstrasi sunatan\n",
      "  hashtag_ratio: 0.025375590778037176\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9177698750300946\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 7: long tweets\n",
      "  avg_hashtags: 0.5960355434039645\n",
      "  avg_length: 271.5871496924128\n",
      "  longest_tweet: buat yang suka demo atau protes, mending kalian serang yang ini. ada puluhan ribu triliun di situ. rebut, kaya jaman penjajahan belanda duludaftar perusahaan asing yang menguasai sda indonesia dan jumlah keuntungannyaanswer by grokberikut adalah daftar beberapa perusahaan asing yang mengelola sumber daya alam (sda) di indonesia, khususnya di sektor pertambangan dan migas, berdasarkan informasi yang tersedia. namun, data spesifik mengenai jumlah keuntungan sering kali tidak diungkap secara rinci dalam sumber publik karena bersifat rahasia perusahaan atau hanya dilaporkan secara agregat. saya akan mencantumkan perusahaan-perusahaan yang dikenal memiliki peran besar dalam pengelolaan sda indonesia beserta informasi keuntungan yang tersedia dari sumber terpercaya. jika data keuntungan tidak tersedia, saya akan menjelaskan kontribusi atau operasi mereka. pt freeport indonesia (freeport-mcmoran, amerika serikat) sektor: tambang emas, tembaga, dan perak di grasberg, papua. operasi: mengelola salah satu tambang emas dan tembaga terbesar di dunia. freeport indonesia telah beroperasi sejak 1967. pemerintah indonesia melalui inalum memiliki 51,23% saham sejak 2018, sedangkan freeport-mcmoran memiliki sisanya. keuntungan: laporan spesifik tentang keuntungan tahunan freeport indonesia sulit didapat karena data keuangan biasanya dilaporkan oleh freeport-mcmoran secara global. pada 2012, pendapatan bersih freeport-mcmoran secara global mencapai miliaran dolar, tetapi tidak ada angka spesifik untuk indonesia. freeport mengalokasikan investasi besar, seperti $17,3 miliar untuk pengembangan tambang bawah tanah dan smelter. catatan: meskipun indonesia kini memiliki mayoritas saham, sebagian besar hasil tambang diekspor sebagai konsentrat, dan keuntungan besar masih mengalir ke pihak asing sebelum divestasi. chevron corporation (amerika serikat) sektor: minyak dan gas (migas). operasi: melalui pt chevron pacific indonesia, chevron mengelola blok rokan dan siak di sumatra (produksi minyak terbesar di indonesia, sekitar 35-40% dari total produksi nasional) serta blok migas lepas pantai di kalimantan timur dan papua. keuntungan: tidak ada data spesifik keuntungan tahunan chevron pacific indonesia di sumber yang tersedia. namun, sebagai produsen minyak mentah terbesar di indonesia, kontribusinya signifikan terhadap devisa ekspor migas. chevron juga mengelola proyek geothermal di gunung salak, jawa barat. catatan: chevron telah beroperasi di indonesia sejak 1952 (dulunya caltex). total e&p indonesie (totalenergies, prancis) sektor: migas. operasi: mengelola blok mahakam di kalimantan timur, salah satu blok gas terbesar di indonesia. keuntungan: data keuntungan spesifik untuk total e&p indonesie tidak tersedia dalam sumber. namun, blok mahakam menghasilkan gas alam dalam jumlah besar, yang sebagian diekspor. catatan: totalenergies bekerja sama dengan pertamina dalam pengelolaan blok ini, tetapi keuntungan besar mengalir ke perusahaan asing melalui bagi hasil. british petroleum (bp, inggris) sektor: migas. operasi: menguasai 37,16% saham di proyek tangguh, papua, yang menghasilkan 7,6 juta ton lng per tahun, sebagian diekspor ke amerika serikat dan china. bp juga mengelola blok migas baru melalui kontrak bagi hasil. keuntungan: tidak ada data spesifik keuntungan proyek tangguh. namun, cadangan gas sebesar 14,4 triliun kaki kubik menunjukkan skala operasi yang besar dan profitabilitas tinggi. catatan: bp berjanji mengalokasikan sebagian gas untuk kebutuhan domestik (pln) setelah pengembangan kilang lng train 3 dan 4. exxonmobil (amerika serikat) sektor: migas. operasi: melalui mobil cepu ltd, exxonmobil mengelola blok cepu di jawa tengah, dengan cadangan minyak mentah 1,4 miliar barel dan gas 8,14 miliar kaki kubik (lapangan banyu urip). keuntungan: tidak ada data keuntungan spesifik untuk indonesia. namun, skala cadangan menunjukkan potensi keuntungan besar. catatan: exxonmobil mengambil alih blok ini dari royal dutch shell dan bekerja sama dengan pertamina. pt newmont nusa tenggara (newmont corporation, amerika serikat) sektor: tambang emas dan tembaga. operasi: mengelola tambang emas dan tembaga di nusa tenggara timur (ntt) dan nusa tenggara barat (ntb), terutama di tambang batu hijau. keuntungan: pada 2012, newmont mencatat pendapatan bersih $4,17 juta (sekitar rp58 miliar) dari operasi di ntb. catatan: newmont telah menghadapi kritik karena dampak lingkungan, seperti limbah tailing di sungai-sungai papua. niko resources (kanada) sektor: migas. operasi: mengelola 12 blok migas di indonesia, dengan 58% lahan eksplorasinya berada di indonesia. keuntungan: tidak ada data keuntungan spesifik dalam sumber. catatan: fokus pada pengeboran lepas pantai, bekerja sama dengan perusahaan lokal seperti indika energy. perusahaan tambang batubara (berau coal, rognar holding b.v, belanda; sojitz corp, jepang) sektor: batubara. operasi: mengoperasikan tambang batubara binunan lati dan sambarata di kalimantan timur, menghasilkan batubara termal dan metalurgi. keuntungan: tidak ada data keuntungan spesifik. catatan: berau coal bekerja sama dengan perusahaan indonesia, tetapi saham mayoritas dipegang asing. essar steel (india, terdaftar di inggris) sektor: batubara dan baja. operasi: menguasai tambang batubara aries di indonesia untuk mendukung pembangkit listrik dan produksi baja. menguasai 35% pangsa pasar baja domestik di indonesia. keuntungan: tidak ada data spesifik. catatan: fokus pada pengamanan pasokan batubara untuk operasi mereka. gmr energy (india) sektor: batubara. operasi: membeli 100% saham pt barasentosa lestari, yang mengelola dua blok batubara di sumatra selatan. keuntungan: tidak ada data spesifik. catatan: bagian dari strategi gmr untuk mendukung pembangkit listrik global mereka. catatan penting: keterbatasan data keuntungan**: banyak perusahaan asing tidak merilis data keuntungan spesifik per negara karena pelaporan keuangan dilakukan secara global atau melalui anak perusahaan. keuntungan sering kali dihitung dalam bagi hasil dengan pemerintah indonesia (melalui kontrak karya atau psc untuk migas). dampak dan kritik**: pengelolaan sda oleh perusahaan asing sering dikritik karena eksploitasi berlebihan, pencemaran lingkungan (misalnya limbah tailing), dan keuntungan yang tidak sebanding dengan kesejahteraan masyarakat lokal. uu no. 4 tahun 2009 tentang pertambangan mineral dan batubara mewajibkan perusahaan asing membangun smelter untuk mengolah bahan mentah di dalam negeri, tetapi implementasinya masih terbatas. regulasi**: pasal 33 uud 1945 menegaskan bahwa sda harus dikuasai negara untuk kemakmuran rakyat. pemerintah indonesia telah mendorong divestasi saham (misalnya freeport) dan melarang ekspor mineral mentah untuk meningkatkan nilai tambah dalam negeri. saran: untuk data keuntungan yang lebih akurat, anda dapat merujuk laporan tahunan perusahaan di bursa saham (jika publik) atau laporan badan koordinasi penanaman modal (bkpm). sumber seperti kementerian esdm atau skk migas juga dapat memberikan gambaran lebih rinci tentang bagi hasil migas. jika anda ingin informasi lebih spesifik tentang perusahaan tertentu, silakan beri tahu, dan saya bisa mencoba mencari data tambahan!tampilkan lebih banyaktanya langsung pada grok\n",
      "  shortest_tweet: okelah jika demikian\n",
      "  hashtag_ratio: 0.03487811635968433\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9146145709019314\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 8: generic\n",
      "  avg_hashtags: 0.30039331366764993\n",
      "  avg_length: 162.36971484759096\n",
      "  longest_tweet: pertama gw disclaimer dulu: -gw bukan buzzerp rezim -gw tidak mendukung ruu tni, ruu polri atau upaya apapun itu yang ingin membangkitkan kembali orba. tapi ya bener, semasif-masifnya demo yg kalian gelar, kalian nggak bisa dan nggak berhak nyetir dan ngatur2 media mainstream. kutipan bareng warga - #indonesiagelap marterlepas dari benar atau tidaknya ancaman ini, apakah aksi massa beberapa hari terakhir disiarkan di tv nasional atau enggaknya itu di luar kendali kita, temen-temen. yang bisa kita lakukan adalah, kita fokus melakukan apa yang ada dalam kendali kita. apa aja tuh? 1. turun aksi x.com/milkachew/stat..tampilkan lebih banyak\n",
      "  shortest_tweet: iirc\n",
      "  hashtag_ratio: 0.0194426038096141\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.944697412485497\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 9: hashtag-heavy\n",
      "  avg_hashtags: 2.708761329305136\n",
      "  avg_length: 103.97462235649547\n",
      "  longest_tweet: #indonesiagelap #prabowotolol #adilijokowi #ndasmu #darkindonesia kutipan manto gudono sk febhahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahatampilkan lebih banyak\n",
      "  shortest_tweet: day6\n",
      "  hashtag_ratio: 0.4223579473421474\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9625720806913991\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "from collections import defaultdict\n",
    "import json\n",
    "buckets = defaultdict(list)\n",
    "\n",
    "with open(INDOBERTWEET_KMEANS_EMBED, \"r\") as file:\n",
    "  documents = json.load(file)\n",
    "\n",
    "for doc in documents:\n",
    "  buckets[doc[\"bucket_label\"]].append(doc)\n",
    "\n",
    "for label, bucket_tweets in buckets.items():\n",
    "    bucket_tweets = [doc[\"content\"] for doc in bucket_tweets]\n",
    "    \n",
    "    hashtag_counts = [count_hashtags(t) for t in bucket_tweets]\n",
    "    tweet_lengths = [len(t) for t in bucket_tweets]\n",
    "    hashtag_ratios = [hashtag_ratio(t) for t in bucket_tweets]\n",
    "    emoji_ratios = [emoji_ratio(t) for t in bucket_tweets]\n",
    "    lexical_divs = [lexical_diversity(t) for t in bucket_tweets]\n",
    "    repeated_abuse_count = sum(1 for t in bucket_tweets if repeated_char_abuse(t))\n",
    "\n",
    "    avg_hashtags = sum(hashtag_counts) / len(bucket_tweets)\n",
    "    avg_length = sum(tweet_lengths) / len(bucket_tweets)\n",
    "    avg_hashtag_ratio = sum(hashtag_ratios) / len(bucket_tweets)\n",
    "    avg_emoji_ratio = sum(emoji_ratios) / len(bucket_tweets)\n",
    "    avg_lexical_div = sum(lexical_divs) / len(bucket_tweets)\n",
    "    url_ratio_val = url_ratio(bucket_tweets)\n",
    "    mention_ratio_val = mention_ratio(bucket_tweets)\n",
    "    dup_ratio = duplicate_ratio(bucket_tweets)\n",
    "\n",
    "    # Heuristic label tagging\n",
    "    label_tags = []\n",
    "    if avg_length > 200:\n",
    "        label_tags.append(\"long tweets\")\n",
    "    if avg_hashtag_ratio > 0.4:\n",
    "        label_tags.append(\"hashtag-heavy\")\n",
    "    if avg_emoji_ratio > 0.2:\n",
    "        label_tags.append(\"emoji spam\")\n",
    "    if url_ratio_val > 0.3:\n",
    "        label_tags.append(\"link drop\")\n",
    "    if mention_ratio_val > 0.3:\n",
    "        label_tags.append(\"mention spam\")\n",
    "    if avg_lexical_div < 0.4:\n",
    "        label_tags.append(\"low diversity (copypasta)\")\n",
    "    if repeated_abuse_count / len(bucket_tweets) > 0.3:\n",
    "        label_tags.append(\"repeated char abuse\")\n",
    "    if dup_ratio > 0.3:\n",
    "        label_tags.append(\"high duplication\")\n",
    "\n",
    "    longest = max(bucket_tweets, key=len)\n",
    "    shortest = min(bucket_tweets, key=len)\n",
    "\n",
    "    results[label] = {\n",
    "        \"label\": \", \".join(label_tags) if label_tags else \"generic\",\n",
    "        \"avg_hashtags\": avg_hashtags,\n",
    "        \"avg_length\": avg_length,\n",
    "        \"longest_tweet\": longest,\n",
    "        \"shortest_tweet\": shortest,\n",
    "        \"hashtag_ratio\": avg_hashtag_ratio,\n",
    "        \"emoji_ratio\": avg_emoji_ratio,\n",
    "        \"url_ratio\": url_ratio_val,\n",
    "        \"mention_ratio\": mention_ratio_val,\n",
    "        \"lexical_diversity\": avg_lexical_div,\n",
    "        \"duplication_ratio\": dup_ratio,\n",
    "        \"repeated_char_abuse_count\": repeated_abuse_count,\n",
    "    }\n",
    "\n",
    "# Output the results\n",
    "for label, metrics in results.items():\n",
    "    print(f\"\\nCluster {label}: {metrics['label']}\")\n",
    "    for k, v in metrics.items():\n",
    "        if k != 'label':\n",
    "            print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e65716c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0 sampled with ratio 0.1419319022945966 and total sample 100\n",
      "Bucket 1 sampled with ratio 0.050456452010856155 and total sample 100\n",
      "Bucket 2 sampled with ratio 0.03380212188502344 and total sample 100\n",
      "Bucket 3 sampled with ratio 0.12601776461880088 and total sample 100\n",
      "Bucket 4 sampled with ratio 0.14902541327411795 and total sample 100\n",
      "Bucket 5 sampled with ratio 0.16259560819146313 and total sample 100\n",
      "Bucket 6 sampled with ratio 0.01838144584258574 and total sample 100\n",
      "Bucket 7 sampled with ratio 0.09024179620034542 and total sample 100\n",
      "Bucket 8 sampled with ratio 0.12546262028127314 and total sample 100\n",
      "Bucket 9 sampled with ratio 0.10208487540093758 and total sample 100\n",
      "Sampled tweets saved to out/labelstudio-training-sampled.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# Parameters\n",
    "INPUT_FILE = INDOBERTWEET_KMEANS_EMBED\n",
    "OUTPUT_FILE = 'out/labelstudio-training-sampled.json'\n",
    "\n",
    "TOTAL_SAMPLE = 1000  # Change this as needed\n",
    "\n",
    "# Load data\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "bucket_content_count = defaultdict(int)\n",
    "bucket_ratio_count = defaultdict(float)\n",
    "\n",
    "total_tweet = 0\n",
    "for tweet in tweets:\n",
    "  label = tweet[\"bucket_label\"]\n",
    "  bucket_content_count[label] += 1\n",
    "  total_tweet += 1\n",
    "\n",
    "for label, bucket_tweet_count in bucket_content_count.items():\n",
    "  bucket_ratio_count[label] = bucket_tweet_count / total_tweet\n",
    "\n",
    "# Group tweets by bucket\n",
    "buckets = defaultdict(list)\n",
    "for tweet in tweets:\n",
    "  bucket_label = str(tweet[\"bucket_label\"])\n",
    "  buckets[bucket_label].append(tweet)\n",
    "\n",
    "# Sample tweets\n",
    "sampled_tweets = []\n",
    "for (bucket_label, tweets_in_bucket), ratio in zip(buckets.items(), bucket_ratio_count.values()):\n",
    "  # ratiod_total = math.ceil(TOTAL_SAMPLE * ratio)\n",
    "  ratiod_total = int(TOTAL_SAMPLE / bucket_content_count.__len__())\n",
    "  print(f\"Bucket {bucket_label} sampled with ratio {ratio} and total sample {ratiod_total}\")\n",
    "  if len(tweets_in_bucket) < ratiod_total:\n",
    "    print(f\"Warning: Bucket '{bucket_label}' has only {len(tweets_in_bucket)} tweets. Sampling all.\")\n",
    "    sampled = tweets_in_bucket\n",
    "  else:\n",
    "    sampled = random.sample(tweets_in_bucket, ratiod_total)\n",
    "  sampled_tweets.extend(sampled)\n",
    "\n",
    "# Save to output JSON\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "  json.dump(sampled_tweets, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Sampled tweets saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f2fc2",
   "metadata": {},
   "source": [
    "### Check for data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a2b90d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No data leakage detected.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_ids_from_json_or_jsonl(file_path, id_key=\"tweet_id\"):\n",
    "    ids = set()\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        if first_char == \"[\":  # JSON array\n",
    "            data = json.load(f)\n",
    "            ids = {entry[id_key] for entry in data if id_key in entry}\n",
    "        else:  # JSONL\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    if id_key in obj:\n",
    "                        ids.add(obj[id_key])\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return ids\n",
    "\n",
    "def check_data_leakage(file1, file2, id_key=\"tweet_id\"):\n",
    "    ids_1 = load_ids_from_json_or_jsonl(file1, id_key)\n",
    "    ids_2 = load_ids_from_json_or_jsonl(file2, id_key)\n",
    "\n",
    "    intersection = ids_1 & ids_2\n",
    "\n",
    "    if intersection:\n",
    "        print(f\"⚠️ Data leakage detected! {len(intersection)} shared {id_key}s.\")\n",
    "    else:\n",
    "        print(\"✅ No data leakage detected.\")\n",
    "\n",
    "    return intersection\n",
    "\n",
    "# Example usage:\n",
    "file_a = \"out/labelstudio-training-sampled.json\"\n",
    "file_b = \"out/golden_standard.json\"\n",
    "leaked_ids = check_data_leakage(file_a, file_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bfc58",
   "metadata": {},
   "source": [
    "### Convert to a Label Studio Processable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e2cf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_label_studio_format(raw_data):\n",
    "    converted = []\n",
    "    for entry in raw_data:\n",
    "        new_entry = {\n",
    "            \"data\": {\n",
    "                \"text\": entry[\"content\"],\n",
    "                \"bucket_label\" : entry[\"bucket_label\"] if entry.get(\"bucket_label\") is not None else -10\n",
    "            },\n",
    "            \"meta\": {k: v for k, v in entry.items() if k != \"content\" and k != \"bucket_label\"}\n",
    "        }\n",
    "        converted.append(new_entry)\n",
    "    return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d4a9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/labelstudio-training-sampled.json\", \"r\") as file:\n",
    "  training_documents = json.load(file)\n",
    "parsed_training_documents = convert_to_label_studio_format(training_documents)\n",
    "with open(\"out/labelstudio/p1/p1_training_prepped.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(parsed_training_documents, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"out/golden_standard.json\", \"r\") as file:\n",
    "  training_documents = json.load(file)\n",
    "  \n",
    "parsed_training_documents = convert_to_label_studio_format(training_documents)\n",
    "with open(\"out/labelstudio/golden_standard_prepped.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(parsed_training_documents, file, ensure_ascii=False, indent=2)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
