{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "11887971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "OUT_DIR =\"OUT/\"\n",
    "ASSET_DIR=\"assets/\"\n",
    "with open(ASSET_DIR + \"dump-formatted.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "  RAW_DOCUMENTS = json.load(file)\n",
    "DOCUMENT_DF = pd.DataFrame.from_records(RAW_DOCUMENTS)\n",
    "DATA_LEN = len(DOCUMENT_DF)\n",
    "  \n",
    "INDOBERT_OUT_FILE = OUT_DIR + \"indobert_embeds.jsonl\"\n",
    "INDOBERTWEET_OUT_FILE = OUT_DIR + \"indobertweet_embeds.jsonl\"\n",
    "INDOBERT_REDUCED_OUT_FILE = OUT_DIR + \"indobert_reduced_embeds.jsonl\"\n",
    "TWEET_REDUCED_OUT_FILE = OUT_DIR + \"indobertweet_reduced_embeds.jsonl\"\n",
    "INDOBERT_NORMALIZED = OUT_DIR + \"indobert_normalized.jsonl\"\n",
    "TWEET_NORMALIZED = OUT_DIR + \"indobertweet_normalized.jsonl\"\n",
    "\n",
    "INDOBERT_KMEANS_EMBED = OUT_DIR + \"indobert/indobert-kmeans-embed.json\"\n",
    "INDOBERT_KMEANS_PROPS = OUT_DIR + \"model-agnostic/mixed-kmeans-props.json\"\n",
    "INDOBERT_KMEANS_CONCAT = OUT_DIR + \"indobert/indobert-kmeans-concat.json\"\n",
    "INDOBERTWEET_KMEANS_EMBED = OUT_DIR + \"indobertweet/indobertweet-kmeans-embed.json\"\n",
    "INDOBERTWEET_KMEANS_PROPS = OUT_DIR + \"model-agnostic/mixed-kmeans-props.json\"\n",
    "INDOBERTWEET_KMEANS_CONCAT = OUT_DIR + \"indobertweet/indobertweet-kmeans-concat.json\"\n",
    "\n",
    "INDOBERT_HDBSCAN_EMBED = OUT_DIR + \"indobert/indobert-hdbscan-embed.json\"\n",
    "INDOBERT_HDBSCAN_PROPS = OUT_DIR + \"model-agnostic/mixed-hdbscan-props.json\"\n",
    "INDOBERT_HDBSCAN_CONCAT = OUT_DIR + \"indobert/indobert-hdbscan-concat.json\"\n",
    "INDOBERTWEET_HDBSCAN_EMBED = OUT_DIR + \"indobertweet/indobertweet-hdbscan-embed.json\"\n",
    "INDOBERTWEET_HDBSCAN_PROPS = OUT_DIR + \"model-agnostic/mixed-hdbscan-props.json\"\n",
    "INDOBERTWEET_HDBSCAN_CONCAT = OUT_DIR + \"indobertweet/indobertweet-hdbscan-concat.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd9c36",
   "metadata": {},
   "source": [
    "### Text Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f687730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "# ─── pre-compiled patterns ────────────────────────────────────────────────\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_KUTI_BEF = re.compile(r'(?i)(?<!\\s)(kutipan)')\n",
    "_KUTI_AFT = re.compile(r'(?i)(kutipan)(?!\\s)')\n",
    "_REPEAT   = re.compile(r'(.)\\1{2,}')       # ≥3 of same char\n",
    "_WS       = re.compile(r'\\s+')\n",
    "\n",
    "# remove from the first token that *begins* with “kutipan” (any case) to the string-end\n",
    "_KUTI_CUT = re.compile(r'(?i)\\bkutipan\\w*.*$', re.DOTALL)   # pre-compile once\n",
    "\n",
    "def cleantext(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = _MENTION.sub(' ', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "\n",
    "    # ⇣ one liner does all the “kutipan” work; the old _KUTI_BEF/_KUTI_AFT are no longer needed\n",
    "    text = _KUTI_CUT.sub('', text)\n",
    "\n",
    "    text = emoji.demojize(text, delimiters=(' ', ' '))\n",
    "    text = _REPEAT.sub(r'\\1\\1', text)\n",
    "    text = _WS.sub(' ', text).strip().lower()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8466d",
   "metadata": {},
   "source": [
    "### Apply cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9d9663d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_DF[\"content\"] = DOCUMENT_DF[\"content\"].apply(cleantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77557b79",
   "metadata": {},
   "source": [
    "### Generate splits and golden standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "372557d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "\n",
    "DOCUMENT_DF = DOCUMENT_DF.drop_duplicates(subset=[\"content\"]).reset_index(drop=True)\n",
    "TRAIN_DF, TEST_DF = train_test_split(\n",
    "  DOCUMENT_DF,\n",
    "  test_size=0.90,\n",
    "  random_state=42,\n",
    ")\n",
    "GOLDEN_STANDARD, UNUSED = train_test_split(\n",
    "  TEST_DF,\n",
    "  test_size=0.99,\n",
    "  random_state=42\n",
    ")\n",
    "print(len(TRAIN_DF))\n",
    "with open(\"out/golden_standard.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(GOLDEN_STANDARD.to_dict(orient=\"records\"), file, ensure_ascii=False, indent=2)\n",
    "with open(\"out/training_split_general.json\", \"w\") as file:\n",
    "  json.dump(TRAIN_DF.to_dict(orient=\"records\"),file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292726f",
   "metadata": {},
   "source": [
    "### Extract hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fac8fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from typing import List, Hashable, Optional\n",
    "with open(\"out/training_split_general.json\", \"r\") as file:\n",
    "  documents = json.load(file)\n",
    "texts = [doc[\"content\"] for doc in documents]\n",
    "\n",
    "hashtags = []\n",
    "\n",
    "for text in texts:\n",
    "  text_split = text.split(\" \")\n",
    "  for token in text_split:\n",
    "    if token.startswith(\"#\"):\n",
    "      hashtags.append(token)\n",
    "\n",
    "def most_common_hashtags(\n",
    "    tags: List[Hashable],\n",
    "    *,\n",
    "    top_n: Optional[int] = None,\n",
    "    min_count: Optional[int] = None,\n",
    ") -> List[Hashable]:\n",
    "    if top_n is None and min_count is None:\n",
    "        raise ValueError(\"Specify either top_n or min_count\")\n",
    "\n",
    "    freq = Counter(tags)\n",
    "    # Sort once by (-count, tag) so result is deterministic for ties\n",
    "    ranked = sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "\n",
    "    if top_n is not None:\n",
    "        selected = ranked[:top_n]\n",
    "    else:\n",
    "        selected = [kv for kv in ranked if kv[1] >= min_count]\n",
    "\n",
    "    return [tag for tag, _ in selected]\n",
    "\n",
    "\n",
    "cleaned_hashtags = most_common_hashtags(hashtags, min_count=20)\n",
    "with open(\"out/hashtag_list.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(cleaned_hashtags, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a05d56",
   "metadata": {},
   "source": [
    "### Initialize all models and tokenizers from IndoBERT and IndoBERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "162726b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "indobert_model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "indobert_tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "\n",
    "tweet_model = AutoModel.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "tweet_tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "\n",
    "indobert_tokenizer.add_tokens(cleaned_hashtags)\n",
    "tweet_tokenizer.add_tokens(cleaned_hashtags)\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "indobert_model = indobert_model.to(device)\n",
    "tweet_model = tweet_model.to(device)\n",
    "\n",
    "#Turn on evaluation mode as default\n",
    "indobert_model.eval()\n",
    "tweet_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965c6ea",
   "metadata": {},
   "source": [
    "### Create functions to get encodings for both indobert and indobertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d2c31a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def getEncodings(textArray):\n",
    "  indobert_inputs = indobert_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  tweet_inputs = tweet_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  indobert_inputs = {k: v.to(\"mps\") for k, v in indobert_inputs.items()}\n",
    "  tweet_inputs = {k: v.to(\"mps\") for k, v in tweet_inputs.items()}\n",
    "  with torch.no_grad():\n",
    "    indobert_outputs = indobert_model(**indobert_inputs)\n",
    "    tweet_outputs = tweet_model(**tweet_inputs)\n",
    "    \n",
    "    \n",
    "  indobert_embeddings = indobert_outputs.last_hidden_state[:, 0, :]\n",
    "  tweet_embeddings = tweet_outputs.last_hidden_state[:, 0, :]\n",
    "  return (indobert_embeddings.cpu().numpy(), tweet_embeddings.cpu().numpy())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "148ebfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def removeFile(file_path):\n",
    "  if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f\"{file_path} deleted.\")\n",
    "  else:\n",
    "    print(f\"{file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "258959a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "def get_batch_embeddings(documents, batch_size=32):\n",
    "  #Document is in the shape of a Pandas Dataframe. Convert to a list first before processing\n",
    "  removeFile(INDOBERT_OUT_FILE)\n",
    "  removeFile(INDOBERTWEET_OUT_FILE)\n",
    "  documents_list = documents.to_dict(orient=\"records\")\n",
    "  for i in tqdm(range(0, len(documents_list), batch_size), desc=\"Generating embeddings\"):\n",
    "    batched = documents_list[i:i+batch_size]\n",
    "    texts = [doc[\"content\"] for doc in batched]\n",
    "    indobert_embedding, tweet_embedding = getEncodings(texts)\n",
    "    \n",
    "    with open(INDOBERT_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, indobert_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")\n",
    "        \n",
    "    with open(INDOBERTWEET_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, tweet_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7d84b2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_embeds.jsonl deleted.\n",
      "OUT/indobertweet_embeds.jsonl deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 481/481 [03:37<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "get_batch_embeddings(TRAIN_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5875d73",
   "metadata": {},
   "source": [
    "### Create function to reduce embed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "04096a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "def reduce_embed_size(embeds):\n",
    "  umap_model = umap.UMAP(n_components=45)\n",
    "  reduced_embedding = umap_model.fit_transform(np.array(embeds))\n",
    "  return reduced_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80dedb8",
   "metadata": {},
   "source": [
    "### Process Indobert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3ae7ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(INDOBERT_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(INDOBERT_REDUCED_OUT_FILE)\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a2933",
   "metadata": {},
   "source": [
    "### Process IndoBERTweet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9aece640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(INDOBERTWEET_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(TWEET_REDUCED_OUT_FILE)\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "34f4ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "try:\n",
    "  stopwords.words('english')\n",
    "except LookupError:\n",
    "  nltk.download('stopwords')\n",
    "stopwords_combined = set(stopwords.words(\"indonesian\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "def extract_structural_features(tweet):\n",
    "  words = tweet.split()\n",
    "  word_lengths = [len(w) for w in words]\n",
    "  \n",
    "  length = len(tweet)\n",
    "  num_hashtags = tweet.count(\"#\")\n",
    "  num_mentions = tweet.count(\"@\")\n",
    "  num_urls = len(re.findall(r\"http\\S+\", tweet))\n",
    "  num_emojis = len([c for c in tweet if c in emoji.EMOJI_DATA])\n",
    "  num_upper = sum(1 for c in tweet if c.isupper())\n",
    "  num_punct = len(re.findall(r\"[^\\w\\s]\", tweet))\n",
    "  avg_word_len = np.mean(word_lengths) if words else 0\n",
    "\n",
    "  # Content/structure-oriented features\n",
    "  is_question = int(tweet.strip().endswith('?'))\n",
    "  is_exclamatory = int(tweet.strip().endswith('!'))\n",
    "  contains_ellipsis = int(\"...\" in tweet)\n",
    "  contains_repeated_chars = int(bool(re.search(r\"(.)\\1{2,}\", tweet)))  # e.g., sooo, yessss\n",
    "  contains_short_link = int(bool(re.search(r\"\\b(?:https?:\\/\\/)?(?:www\\.)?(bit\\.ly|t\\.co|tinyurl\\.com|goo\\.gl|ow\\.ly|is\\.gd|buff\\.ly|adf\\.ly|bitly\\.com|cutt\\.ly|rb\\.gy|rebrand\\.ly)\\/[A-Za-z0-9]+\", tweet)))\n",
    "  contains_digit = int(bool(re.search(r\"\\d\", tweet)))\n",
    "  is_all_caps = int(tweet.isupper() and len(tweet) > 3)\n",
    "  is_emoji_only = int(all(c in emoji.EMOJI_DATA or c.isspace() for c in tweet.strip()) and tweet.strip() != \"\")\n",
    "  contains_quote_or_rt = int(bool(re.search(r\"(RT\\s@|\\\".+\\\")\", tweet)))\n",
    "  word_count = len(words)\n",
    "  stopword_ratio = np.mean([w.lower() in stopwords_combined for w in words]) if words else 0\n",
    "\n",
    "  return [\n",
    "    length, num_hashtags, num_mentions, num_urls,\n",
    "    num_emojis, num_upper, num_punct, avg_word_len,\n",
    "    is_question, is_exclamatory, contains_ellipsis,\n",
    "    contains_repeated_chars, contains_short_link,\n",
    "    contains_digit, is_all_caps, is_emoji_only,\n",
    "    contains_quote_or_rt, word_count, stopword_ratio\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9316084",
   "metadata": {},
   "source": [
    "### Generate structural features of each cleaned content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a3c7ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"content\"]) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props) * 2])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(INDOBERT_NORMALIZED)\n",
    "with open(INDOBERT_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5c335379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"content\"]) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props) * 2])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(TWEET_NORMALIZED)\n",
    "with open(TWEET_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d01c4",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobert\n",
    "1. KMeans + IndoBERT Embeddings\n",
    "2. KMeans + IndoBERT Embeddings + Structure Properties\n",
    "3. KMeans + Structure Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "edb6699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clustered_docs(docs, cluster_labels, output_path):\n",
    "    for doc, label in zip(docs, cluster_labels):\n",
    "        doc[\"bucket_label\"] = int(label)\n",
    "    for doc in docs:\n",
    "        del doc[\"__v\"]\n",
    "        del doc[\"_id\"]\n",
    "        if \"embedding\" in doc:\n",
    "            del doc[\"embedding\"]\n",
    "        if \"structural_property\" in doc:\n",
    "            del doc[\"structural_property\"]\n",
    "        if \"concatenated_features\" in doc:\n",
    "            del doc[\"concatenated_features\"]\n",
    "            \n",
    "    # Sort by label for better organization (optional)\n",
    "    docs_sorted = sorted(docs, key=lambda x: x[\"bucket_label\"])\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs_sorted, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "264d73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15364 15364 15364\n"
     ]
    }
   ],
   "source": [
    "from sre_parse import Verbose\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "with open(INDOBERT_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=10, random_state=42).fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = KMeans(n_clusters=10, random_state=42).fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = KMeans(n_clusters=10, random_state=42).fit(properties)\n",
    "\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERT_KMEANS_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERT_KMEANS_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERT_KMEANS_PROPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1875ee",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobertweet\n",
    "1. KMeans + IndoBERTweet Embeddings\n",
    "2. KMeans + IndoBERTweet Embeddings + Structure Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9b408120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15364 15364 15364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "with open(TWEET_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=10, random_state=42).fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = KMeans(n_clusters=10, random_state=42).fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = KMeans(n_clusters=10, random_state=42).fit(properties)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "assert len(labels_embed) == len(normalized_indobert_documents)\n",
    "\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERTWEET_KMEANS_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERTWEET_KMEANS_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERTWEET_KMEANS_PROPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e489d",
   "metadata": {},
   "source": [
    "### Utilize HDBSCAN and generate buckets on IndoBERT\n",
    "1. HDBSCAN + IndoBERT Embeddings\n",
    "2. HDBSCAN + IndoBERT Embeddings + Structure Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "20be0838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15364 15364 15364\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import copy\n",
    "import hdbscan\n",
    "\n",
    "with open(INDOBERT_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = hdbscan.HDBSCAN(min_cluster_size=105, metric=\"euclidean\").fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = hdbscan.HDBSCAN(min_cluster_size=18, metric=\"euclidean\").fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = hdbscan.HDBSCAN(min_cluster_size=30, metric=\"euclidean\").fit(properties)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERT_HDBSCAN_EMBED)\n",
    "# save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERT_HDBSCAN_CONCAT)\n",
    "# save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERT_HDBSCAN_PROPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d9e59b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15364 15364 15364\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import copy\n",
    "import hdbscan\n",
    "\n",
    "with open(TWEET_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only #before waas  18\n",
    "kmeans_embed = hdbscan.HDBSCAN(min_cluster_size=34, metric=\"euclidean\").fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props #before was 17\n",
    "kmeans_concat = hdbscan.HDBSCAN(min_cluster_size=25, metric=\"euclidean\").fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = hdbscan.HDBSCAN(min_cluster_size=14, metric=\"euclidean\").fit(properties)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERTWEET_HDBSCAN_EMBED)\n",
    "# save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERTWEET_HDBSCAN_CONCAT)\n",
    "# save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERTWEET_HDBSCAN_PROPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3dcdecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running silhouette score and gini coefficient tests...\n",
      "\n",
      "--- IndoBERT HDBSCAN + Concat ---\n",
      "OUT/indobert/indobert-hdbscan-concat.json Silhouette Score: 0.1186\n",
      "OUT/indobert/indobert-hdbscan-concat.json Gini Coefficient of cluster sizes: 0.8460\n",
      "Cluster counts: {0: 22, 1: 33, 2: 220, 3: 460, 4: 30, 5: 347, 6: 65, 7: 191, 8: 13606, 9: 86}\n",
      "\n",
      "--- IndoBERT HDBSCAN + Embed ---\n",
      "OUT/indobert/indobert-hdbscan-embed.json Silhouette Score: 0.2125\n",
      "OUT/indobert/indobert-hdbscan-embed.json Gini Coefficient of cluster sizes: 0.5369\n",
      "Cluster counts: {0: 507, 1: 225, 2: 302, 3: 3339, 4: 1504, 5: 122, 6: 323, 7: 367, 8: 268, 9: 348, 10: 801, 11: 2795}\n",
      "\n",
      "--- IndoBERT KMeans + Concat ---\n",
      "OUT/indobert/indobert-kmeans-concat.json Silhouette Score: 0.2859\n",
      "OUT/indobert/indobert-kmeans-concat.json Gini Coefficient of cluster sizes: 0.3392\n",
      "Cluster counts: {0: 3242, 1: 3022, 2: 758, 3: 1531, 4: 1285, 5: 546, 6: 1486, 7: 476, 8: 776, 9: 2242}\n",
      "\n",
      "--- IndoBERT KMeans + Embed ---\n",
      "OUT/indobert/indobert-kmeans-embed.json Silhouette Score: 0.4094\n",
      "OUT/indobert/indobert-kmeans-embed.json Gini Coefficient of cluster sizes: 0.3159\n",
      "Cluster counts: {0: 2145, 1: 644, 2: 3014, 3: 1568, 4: 2053, 5: 2537, 6: 508, 7: 326, 8: 1311, 9: 1258}\n",
      "\n",
      "--- IndoBERTweet HDBSCAN + Concat ---\n",
      "OUT/indobertweet/indobertweet-hdbscan-concat.json Silhouette Score: 0.1272\n",
      "OUT/indobertweet/indobertweet-hdbscan-concat.json Gini Coefficient of cluster sizes: 0.8415\n",
      "Cluster counts: {0: 26, 1: 32, 2: 72, 3: 26, 4: 48, 5: 235, 6: 454, 7: 457, 8: 13564, 9: 274}\n",
      "\n",
      "--- IndoBERTweet HDBSCAN + Embed ---\n",
      "OUT/indobertweet/indobertweet-hdbscan-embed.json Silhouette Score: 0.5200\n",
      "OUT/indobertweet/indobertweet-hdbscan-embed.json Gini Coefficient of cluster sizes: 0.7285\n",
      "Cluster counts: {0: 48, 1: 98, 2: 14825, 3: 316}\n",
      "\n",
      "--- IndoBERTweet KMeans + Concat ---\n",
      "OUT/indobertweet/indobertweet-kmeans-concat.json Silhouette Score: 0.2154\n",
      "OUT/indobertweet/indobertweet-kmeans-concat.json Gini Coefficient of cluster sizes: 0.3954\n",
      "Cluster counts: {0: 2960, 1: 3124, 2: 1249, 3: 712, 4: 48, 5: 1114, 6: 2855, 7: 2129, 8: 831, 9: 342}\n",
      "\n",
      "--- IndoBERTweet KMeans + Embed ---\n",
      "OUT/indobertweet/indobertweet-kmeans-embed.json Silhouette Score: 0.3696\n",
      "OUT/indobertweet/indobertweet-kmeans-embed.json Gini Coefficient of cluster sizes: 0.4328\n",
      "Cluster counts: {0: 3103, 1: 3228, 2: 2302, 3: 2012, 4: 32, 5: 316, 6: 165, 7: 2042, 8: 2138, 9: 26}\n",
      "\n",
      "--- KMeans + Properties ---\n",
      "OUT/model-agnostic/mixed-kmeans-props.json Silhouette Score: 0.2987\n",
      "OUT/model-agnostic/mixed-kmeans-props.json Gini Coefficient of cluster sizes: 0.4926\n",
      "Cluster counts: {0: 4998, 1: 1857, 2: 2660, 3: 1832, 4: 5, 5: 1, 6: 255, 7: 1543, 8: 1700, 9: 513}\n",
      "\n",
      "--- HDBSCAN + Properties ---\n",
      "OUT/model-agnostic/mixed-hdbscan-props.json Silhouette Score: 0.2516\n",
      "OUT/model-agnostic/mixed-hdbscan-props.json Gini Coefficient of cluster sizes: 0.7907\n",
      "Cluster counts: {0: 19, 1: 19, 2: 103, 3: 385, 4: 49, 5: 167, 6: 196, 7: 293, 8: 3797, 9: 10251}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def gini_coefficient(array):\n",
    "    \"\"\"Compute Gini coefficient of array of values.\"\"\"\n",
    "    # Based on mean absolute difference formula\n",
    "    array = np.array(array, dtype=np.float64)\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array)  # Ensure non-negative\n",
    "    array += 1e-10  # Avoid division by zero\n",
    "    array = np.sort(array)\n",
    "    n = array.size\n",
    "    cumvals = np.cumsum(array)\n",
    "    gini = (n + 1 - 2 * np.sum(cumvals) / cumvals[-1]) / n\n",
    "    return gini\n",
    "\n",
    "def evaluate_gini(bucketed):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    labels = labels_data[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    # Exclude noise points (-1)\n",
    "    valid_labels = labels[labels >= 0]\n",
    "\n",
    "    counts = pd.Series(valid_labels).value_counts().sort_index()\n",
    "    gini = gini_coefficient(counts.values)\n",
    "\n",
    "    print(f\"{bucketed} Gini Coefficient of cluster sizes: {gini:.4f}\")\n",
    "    print(f\"Cluster counts: {counts.to_dict()}\")\n",
    "\n",
    "def get_silhouette_score_embedding(bucketed, model_features):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    with open(model_features, \"r\") as file:\n",
    "        features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "    merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "    features_embedding = np.stack(merged_df[\"embedding\"].to_numpy())\n",
    "    labels = merged_df[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    embedding_score = silhouette_score(features_embedding, labels, metric='euclidean')\n",
    "    print(f\"{bucketed} Silhouette Score: {embedding_score:.4f}\")\n",
    "\n",
    "def get_silhouette_score_properties(bucketed, model_features):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    with open(model_features, \"r\") as file:\n",
    "        features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "    merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "    features_properties = np.stack(merged_df[\"structural_property\"].to_numpy())\n",
    "    labels = merged_df[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    # valid_mask = labels >= 0\n",
    "    # features_properties_valid = features_properties[valid_mask]\n",
    "    # labels_valid = labels[valid_mask]\n",
    "\n",
    "    properties_score = silhouette_score(features_properties, labels, metric='euclidean')\n",
    "    print(f\"{bucketed} Silhouette Score: {properties_score:.4f}\")\n",
    "\n",
    "def get_silhouette_score_concat(bucketed, model_features):\n",
    "    with open(bucketed, \"r\") as file:\n",
    "        labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "    with open(model_features, \"r\") as file:\n",
    "        features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "    merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "    features_concat = np.stack(merged_df[\"concatenated_features\"].to_numpy())\n",
    "    labels = merged_df[\"bucket_label\"].to_numpy()\n",
    "\n",
    "    concat_score = silhouette_score(features_concat, labels, metric='euclidean')\n",
    "    print(f\"{bucketed} Silhouette Score: {concat_score:.4f}\")\n",
    "\n",
    "\n",
    "tests = [\n",
    "  [\"IndoBERT HDBSCAN + Concat\", INDOBERT_HDBSCAN_CONCAT, INDOBERT_NORMALIZED, get_silhouette_score_concat],\n",
    "  [\"IndoBERT HDBSCAN + Embed\",  INDOBERT_HDBSCAN_EMBED, INDOBERT_NORMALIZED, get_silhouette_score_embedding],\n",
    "  [\"IndoBERT KMeans + Concat\",  INDOBERT_KMEANS_CONCAT, INDOBERT_NORMALIZED, get_silhouette_score_concat],\n",
    "  [\"IndoBERT KMeans + Embed\",   INDOBERT_KMEANS_EMBED,  INDOBERT_NORMALIZED, get_silhouette_score_embedding],\n",
    "  [\"IndoBERTweet HDBSCAN + Concat\", INDOBERTWEET_HDBSCAN_CONCAT, TWEET_NORMALIZED, get_silhouette_score_concat],\n",
    "  [\"IndoBERTweet HDBSCAN + Embed\",  INDOBERTWEET_HDBSCAN_EMBED,  TWEET_NORMALIZED, get_silhouette_score_embedding],\n",
    "  [\"IndoBERTweet KMeans + Concat\",  INDOBERTWEET_KMEANS_CONCAT, TWEET_NORMALIZED, get_silhouette_score_concat],\n",
    "  [\"IndoBERTweet KMeans + Embed\",   INDOBERTWEET_KMEANS_EMBED,  TWEET_NORMALIZED, get_silhouette_score_embedding],\n",
    "  [\"KMeans + Properties\", INDOBERTWEET_KMEANS_PROPS, TWEET_NORMALIZED, get_silhouette_score_properties],\n",
    "  [\"HDBSCAN + Properties\", INDOBERTWEET_HDBSCAN_PROPS, TWEET_NORMALIZED, get_silhouette_score_properties]\n",
    "]\n",
    "\n",
    "print(\"Running silhouette score and gini coefficient tests...\\n\")\n",
    "for name, label_file, feature_file, scorer in tests:\n",
    "    print(f\"--- {name} ---\")\n",
    "    scorer(label_file, feature_file)\n",
    "    evaluate_gini(label_file)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2b839114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Davies–Bouldin Index tests...\n",
      "\n",
      "OUT/indobert/indobert-hdbscan-concat.json DBI (Concat): 1.1871\n",
      "OUT/indobert/indobert-hdbscan-embed.json DBI (Embedding): 1.0515\n",
      "OUT/indobert/indobert-kmeans-concat.json DBI (Concat): 1.1228\n",
      "OUT/indobert/indobert-kmeans-embed.json DBI (Embedding): 0.8676\n",
      "OUT/indobertweet/indobertweet-hdbscan-concat.json DBI (Concat): 1.2532\n",
      "OUT/indobertweet/indobertweet-hdbscan-embed.json DBI (Embedding): 0.4677\n",
      "OUT/indobertweet/indobertweet-kmeans-concat.json DBI (Concat): 1.3443\n",
      "OUT/indobertweet/indobertweet-kmeans-embed.json DBI (Embedding): 0.8790\n",
      "OUT/model-agnostic/mixed-kmeans-props.json DBI (Properties): 1.1380\n",
      "OUT/model-agnostic/mixed-hdbscan-props.json DBI (Properties): 0.8193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# DBI SCORERS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def dbi_embedding(bucketed: str, model_features: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the Davies–Bouldin Index using only the embedding vectors.\n",
    "    \"\"\"\n",
    "    labels_df   = pd.read_json(bucketed)\n",
    "    labels_df[\"tweet_id\"] = labels_df[\"tweet_id\"].astype(str)\n",
    "    \n",
    "    features_df = pd.DataFrame([json.loads(line) for line in open(model_features)])\n",
    "\n",
    "    merged_df   = pd.merge(labels_df, features_df, on=\"tweet_id\", how=\"inner\")\n",
    "    X           = np.stack(merged_df[\"embedding\"].to_numpy())\n",
    "    y           = merged_df[\"bucket_label\"]\n",
    "\n",
    "    score = davies_bouldin_score(X, y)\n",
    "    print(f\"{bucketed} DBI (Embedding): {score:.4f}\")\n",
    "\n",
    "def dbi_properties(bucketed: str, model_features: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the Davies–Bouldin Index on structural-property feature vectors.\n",
    "    \"\"\"\n",
    "    labels_df   = pd.read_json(bucketed)\n",
    "    labels_df[\"tweet_id\"] = labels_df[\"tweet_id\"].astype(str)\n",
    "    features_df = pd.DataFrame([json.loads(line) for line in open(model_features)])\n",
    "\n",
    "    merged_df   = pd.merge(labels_df, features_df, on=\"tweet_id\", how=\"inner\")\n",
    "    X           = np.stack(merged_df[\"structural_property\"].to_numpy())\n",
    "    y           = merged_df[\"bucket_label\"]\n",
    "\n",
    "    score = davies_bouldin_score(X, y)\n",
    "    print(f\"{bucketed} DBI (Properties): {score:.4f}\")\n",
    "\n",
    "def dbi_concat(bucketed: str, model_features: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the Davies–Bouldin Index on concatenated feature vectors.\n",
    "    \"\"\"\n",
    "    labels_df   = pd.read_json(bucketed)\n",
    "    labels_df[\"tweet_id\"] = labels_df[\"tweet_id\"].astype(str)\n",
    "    features_df = pd.DataFrame([json.loads(line) for line in open(model_features)])\n",
    "\n",
    "    merged_df   = pd.merge(labels_df, features_df, on=\"tweet_id\", how=\"inner\")\n",
    "    X           = np.stack(merged_df[\"concatenated_features\"].to_numpy())\n",
    "    y           = merged_df[\"bucket_label\"]\n",
    "\n",
    "    score = davies_bouldin_score(X, y)\n",
    "    print(f\"{bucketed} DBI (Concat): {score:.4f}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# TEST MATRIX\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "tests = [\n",
    "    [\"IndoBERT HDBSCAN + Concat\",  INDOBERT_HDBSCAN_CONCAT,  INDOBERT_NORMALIZED, dbi_concat],\n",
    "    [\"IndoBERT HDBSCAN + Embed\",   INDOBERT_HDBSCAN_EMBED,   INDOBERT_NORMALIZED, dbi_embedding],\n",
    "    [\"IndoBERT KMeans  + Concat\",  INDOBERT_KMEANS_CONCAT,   INDOBERT_NORMALIZED, dbi_concat],\n",
    "    [\"IndoBERT KMeans  + Embed\",   INDOBERT_KMEANS_EMBED,    INDOBERT_NORMALIZED, dbi_embedding],\n",
    "    [\"IndoBERTweet HDBSCAN + Concat\", INDOBERTWEET_HDBSCAN_CONCAT, TWEET_NORMALIZED, dbi_concat],\n",
    "    [\"IndoBERTweet HDBSCAN + Embed\",  INDOBERTWEET_HDBSCAN_EMBED,  TWEET_NORMALIZED, dbi_embedding],\n",
    "    [\"IndoBERTweet KMeans  + Concat\", INDOBERTWEET_KMEANS_CONCAT,  TWEET_NORMALIZED, dbi_concat],\n",
    "    [\"IndoBERTweet KMeans  + Embed\",  INDOBERTWEET_KMEANS_EMBED,   TWEET_NORMALIZED, dbi_embedding],\n",
    "    [\"KMeans  + Properties\",          INDOBERTWEET_KMEANS_PROPS,   TWEET_NORMALIZED, dbi_properties],\n",
    "    [\"HDBSCAN + Properties\",          INDOBERTWEET_HDBSCAN_PROPS,  TWEET_NORMALIZED, dbi_properties],\n",
    "]\n",
    "\n",
    "print(\"Running Davies–Bouldin Index tests...\\n\")\n",
    "for name, label_file, feature_file, scorer in tests:\n",
    "    scorer(label_file, feature_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c1606",
   "metadata": {},
   "source": [
    "### Do a bit of bucket analization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "18d9ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from collections import Counter\n",
    "import re\n",
    "def count_hashtags(text):\n",
    "    return len(re.findall(r\"#\\w+\", text))\n",
    "\n",
    "def hashtag_ratio(text):\n",
    "    hashtags = ''.join(re.findall(r\"#\\w+\", text))\n",
    "    return len(hashtags) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\"\n",
    "                               \"\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "def emoji_ratio(text):\n",
    "    emojis = extract_emojis(text)\n",
    "    return len(''.join(emojis)) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def url_ratio(tweets):\n",
    "    return sum(1 for t in tweets if re.search(r\"http\\S+\", t)) / len(tweets)\n",
    "\n",
    "def mention_ratio(tweets):\n",
    "    return sum(1 for t in tweets if re.search(r\"@\\w+\", t)) / len(tweets)\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "def repeated_char_abuse(text):\n",
    "    return bool(re.search(r\"(.)\\1{3,}\", text))\n",
    "\n",
    "def duplicate_ratio(tweets):\n",
    "    freq = Counter(tweets)\n",
    "    return sum(count for tweet, count in freq.items() if count > 1) / len(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c8f84332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0: generic\n",
      "  avg_hashtags: 0.2755244755244755\n",
      "  avg_length: 70.04568764568765\n",
      "  longest_tweet: sorry bgt ye gw bukannya tone deaf atau apalah cuma badan gw sakit sakitan puki. semalem sakin overwhelmed nya seharian ngikutin berita ruu tni, ditambah timnas kalah, ditambah berita yang ga udah udah gw muntah muntah tengah malem sampe ga kuat makan lagi sakin lemesnya\n",
      "  shortest_tweet: pap\n",
      "  hashtag_ratio: 0.05821706207157399\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9720739969124486\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 1: generic\n",
      "  avg_hashtags: 0.8354037267080745\n",
      "  avg_length: 188.0\n",
      "  longest_tweet: selain kempen derma darah, turut diadakan demonstrasi pertolongan cemas cardiopulmonary resuscitation (cpr) serta penggunaan automated external defibrillator (aed) dalam program pada 8 disember lalu ini.nabalunews.comkerjasama komuniti, kerajaan dan swasta: kempen derma darah berjaya dilaksanakan di kampung..11 disember 2024 kota marudu : kempen derma darah anjuran kawasan rukun tetangga (krt) dan komuniti sihat perkasa negara (kospen) kampung masolog dengan kerjasama unit tabung darah hospital kota..\n",
      "  shortest_tweet: harap pn buat demonstrasi lah dakwa tok saka\n",
      "  hashtag_ratio: 0.06348265196149985\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9342568056869182\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 2: generic\n",
      "  avg_hashtags: 2.11877903118779\n",
      "  avg_length: 141.57332448573325\n",
      "  longest_tweet: ya, ini cuma rumor. tidak ada laporan resmi dari world bank yang menyebut masa depan indonesia \"gelap\". sebaliknya, world bank dan adb memprediksi pertumbuhan ekonomi stabil sekitar 5,0-5,2% untuk 2025. istilah \"dark indonesia\" lebih terkait protes politik, bukan pernyataan worldtampilkan lebih banyakgrokproductivitypasang\n",
      "  shortest_tweet: lawan dwifungsi tni!!\n",
      "  hashtag_ratio: 0.2477753653429727\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9564934210551687\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 3: hashtag-heavy\n",
      "  avg_hashtags: 8.644770408163266\n",
      "  avg_length: 171.6549744897959\n",
      "  longest_tweet: #cabutuutni #tolakruupolri #supremasisipil #adilijokowi #makzulkanprabowogibran #usiroligarki #batalkanpsn #indonesiagelap #peringatandarurat #sos #nalar marperempuan mengalami kekerasan pada aksi demo di gedung dpr ri.. #cabutuutni #tolakruutni #tolakrevisiuutni #tolakdwifungsiabri #indonesiagelap #peringatandarurat\n",
      "  shortest_tweet: go!! dwifungsi tni go!!\n",
      "  hashtag_ratio: 0.7895076333411463\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.956657900251721\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 4: long tweets\n",
      "  avg_hashtags: 1.2464685825621042\n",
      "  avg_length: 246.55577204091574\n",
      "  longest_tweet: updates (baca di threadnya) gerakan hamas: kami menyerukan kelanjutan dan peningkatan demonstrasi jumat,sabtu,minggu sebagai penolakan terhadap kelaparan anak\" & mengecam agresi zionis terhadap jalur gaza. 09.11.23 #freepalenstine #stopgenocideingaza #sahabatpalestina_idkutipańme_ nov 2023 akun parodihamas tidak akan jatuh : demi allah, hamas tidak akan jatuh meskipun pihak-pihak yang dekat dan jauh menentangnya demi allah,hamas tidak akan jatuh meskipun setan dari barat dan arab menyatakan perang terhadapnya #freepalenstine #stopgenocideingaza #sahabatpalestina_id x.com/kheyl4_8/statu..\n",
      "  shortest_tweet: ⋆౨ৎ ̊⟡˖ ࣪ cahol dwifungsi —\n",
      "  hashtag_ratio: 0.07070696719267948\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.8738409334023365\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 5: generic\n",
      "  avg_hashtags: 0.2471422940480883\n",
      "  avg_length: 188.3728813559322\n",
      "  longest_tweet: beginilah nasib menjadi negara komedi yang dicengkeram oligarki dan dikuasai politik dinasti yang suka memberangus oposisi demi bisa mesra dengan bestie karena alergi demonstrasi parahnya persekusi sudah jadi hobi serta intimidasi jadi insting alami <url> mar kepercayaan publik dan dunia usaha makin terkikis akibat kemenkeu tak transparan dalam menyampaikan pengelolaan keuangan negara. bulan lalu, konferensi pers apbn kita yang biasanya rutin digelar, tiba-tiba tidak diadakan. bahkan, dokumen apbn kita edisi januari & februari 2025\n",
      "  shortest_tweet: hah kenapa?\n",
      "  hashtag_ratio: 0.020904024416048574\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9273427964591474\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 6: generic\n",
      "  avg_hashtags: 0.5\n",
      "  avg_length: 131.11023622047244\n",
      "  longest_tweet: nggak mau transparan dalam berproses membuat legislasi tapi mengaharapkan publik tidak berburuk sangka setelah apa yang terjadi dalam proses perubahan uu tni. anda punya otak?! #indonesiagelap aprpimpinan dpr minta publik tak berburuk sangka soal pembahasan ruu kuhap <url>\n",
      "  shortest_tweet: <url>\n",
      "  hashtag_ratio: 0.04801945478117335\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.970416505449059\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 7: generic\n",
      "  avg_hashtags: 1.9969325153374233\n",
      "  avg_length: 152.50306748466258\n",
      "  longest_tweet: ini adalah salah satu jeritan hati warga desa ria-ria yang berulang kali menggelar demonstrasi, mendesak pemerintah menyelesaikan berbagai masalah yang terjadi akibat food estate. <url> news indonesia okt 2024\"kalau kamu harus kehilangan tanah kami, lebih baik bunuh kami semua!\" ini adalah salah satu jeritan hati warga desa ria-ria yang berulang kali menggelar demonstrasi, mendesak pemerintah menyelesaikan berbagai masalah yang terjadi akibat food estate. <url>\n",
      "  shortest_tweet: pengin kuliah lg yaallah :(\n",
      "  hashtag_ratio: 0.21124532872700005\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9510482435763299\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 8: hashtag-heavy\n",
      "  avg_hashtags: 2.314263920671243\n",
      "  avg_length: 68.42944317315026\n",
      "  longest_tweet: to everyone who will take to the streets today, raising our voices against injustice, may you be protected by god, the kindest of all beings. and to those who cannot be there physically but still care, still fight in their own way—our voice matters just as much. #indonesiagelap\n",
      "  shortest_tweet: 1\n",
      "  hashtag_ratio: 0.5195099159953707\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9763561227458517\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 9: generic\n",
      "  avg_hashtags: 2.643879173290938\n",
      "  avg_length: 114.7066772655008\n",
      "  longest_tweet: nigeria: wapco trains 164 nigerian artisans, invests $750,00 annually #rajatdalal |#fenerintetikcisivincic |#skynani #skynani | |#tubabüyüküstün |#bbnaija|#tolakdwifungsiabri |#rnaq40|#volkankonak|resign|big akwes|tyla|e-levy|betting taenergynewsafrica.comnigeria: wapco trains 164 nigerian artisans, invests $750,00 annuallythe west african gas pipeline company ltd. (wapco) has committed over 750,00 dollars annually to train more than 164 artisans in nigeria, with a focus on empowering local communities. dr isaac..\n",
      "  shortest_tweet: #unjuk rasa\n",
      "  hashtag_ratio: 0.3105336236716718\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.967769575956962\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "from collections import defaultdict\n",
    "import json\n",
    "buckets = defaultdict(list)\n",
    "\n",
    "with open(INDOBERT_KMEANS_EMBED, \"r\") as file:\n",
    "  documents = json.load(file)\n",
    "\n",
    "for doc in documents:\n",
    "  buckets[doc[\"bucket_label\"]].append(doc)\n",
    "\n",
    "for label, bucket_tweets in buckets.items():\n",
    "    bucket_tweets = [doc[\"content\"] for doc in bucket_tweets]\n",
    "    \n",
    "    hashtag_counts = [count_hashtags(t) for t in bucket_tweets]\n",
    "    tweet_lengths = [len(t) for t in bucket_tweets]\n",
    "    hashtag_ratios = [hashtag_ratio(t) for t in bucket_tweets]\n",
    "    emoji_ratios = [emoji_ratio(t) for t in bucket_tweets]\n",
    "    lexical_divs = [lexical_diversity(t) for t in bucket_tweets]\n",
    "    repeated_abuse_count = sum(1 for t in bucket_tweets if repeated_char_abuse(t))\n",
    "\n",
    "    avg_hashtags = sum(hashtag_counts) / len(bucket_tweets)\n",
    "    avg_length = sum(tweet_lengths) / len(bucket_tweets)\n",
    "    avg_hashtag_ratio = sum(hashtag_ratios) / len(bucket_tweets)\n",
    "    avg_emoji_ratio = sum(emoji_ratios) / len(bucket_tweets)\n",
    "    avg_lexical_div = sum(lexical_divs) / len(bucket_tweets)\n",
    "    url_ratio_val = url_ratio(bucket_tweets)\n",
    "    mention_ratio_val = mention_ratio(bucket_tweets)\n",
    "    dup_ratio = duplicate_ratio(bucket_tweets)\n",
    "\n",
    "    # Heuristic label tagging\n",
    "    label_tags = []\n",
    "    if avg_length > 200:\n",
    "        label_tags.append(\"long tweets\")\n",
    "    if avg_hashtag_ratio > 0.4:\n",
    "        label_tags.append(\"hashtag-heavy\")\n",
    "    if avg_emoji_ratio > 0.2:\n",
    "        label_tags.append(\"emoji spam\")\n",
    "    if url_ratio_val > 0.3:\n",
    "        label_tags.append(\"link drop\")\n",
    "    if mention_ratio_val > 0.3:\n",
    "        label_tags.append(\"mention spam\")\n",
    "    if avg_lexical_div < 0.4:\n",
    "        label_tags.append(\"low diversity (copypasta)\")\n",
    "    if repeated_abuse_count / len(bucket_tweets) > 0.3:\n",
    "        label_tags.append(\"repeated char abuse\")\n",
    "    if dup_ratio > 0.3:\n",
    "        label_tags.append(\"high duplication\")\n",
    "\n",
    "    longest = max(bucket_tweets, key=len)\n",
    "    shortest = min(bucket_tweets, key=len)\n",
    "\n",
    "    results[label] = {\n",
    "        \"label\": \", \".join(label_tags) if label_tags else \"generic\",\n",
    "        \"avg_hashtags\": avg_hashtags,\n",
    "        \"avg_length\": avg_length,\n",
    "        \"longest_tweet\": longest,\n",
    "        \"shortest_tweet\": shortest,\n",
    "        \"hashtag_ratio\": avg_hashtag_ratio,\n",
    "        \"emoji_ratio\": avg_emoji_ratio,\n",
    "        \"url_ratio\": url_ratio_val,\n",
    "        \"mention_ratio\": mention_ratio_val,\n",
    "        \"lexical_diversity\": avg_lexical_div,\n",
    "        \"duplication_ratio\": dup_ratio,\n",
    "        \"repeated_char_abuse_count\": repeated_abuse_count,\n",
    "    }\n",
    "\n",
    "# Output the results\n",
    "for label, metrics in results.items():\n",
    "    print(f\"\\nCluster {label}: {metrics['label']}\")\n",
    "    for k, v in metrics.items():\n",
    "        if k != 'label':\n",
    "            print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e65716c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0 sampled with ratio 0.1396120801874512 and total sample 110\n",
      "Bucket 1 sampled with ratio 0.041916167664670656 and total sample 110\n",
      "Bucket 2 sampled with ratio 0.19617287164800834 and total sample 110\n",
      "Bucket 3 sampled with ratio 0.10205675605311117 and total sample 110\n",
      "Bucket 4 sampled with ratio 0.13362405623535537 and total sample 110\n",
      "Bucket 5 sampled with ratio 0.16512626920072898 and total sample 110\n",
      "Bucket 6 sampled with ratio 0.03306430617026816 and total sample 110\n",
      "Bucket 7 sampled with ratio 0.021218432699817755 and total sample 110\n",
      "Bucket 8 sampled with ratio 0.08532934131736528 and total sample 110\n",
      "Bucket 9 sampled with ratio 0.08187971882322312 and total sample 110\n",
      "Sampled tweets saved to out/labelstudio-training-sampled.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# Parameters\n",
    "INPUT_FILE = INDOBERT_KMEANS_EMBED\n",
    "OUTPUT_FILE = 'out/labelstudio-training-sampled.json'\n",
    "\n",
    "TOTAL_SAMPLE = 1100  # Change this as needed\n",
    "\n",
    "# Load data\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "bucket_content_count = defaultdict(int)\n",
    "bucket_ratio_count = defaultdict(float)\n",
    "\n",
    "total_tweet = 0\n",
    "for tweet in tweets:\n",
    "  label = tweet[\"bucket_label\"]\n",
    "  bucket_content_count[label] += 1\n",
    "  total_tweet += 1\n",
    "\n",
    "for label, bucket_tweet_count in bucket_content_count.items():\n",
    "  bucket_ratio_count[label] = bucket_tweet_count / total_tweet\n",
    "\n",
    "# Group tweets by bucket\n",
    "buckets = defaultdict(list)\n",
    "for tweet in tweets:\n",
    "  bucket_label = str(tweet[\"bucket_label\"])\n",
    "  buckets[bucket_label].append(tweet)\n",
    "\n",
    "# Sample tweets\n",
    "sampled_tweets = []\n",
    "for (bucket_label, tweets_in_bucket), ratio in zip(buckets.items(), bucket_ratio_count.values()):\n",
    "  # ratiod_total = math.ceil(TOTAL_SAMPLE * ratio)\n",
    "  ratiod_total = int(TOTAL_SAMPLE / bucket_content_count.__len__())\n",
    "  print(f\"Bucket {bucket_label} sampled with ratio {ratio} and total sample {ratiod_total}\")\n",
    "  if len(tweets_in_bucket) < ratiod_total:\n",
    "    print(f\"Warning: Bucket '{bucket_label}' has only {len(tweets_in_bucket)} tweets. Sampling all.\")\n",
    "    sampled = tweets_in_bucket\n",
    "  else:\n",
    "    sampled = random.sample(tweets_in_bucket, ratiod_total)\n",
    "  sampled_tweets.extend(sampled)\n",
    "\n",
    "# Save to output JSON\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "  json.dump(sampled_tweets, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Sampled tweets saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f2fc2",
   "metadata": {},
   "source": [
    "### Check for data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4a2b90d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No data leakage detected.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_ids_from_json_or_jsonl(file_path, id_key=\"tweet_id\"):\n",
    "    ids = set()\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        if first_char == \"[\":  # JSON array\n",
    "            data = json.load(f)\n",
    "            ids = {entry[id_key] for entry in data if id_key in entry}\n",
    "        else:  # JSONL\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    if id_key in obj:\n",
    "                        ids.add(obj[id_key])\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return ids\n",
    "\n",
    "def check_data_leakage(file1, file2, id_key=\"tweet_id\"):\n",
    "    ids_1 = load_ids_from_json_or_jsonl(file1, id_key)\n",
    "    ids_2 = load_ids_from_json_or_jsonl(file2, id_key)\n",
    "\n",
    "    intersection = ids_1 & ids_2\n",
    "\n",
    "    if intersection:\n",
    "        print(f\"⚠️ Data leakage detected! {len(intersection)} shared {id_key}s.\")\n",
    "    else:\n",
    "        print(\"✅ No data leakage detected.\")\n",
    "\n",
    "    return intersection\n",
    "\n",
    "# Example usage:\n",
    "file_a = \"out/labelstudio-training-sampled.json\"\n",
    "file_b = \"out/golden_standard.json\"\n",
    "leaked_ids = check_data_leakage(file_a, file_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bfc58",
   "metadata": {},
   "source": [
    "### Convert to a Label Studio Processable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4e2cf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_label_studio_format(raw_data):\n",
    "    converted = []\n",
    "    for entry in raw_data:\n",
    "        new_entry = {\n",
    "            \"data\": {\n",
    "                \"text\": entry[\"content\"],\n",
    "                \"bucket_label\" : entry[\"bucket_label\"] if entry.get(\"bucket_label\") is not None else -10\n",
    "            },\n",
    "            \"meta\": {k: v for k, v in entry.items() if k != \"content\" and k != \"bucket_label\"}\n",
    "        }\n",
    "        converted.append(new_entry)\n",
    "    return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0d4a9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/labelstudio-training-sampled.json\", \"r\") as file:\n",
    "  training_documents = json.load(file)\n",
    "parsed_training_documents = convert_to_label_studio_format(training_documents)\n",
    "with open(\"out/labelstudio/p1/p1_training_prepped.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(parsed_training_documents, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"out/golden_standard.json\", \"r\") as file:\n",
    "  training_documents = json.load(file)\n",
    "  \n",
    "parsed_training_documents = convert_to_label_studio_format(training_documents)\n",
    "with open(\"out/labelstudio/golden_standard_prepped.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(parsed_training_documents, file, ensure_ascii=False, indent=2)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
