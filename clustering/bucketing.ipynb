{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e6a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "OUT_DIR =\"OUT/\"\n",
    "ASSET_DIR=\"assets/\"\n",
    "with open(ASSET_DIR + \"dump-formatted.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "  RAW_DOCUMENTS = json.load(file)\n",
    "DOCUMENT_DF = pd.DataFrame.from_records(RAW_DOCUMENTS)\n",
    "DATA_LEN = len(DOCUMENT_DF)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11887971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INDOBERT_OUT_FILE = OUT_DIR + \"indobert_embeds.jsonl\"\n",
    "INDOBERTWEET_OUT_FILE = OUT_DIR + \"indobertweet_embeds.jsonl\"\n",
    "INDOBERT_REDUCED_OUT_FILE = OUT_DIR + \"indobert_reduced_embeds.jsonl\"\n",
    "TWEET_REDUCED_OUT_FILE = OUT_DIR + \"indobertweet_reduced_embeds.jsonl\"\n",
    "INDOBERT_NORMALIZED = OUT_DIR + \"indobert_normalized.jsonl\"\n",
    "TWEET_NORMALIZED = OUT_DIR + \"indobertweet_normalized.jsonl\"\n",
    "\n",
    "INDOBERT_KMEANS_EMBED = OUT_DIR + \"indobert/indobert-kmeans-embed.json\"\n",
    "INDOBERT_KMEANS_PROPS = OUT_DIR + \"model-agnostic/mixed-kmeans-props.json\"\n",
    "INDOBERT_KMEANS_CONCAT = OUT_DIR + \"indobert/indobert-kmeans-concat.json\"\n",
    "INDOBERTWEET_KMEANS_EMBED = OUT_DIR + \"indobertweet/indobertweet-kmeans-embed.json\"\n",
    "INDOBERTWEET_KMEANS_PROPS = OUT_DIR + \"model-agnostic/mixed-kmeans-props.json\"\n",
    "INDOBERTWEET_KMEANS_CONCAT = OUT_DIR + \"indobertweet/indobertweet-kmeans-concat.json\"\n",
    "\n",
    "INDOBERT_HDBSCAN_EMBED = OUT_DIR + \"indobert/indobert-hdbscan-embed.json\"\n",
    "INDOBERT_HDBSCAN_PROPS = OUT_DIR + \"model-agnostic/mixed-hdbscan-props.json\"\n",
    "INDOBERT_HDBSCAN_CONCAT = OUT_DIR + \"indobert/indobert-hdbscan-concat.json\"\n",
    "INDOBERTWEET_HDBSCAN_EMBED = OUT_DIR + \"indobertweet/indobertweet-hdbscan-embed.json\"\n",
    "INDOBERTWEET_HDBSCAN_PROPS = OUT_DIR + \"model-agnostic/mixed-hdbscan-props.json\"\n",
    "INDOBERTWEET_HDBSCAN_CONCAT = OUT_DIR + \"indobertweet/indobertweet-hdbscan-concat.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd9c36",
   "metadata": {},
   "source": [
    "### Text Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f687730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaconv\n",
    "import re\n",
    "def cleantext(text):\n",
    "  text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "  text = text.replace('\\\\n', \" \").replace(\"\\\\r\", \" \")\n",
    "  text = re.sub(r'\\s+', ' ', text)  \n",
    "  text = text.strip()\n",
    "  text = text.lower()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8466d",
   "metadata": {},
   "source": [
    "### Apply cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9663d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_DF[\"content\"] = DOCUMENT_DF[\"content\"].apply(cleantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77557b79",
   "metadata": {},
   "source": [
    "### Generate splits and golden standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372557d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "\n",
    "DOCUMENT_DF = DOCUMENT_DF.drop_duplicates(subset=[\"content\"]).reset_index(drop=True)\n",
    "TRAIN_DF, TEST_DF = train_test_split(\n",
    "  DOCUMENT_DF,\n",
    "  test_size=0.90,\n",
    "  random_state=42,\n",
    ")\n",
    "GOLDEN_STANDARD, UNUSED = train_test_split(\n",
    "  TEST_DF,\n",
    "  test_size=0.99,\n",
    "  random_state=42\n",
    ")\n",
    "print(len(TRAIN_DF))\n",
    "with open(\"out/golden_standard.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(GOLDEN_STANDARD.to_dict(orient=\"records\"), file, ensure_ascii=False, indent=2)\n",
    "with open(\"out/training_split_generalqjson\", \"w\") as file:\n",
    "  json.dump(TRAIN_DF.to_dict(orient=\"records\"),file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a05d56",
   "metadata": {},
   "source": [
    "### Initialize all models and tokenizers from IndoBERT and IndoBERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "162726b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "indobert_model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "indobert_tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "\n",
    "tweet_model = AutoModel.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "tweet_tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "indobert_model = indobert_model.to(device)\n",
    "tweet_model = tweet_model.to(device)\n",
    "\n",
    "#Turn on evaluation mode as default\n",
    "indobert_model.eval()\n",
    "tweet_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965c6ea",
   "metadata": {},
   "source": [
    "### Create functions to get encodings for both indobert and indobertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2c31a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def getEncodings(textArray):\n",
    "  indobert_inputs = indobert_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  tweet_inputs = tweet_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  indobert_inputs = {k: v.to(\"mps\") for k, v in indobert_inputs.items()}\n",
    "  tweet_inputs = {k: v.to(\"mps\") for k, v in tweet_inputs.items()}\n",
    "  with torch.no_grad():\n",
    "    indobert_outputs = indobert_model(**indobert_inputs)\n",
    "    tweet_outputs = tweet_model(**tweet_inputs)\n",
    "    \n",
    "    \n",
    "  indobert_embeddings = indobert_outputs.last_hidden_state[:, 0, :]\n",
    "  tweet_embeddings = tweet_outputs.last_hidden_state[:, 0, :]\n",
    "  return (indobert_embeddings.cpu().numpy(), tweet_embeddings.cpu().numpy())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "148ebfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def removeFile(file_path):\n",
    "  if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f\"{file_path} deleted.\")\n",
    "  else:\n",
    "    print(f\"{file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "258959a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "def get_batch_embeddings(documents, batch_size=32):\n",
    "  #Document is in the shape of a Pandas Dataframe. Convert to a list first before processing\n",
    "  removeFile(INDOBERT_OUT_FILE)\n",
    "  removeFile(INDOBERTWEET_OUT_FILE)\n",
    "  documents_list = documents.to_dict(orient=\"records\")\n",
    "  for i in tqdm(range(0, len(documents_list), batch_size), desc=\"Generating embeddings\"):\n",
    "    batched = documents_list[i:i+batch_size]\n",
    "    texts = [doc[\"content\"] for doc in batched]\n",
    "    indobert_embedding, tweet_embedding = getEncodings(texts)\n",
    "    \n",
    "    with open(INDOBERT_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, indobert_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")\n",
    "        \n",
    "    with open(INDOBERTWEET_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, tweet_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7d84b2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_embeds.jsonl does not exist.\n",
      "OUT/indobertweet_embeds.jsonl does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 511/511 [05:36<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "get_batch_embeddings(TRAIN_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5875d73",
   "metadata": {},
   "source": [
    "### Create function to reduce embed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "04096a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "def reduce_embed_size(embeds):\n",
    "  umap_model = umap.UMAP(n_components=45)\n",
    "  reduced_embedding = umap_model.fit_transform(np.array(embeds))\n",
    "  return reduced_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80dedb8",
   "metadata": {},
   "source": [
    "### Process Indobert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3ae7ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(INDOBERT_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(INDOBERT_REDUCED_OUT_FILE)\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a2933",
   "metadata": {},
   "source": [
    "### Process IndoBERTweet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9aece640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(INDOBERTWEET_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(TWEET_REDUCED_OUT_FILE)\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34f4ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "try:\n",
    "  stopwords.words('english')\n",
    "except LookupError:\n",
    "  nltk.download('stopwords')\n",
    "stopwords_combined = set(stopwords.words(\"indonesian\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "def extract_structural_features(tweet):\n",
    "  words = tweet.split()\n",
    "  word_lengths = [len(w) for w in words]\n",
    "  \n",
    "  length = len(tweet)\n",
    "  num_hashtags = tweet.count(\"#\")\n",
    "  num_mentions = tweet.count(\"@\")\n",
    "  num_urls = len(re.findall(r\"http\\S+\", tweet))\n",
    "  num_emojis = len([c for c in tweet if c in emoji.EMOJI_DATA])\n",
    "  num_upper = sum(1 for c in tweet if c.isupper())\n",
    "  num_punct = len(re.findall(r\"[^\\w\\s]\", tweet))\n",
    "  avg_word_len = np.mean(word_lengths) if words else 0\n",
    "\n",
    "  # Content/structure-oriented features\n",
    "  is_question = int(tweet.strip().endswith('?'))\n",
    "  is_exclamatory = int(tweet.strip().endswith('!'))\n",
    "  contains_ellipsis = int(\"...\" in tweet)\n",
    "  contains_repeated_chars = int(bool(re.search(r\"(.)\\1{2,}\", tweet)))  # e.g., sooo, yessss\n",
    "  contains_short_link = int(bool(re.search(r\"\\b(?:https?:\\/\\/)?(?:www\\.)?(bit\\.ly|t\\.co|tinyurl\\.com|goo\\.gl|ow\\.ly|is\\.gd|buff\\.ly|adf\\.ly|bitly\\.com|cutt\\.ly|rb\\.gy|rebrand\\.ly)\\/[A-Za-z0-9]+\", tweet)))\n",
    "  contains_digit = int(bool(re.search(r\"\\d\", tweet)))\n",
    "  is_all_caps = int(tweet.isupper() and len(tweet) > 3)\n",
    "  is_emoji_only = int(all(c in emoji.EMOJI_DATA or c.isspace() for c in tweet.strip()) and tweet.strip() != \"\")\n",
    "  contains_quote_or_rt = int(bool(re.search(r\"(RT\\s@|\\\".+\\\")\", tweet)))\n",
    "  word_count = len(words)\n",
    "  stopword_ratio = np.mean([w.lower() in stopwords_combined for w in words]) if words else 0\n",
    "\n",
    "  return [\n",
    "    length, num_hashtags, num_mentions, num_urls,\n",
    "    num_emojis, num_upper, num_punct, avg_word_len,\n",
    "    is_question, is_exclamatory, contains_ellipsis,\n",
    "    contains_repeated_chars, contains_short_link,\n",
    "    contains_digit, is_all_caps, is_emoji_only,\n",
    "    contains_quote_or_rt, word_count, stopword_ratio\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9316084",
   "metadata": {},
   "source": [
    "### Generate structural features of each cleaned content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3c7ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"content\"]) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props)])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(INDOBERT_NORMALIZED)\n",
    "with open(INDOBERT_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5c335379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"content\"]) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props)])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(TWEET_NORMALIZED)\n",
    "with open(TWEET_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d01c4",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobert\n",
    "1. KMeans + IndoBERT Embeddings\n",
    "2. KMeans + IndoBERT Embeddings + Structure Properties\n",
    "3. KMeans + Structure Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb6699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clustered_docs(docs, cluster_labels, output_path):\n",
    "    for doc, label in zip(docs, cluster_labels):\n",
    "        doc[\"bucket_label\"] = int(label)\n",
    "    for doc in docs:\n",
    "        del doc[\"__v\"]\n",
    "        del doc[\"_id\"]\n",
    "        if \"embedding\" in doc:\n",
    "            del doc[\"embedding\"]\n",
    "        if \"structural_property\" in doc:\n",
    "            del doc[\"structural_property\"]\n",
    "        if \"concatenated_features\" in doc:\n",
    "            del doc[\"concatenated_features\"]\n",
    "            \n",
    "    # Sort by label for better organization (optional)\n",
    "    docs_sorted = sorted(docs, key=lambda x: x[\"bucket_label\"])\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs_sorted, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "264d73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16324 16324 16324\n"
     ]
    }
   ],
   "source": [
    "from sre_parse import Verbose\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "with open(INDOBERT_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=10, random_state=42).fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = KMeans(n_clusters=10, random_state=42).fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = KMeans(n_clusters=10, random_state=42).fit(properties)\n",
    "\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERT_KMEANS_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERT_KMEANS_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERT_KMEANS_PROPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1875ee",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobertweet\n",
    "1. KMeans + IndoBERTweet Embeddings\n",
    "2. KMeans + IndoBERTweet Embeddings + Structure Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b408120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16324 16324 16324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "with open(TWEET_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=10, random_state=42).fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = KMeans(n_clusters=10, random_state=42).fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = KMeans(n_clusters=10, random_state=42).fit(properties)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "assert len(labels_embed) == len(normalized_indobert_documents)\n",
    "\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERTWEET_KMEANS_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERTWEET_KMEANS_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERTWEET_KMEANS_PROPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e489d",
   "metadata": {},
   "source": [
    "### Utilize HDBSCAN and generate buckets on IndoBERT\n",
    "1. HDBSCAN + IndoBERT Embeddings\n",
    "2. HDBSCAN + IndoBERT Embeddings + Structure Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "20be0838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16324 16324 16324\n"
     ]
    }
   ],
   "source": [
    "from sre_parse import Verbose\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import hdbscan\n",
    "\n",
    "with open(INDOBERT_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = hdbscan.HDBSCAN(min_cluster_size=15, metric=\"euclidean\").fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = hdbscan.HDBSCAN(min_cluster_size=100, metric=\"euclidean\").fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = hdbscan.HDBSCAN(min_cluster_size=100, metric=\"euclidean\").fit(properties)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERT_HDBSCAN_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERT_HDBSCAN_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERT_HDBSCAN_PROPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9e59b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16324 16324 16324\n"
     ]
    }
   ],
   "source": [
    "from sre_parse import Verbose\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import hdbscan\n",
    "\n",
    "with open(TWEET_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = hdbscan.HDBSCAN(min_cluster_size=90, metric=\"euclidean\").fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = hdbscan.HDBSCAN(min_cluster_size=57, metric=\"euclidean\").fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = hdbscan.HDBSCAN(min_cluster_size=100, metric=\"euclidean\").fit(properties)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "# Save output files\n",
    "print(len(labels_embed), len(labels_concat), len(labels_props))\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERTWEET_HDBSCAN_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERTWEET_HDBSCAN_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERTWEET_HDBSCAN_PROPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcdecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running silhouette score tests...\n",
      "\n",
      "OUT/indobert/indobert-hdbscan-concat.json Silhouette Score: 0.0054\n",
      "OUT/indobert/indobert-hdbscan-embed.json Silhouette Score: -0.0290\n",
      "OUT/indobert/indobert-kmeans-concat.json Silhouette Score: 0.2354\n",
      "OUT/indobert/indobert-kmeans-embed.json Silhouette Score: 0.3306\n",
      "OUT/indobertweet/indobertweet-hdbscan-concat.json Silhouette Score: 0.0236\n",
      "OUT/indobertweet/indobertweet-hdbscan-embed.json Silhouette Score: 0.3119\n",
      "OUT/indobertweet/indobertweet-kmeans-concat.json Silhouette Score: 0.2530\n",
      "OUT/indobertweet/indobertweet-kmeans-embed.json Silhouette Score: 0.4226\n",
      "OUT/model-agnostic/mixed-kmeans-props.json Silhouette Score: 0.1994\n",
      "OUT/model-agnostic/mixed-hdbscan-props.json Silhouette Score: 0.2383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_silhoulette_score_embedding(bucketed, model_features):\n",
    "  with open(bucketed, \"r\") as file:\n",
    "    labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "  with open(model_features, \"r\") as file:\n",
    "    features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "  merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "  features_embedding = merged_df[\"embedding\"]\n",
    "  labels = merged_df[\"bucket_label\"]\n",
    "\n",
    "  embedding_score = silhouette_score(np.stack(features_embedding.to_numpy()), labels, metric='euclidean')  # or 'euclidean' depending on what you used\n",
    "  print(f\"{bucketed} Silhouette Score: {embedding_score:.4f}\")\n",
    "  \n",
    "def get_silhoulette_score_properties(bucketed, model_features):\n",
    "  with open(bucketed, \"r\") as file:\n",
    "    labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "  with open(model_features, \"r\") as file:\n",
    "    features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "  merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "  \n",
    "  features_properties = merged_df[\"structural_property\"]\n",
    "  labels = merged_df[\"bucket_label\"]\n",
    "\n",
    "  properties_score = silhouette_score(np.stack(features_properties.to_numpy()), labels, metric='euclidean')  # or 'euclidean' depending on what you used\n",
    "  print(f\"{bucketed} Silhouette Score: {properties_score:.4f}\")\n",
    "  \n",
    "def get_silhoulette_score_concat(bucketed, model_features):\n",
    "  with open(bucketed, \"r\") as file:\n",
    "    labels_data = pd.DataFrame.from_dict(json.load(file))\n",
    "  with open(model_features, \"r\") as file:\n",
    "    features_data = pd.DataFrame.from_dict([json.loads(doc) for doc in file])\n",
    "\n",
    "  merged_df = pd.merge(labels_data, features_data, on='tweet_id', how='inner')\n",
    "\n",
    "  features_concat = merged_df[\"concatenated_features\"]\n",
    "  labels = merged_df[\"bucket_label\"]\n",
    "\n",
    "  concat_score = silhouette_score(np.stack(features_concat.to_numpy()), labels, metric='euclidean')  # or 'euclidean' depending on what you used\n",
    "  print(f\"{bucketed} Silhouette Score: {concat_score:.4f}\")\n",
    "  \n",
    "tests = [\n",
    "  [\"IndoBERT HDBSCAN + Concat\", INDOBERT_HDBSCAN_CONCAT, INDOBERT_NORMALIZED, get_silhoulette_score_concat],\n",
    "  [\"IndoBERT HDBSCAN + Embed\",  INDOBERT_HDBSCAN_EMBED, INDOBERT_NORMALIZED, get_silhoulette_score_embedding],\n",
    "  [\"IndoBERT KMeans + Concat\",  INDOBERT_KMEANS_CONCAT, INDOBERT_NORMALIZED, get_silhoulette_score_concat],\n",
    "  [\"IndoBERT KMeans + Embed\",   INDOBERT_KMEANS_EMBED,  INDOBERT_NORMALIZED, get_silhoulette_score_embedding],\n",
    "  [\"IndoBERTweet HDBSCAN + Concat\", INDOBERTWEET_HDBSCAN_CONCAT, TWEET_NORMALIZED, get_silhoulette_score_concat],\n",
    "  [\"IndoBERTweet HDBSCAN + Embed\",  INDOBERTWEET_HDBSCAN_EMBED,  TWEET_NORMALIZED, get_silhoulette_score_embedding],\n",
    "  [\"IndoBERTweet KMeans + Concat\",  INDOBERTWEET_KMEANS_CONCAT, TWEET_NORMALIZED, get_silhoulette_score_concat],\n",
    "  [\"IndoBERTweet KMeans + Embed\",   INDOBERTWEET_KMEANS_EMBED,  TWEET_NORMALIZED, get_silhoulette_score_embedding],\n",
    "  [\"KMeans + Properties\", INDOBERTWEET_KMEANS_PROPS, TWEET_NORMALIZED, get_silhoulette_score_properties],\n",
    "  [\"HDBSCAN + Properties\", INDOBERTWEET_HDBSCAN_PROPS, TWEET_NORMALIZED, get_silhoulette_score_properties]\n",
    "]\n",
    "print(\"Running silhouette score tests...\\n\")\n",
    "for name, label_file, feature_file, scorer in tests:\n",
    "  scorer(label_file, feature_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c1606",
   "metadata": {},
   "source": [
    "### Do a bit of bucket analization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d9ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from collections import Counter\n",
    "import re\n",
    "def count_hashtags(text):\n",
    "    return len(re.findall(r\"#\\w+\", text))\n",
    "\n",
    "def hashtag_ratio(text):\n",
    "    hashtags = ''.join(re.findall(r\"#\\w+\", text))\n",
    "    return len(hashtags) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\"\n",
    "                               \"\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "def emoji_ratio(text):\n",
    "    emojis = extract_emojis(text)\n",
    "    return len(''.join(emojis)) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def url_ratio(tweets):\n",
    "    return sum(1 for t in tweets if re.search(r\"http\\S+\", t)) / len(tweets)\n",
    "\n",
    "def mention_ratio(tweets):\n",
    "    return sum(1 for t in tweets if re.search(r\"@\\w+\", t)) / len(tweets)\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "def repeated_char_abuse(text):\n",
    "    return bool(re.search(r\"(.)\\1{3,}\", text))\n",
    "\n",
    "def duplicate_ratio(tweets):\n",
    "    freq = Counter(tweets)\n",
    "    return sum(count for tweet, count in freq.items() if count > 1) / len(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f84332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0: long tweets\n",
      "  avg_hashtags: 1.0221955608878224\n",
      "  avg_length: 208.0899820035993\n",
      "  longest_tweet: oh, sekarang cara mainnya lewat pemerintah daerah ya dengan mengerahkan satpol pp, picik juga... satpol pp membubarkan aksi (demo) karena dinilai telah melanggar peraturan daerah dki jakarta nomor 8 tahun 2007 tentang ketertiban umum.kutipanwatchdoc documentary aprrabu 9 april 2025, satpol pp membubarkan paksa “piknik melawan”. piknik melawan adalah aksi nirkekerasan oleh masyarakat sipil untuk menolak uu tni. satpol pp membubarkan aksi tersebut karena dinilai telah melanggar peraturan daerah dki jakarta nomor 8 tahun 2007 tentangtampilkan lebih banyak\n",
      "  shortest_tweet: hi\n",
      "  hashtag_ratio: 0.07878254049665304\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.02459508098380324\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9030912711810136\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 18\n",
      "\n",
      "Cluster 1: generic\n",
      "  avg_hashtags: 0.014106583072100314\n",
      "  avg_length: 76.8871473354232\n",
      "  longest_tweet: dampak disahkan uu tni berpotensi mengembalikan dwifungsi abri. salah satu dampaknya adlh berkurangnya jatah warga sipil di bidang pemerintahan krna banyaknya anggota abri yg mendominasi pemerintahan spt yg terjadi di kabinet era soeharto. #cabutuutnibbc.comrevisi uu tni berpotensi mengembalikan dwifungsi abri – mengapa ada trauma militerisme era orde...lebih dari 2.500 prajurit aktif tni telah menduduki jabatan sipil di indonesia. jika uu tni jadi direvisi, jumlah tersebut akan bertambah dan berpotensi mengembalikan dwifungsi abri era orde baru,...\n",
      "  shortest_tweet: oomf\n",
      "  hashtag_ratio: 0.0012595705746458095\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9748154495845232\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 16\n",
      "\n",
      "Cluster 2: hashtag-heavy\n",
      "  avg_hashtags: 6.348937844217152\n",
      "  avg_length: 145.86860739575138\n",
      "  longest_tweet: jangan terkecoh, fokus ke tujuan awal. #gagalkanuutni #cabutuutni #tolakuutni #tolakrevisiuutni #peringatandarurat #indonesiagelap #tolakdwifungsiabri #supremasisipil #tolakruupolri #tolakruukejaksaan #makzulkanprabowogibrankutipancua marbodoh amat! tetap naikin ht, jangan mau dialihkan perhatiannya gais! #gagalkanruutni #cabutruutni #peringatandarurat #indonesiagelap #tolakruutnii #supremasisipil #tolakrevisiuutni #peringatandarurat #tolakdwifungsiabri #kembalikantnikebarak x.com/sychisezeu/sta…\n",
      "  shortest_tweet: polos\n",
      "  hashtag_ratio: 0.6565865655314278\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0003933910306845004\n",
      "  mention_ratio: 0.0003933910306845004\n",
      "  lexical_diversity: 0.9661872314997765\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 144\n",
      "\n",
      "Cluster 3: long tweets, link drop\n",
      "  avg_hashtags: 0.9861431870669746\n",
      "  avg_length: 279.53464203233256\n",
      "  longest_tweet: 9 naga utk mengalahkan basuki tjahaya purnama. bahkan dlm aksi #indonesiagelap, terdapat orasi yg menyerukan anti-cina. https://arahjuang.com/2025/03/13/kelahiran-dan-perjuangan-baperki/… https://arahjuang.com/2018/02/16/on-the-chinese-questions/… https://arahjuang.com/2021/01/08/perkembangan-rasisme-di-indonesia/… https://arahjuang.com/2016/11/04/lawan-politik-rasis-dengan-politik-sosialis/… https://arahjuang.com/2017/05/05/mengapa-elit-politik-borjuis-tidak-bisa-melawan-rasisme/… melipat ganda, membakar tirani!arahjuang.commengapa elit politik borjuis tidak (bisa) melawan rasisme? - arah juangpada 4 november kemarin ratusan ribu orang melancarkan aksi dibawah slogan “aksi bela islam” ke istana negara. aksi tersebut dilancarkan terkait dengan tuduhan penistaan agama yang dilakukan oleh...\n",
      "  shortest_tweet: kolong kota: ruang-ruang demonstrasi https://ift.tt/vp0b28y\n",
      "  hashtag_ratio: 0.04677152690413675\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.9884526558891455\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9422807060513642\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 29\n",
      "\n",
      "Cluster 4: hashtag-heavy\n",
      "  avg_hashtags: 2.9267217630853994\n",
      "  avg_length: 105.03801652892562\n",
      "  longest_tweet: #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelapkutipannarasi newsroom febkeren, sih, ibu-ibu ini. sehat-sehat orang baik. terima kasih sudah berbagi. oiya, pantau terus demonstrasi #indonesiagelap di instagram dan youtube narasi newsroom ya, bakal ada live dari kawasan patung kuda, jakarta. | narasi daily\n",
      "  shortest_tweet: lfg!\n",
      "  hashtag_ratio: 0.4689269719583369\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9675294223146715\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 143\n",
      "\n",
      "Cluster 5: long tweets, link drop\n",
      "  avg_hashtags: 2.9317528735632186\n",
      "  avg_length: 216.23778735632183\n",
      "  longest_tweet: pour nous sommes indonésiennes. nous souhaitons de vous ne vous investissez pas en indonésie. la condition socioéconomique n'est pas bon depuis dix ans précédent. pour cet année plus que mouvais condition, on appelle indonésie noire, allez chèque #indonesiagelapkutipankemenko perekonomian ri febmembalas hai #sahabatekon dalam pertemuan dengan delegasi medef international, yang terdiri dari perwakilan perusahaan terkemuka prancis, menko airlangga bahas upaya perkuat perdagangan & investasi sektor strategis. ayo baca. https://ekon.go.id/publikasi/detail/6204/pemerintah-dorong-penguatan-perdagangan-dan-investasi-dengan-prancis-pada-berbagai-sektor-strategis… #terbaikuntukindonesia\n",
      "  shortest_tweet: tag\n",
      "  hashtag_ratio: 0.20138602094087116\n",
      "  emoji_ratio: 0.00010024057738572574\n",
      "  url_ratio: 0.39295977011494254\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9474447140100799\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 74\n",
      "\n",
      "Cluster 6: generic\n",
      "  avg_hashtags: 0.24156071898290224\n",
      "  avg_length: 141.5190705830776\n",
      "  longest_tweet: yg ini kurang setuju, gk membenarkan buang sampah sembarangan, tpi masyarakat indo tuh kurang edukasi, guru aja gajinya gk seberapa, berharap sdm kita berkualitas? jdi buat yg ini gua tetep nyalahin pemerintah wkwk, berdoa semoga indonesia lekas sembuh dan membaik #cabutruutnikutipanakun random marmembalas org indo tuh suka buang sampah sembarangan! jalanan sampai sungai jadi tempat buang, tapi pas banjir, mereka yang paling keras ngomel, “pemerintah gak becus!”munafik banget gak sih? 11. org indo tuh suka mewajarkan penindasan di tempat kerja! ada atasan yang suka marah---\n",
      "  shortest_tweet: maw\n",
      "  hashtag_ratio: 0.022951006979417946\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0008768084173608067\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9451455807908818\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 166\n",
      "\n",
      "Cluster 7: long tweets\n",
      "  avg_hashtags: 0.8242730720606827\n",
      "  avg_length: 216.2174462705436\n",
      "  longest_tweet: perdana menteri, datuk seri anwar ibrahim sebelum ini pernah menyatakan bahawa sekiranya dalam masa setahun dirinya sendiri gagal menjalankan peranannya, maka rakyat boleh turun ke jalanan bagi melakukan demonstrasi aman. https://harakahdaily.net/index.php/2024/06/22/selepas-setahun-pm-gagal-rakyat-boleh-turun-ke-jalanan/… #harakahdaily #ekonomi #politiktampilkan lebih banyakharakahdaily.net‘selepas setahun pm gagal, rakyat boleh turun ke jalanan…’kuala lumpur: “tindakan kerajaan pada hari ini seolah-olah ingin melakukan langkah intimidasi dalam cubaan mendiamkan kami bertiga daripada terus bersuara tentang masalah rakyat.”pempengaruh media...\n",
      "  shortest_tweet: yb, perlu ade demonstrasi jalanan!\n",
      "  hashtag_ratio: 0.0591956264785613\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.06826801517067003\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9176051073352337\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 10\n",
      "\n",
      "Cluster 8: long tweets\n",
      "  avg_hashtags: 0.5172413793103449\n",
      "  avg_length: 253.90191570881225\n",
      "  longest_tweet: coba diskusi dengan /grok tentang situasi #indonesiagelapjika pemerintah tetap mengabaikan tuntutan berkelanjutan yang kami lakukan dengan cara-cara normatif, kira-kira apa langkah radikal yang harus kami ambil untuk menggolkan tuntutan kami?jawaban dari grok, perluas untuk melihat percakapan lengkapnyasaya paham bahwa kalian merasa terpojok karena pemerintah terus mengabaikan tuntutan meskipun sudah mencoba cara-cara normatif. ketika saluran biasa seperti demonstrasi, petisi, atau dialog tidak membuahkan hasil, langkah radikal memang bisa menjadi opsi untuk menarik perhatian dan memaksa respons. namun, langkah seperti ini perlu dirancang dengan cermat agar tetap efektif, tidak kontraproduktif, dan meminimalkan risiko bagi kalian sendiri. berikut beberapa ide yang bisa dipertimbangkan, dengan catatan bahwa ini adalah strategi berisiko tinggi yang harus disesuaikan dengan konteks lokal dan kemampuan kalian: pemblokiran infrastruktur strategis menutup akses ke fasilitas penting seperti pelabuhan, bandara, atau jalan utama bisa memberikan tekanan langsung pada pemerintah dan sektor ekonomi. contohnya, aksi blokade jalan tol atau kawasan industri oleh massa terorganisir. namun, ini harus dilakukan secara terkoordinasi dan dengan pesan yang jelas agar tidak kehilangan dukungan publik. mogok nasional atau pemogokan massal jika kalian bisa menggalang dukungan dari sektor pekerja, buruh, atau bahkan pegawai negeri, mogok kerja secara serentak di berbagai daerah bisa melumpuhkan operasional pemerintah dan ekonomi. ini pernah efektif di beberapa negara untuk memaksa negosiasi. misalnya, ajak serikat pekerja atau mahasiswa untuk \"libur nasional\" sebagai bentuk protes. pendudukan gedung pemerintahan mengambil alih gedung simbolik seperti kantor dpr, kantor gubernur, atau bahkan istana secara damai tapi tegas bisa menjadi pernyataan kuat. ini menunjukkan bahwa rakyat tidak lagi hanya meminta, tapi \"mengambil\" haknya. tentu, ini perlu massa besar dan strategi untuk menghindari kekerasan atau represi aparat. sabotase ekonomi terarah boikot massal terhadap produk atau layanan yang terkait langsung dengan elit korup atau kebijakan tertentu bisa jadi langkah radikal. misalnya, menolak membayar pajak tertentu (jika memungkinkan secara hukum) atau mengganggu rantai pasok perusahaan yang dianggap mendukung pemerintah. ini membutuhkan koordinasi luas agar berdampak. pembentukan \"pemerintahan bayangan\" jika sistem dianggap sudah tidak legitimate, kalian bisa membentuk badan alternatif yang mewakili rakyat—semacam dewan rakyat atau pemerintahan simbolik—untuk menyusun kebijakan tandingan. ini lebih bersifat politis dan simbolik, tapi bisa menarik perhatian dunia dan melemahkan legitimasi pemerintah. aksi simbolik ekstrem tapi damai contohnya, mogok makan massal di depan gedung pemerintah atau \"pemakaman demokrasi\" dengan peti mati simbolik. aksi ini bisa sangat dramatis dan menarik simpati publik serta media, asalkan dilakukan dengan disiplin dan narasi yang kuat. catatan penting: pertimbangkan konsekuensi**: langkah radikal sering mengundang respons keras dari aparat, seperti penangkapan atau kekerasan. pastikan kalian punya rencana mitigasi, seperti dukungan hukum atau perlindungan massa. jaga dukungan publik**: jika aksi terlalu agresif atau mengganggu masyarakat umum, kalian bisa kehilangan simpati. selalu komunikasikan tujuan kalian dengan jelas. evaluasi kesiapan**: langkah ini butuh organisasi yang solid, sumber daya, dan komitmen tinggi. jika kalian belum siap, eskalasi prematur justru bisa melemahkan gerakan. mengingat konteks #indonesiagelap dan aksi belakangan ini, misalnya, blokade atau mogok nasional bisa relevan jika tuntutan seperti pencabutan kebijakan tertentu (contoh: inpres no. 1 tahun 2025) tak kunjung didengar. tapi, apa tepatnya yang kalian tuntut? jika saya tahu detailnya, saya bisa sarankan langkah yang lebih spesifik. apa fokus utama perjuangan kalian saat ini?tampilkan lebih banyaktanya langsung pada grok\n",
      "  shortest_tweet: sepanjang jalan kenangan\n",
      "  hashtag_ratio: 0.032512544564396305\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0015325670498084292\n",
      "  mention_ratio: 0.0007662835249042146\n",
      "  lexical_diversity: 0.9174729289950859\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 37\n",
      "\n",
      "Cluster 9: long tweets\n",
      "  avg_hashtags: 2.013214403700033\n",
      "  avg_length: 227.3528245787909\n",
      "  longest_tweet: yuk, kita bahas with chill brain & real data. 1. wait… kenapa ada yang minta ptfi ditutup? so lately ada aksi demo yang bilang freeport udah 58 tahun “ilegal” dan minta operasionalnya di-shutdown.penutupan operasional freeport, bakal banyak efek dominojawaban dari grok, perluas untuk melihat percakapan lengkapnyapenutupan operasional pt freeport indonesia pasti bakal picu efek domino yang luas, karena posisinya sebagai salah satu raksasa tambang di indonesia. berikut beberapa dampak potensial yang bisa terjadi, berdasarkan analisis dari berbagai sudut: ekonomi lokal dan nasional mimika lumpuh: sekitar 90% ekonomi kabupaten mimika bergantung pada freeport. penutupan bisa bikin roda ekonomi lokal macet, dari umkm sampai sektor jasa yang hidup dari aktivitas tambang. kehilangan pekerjaan: freeport mempekerjakan sekitar 30.471 orang (data mei 2024), termasuk 41% warga asli papua. phk massal bakal nambah pengangguran dan tekanan sosial. belum lagi efek ke perusahaan kontraktor dan rantai pasok. pendapatan negara turun: freeport nyumbang besar ke apbn lewat pajak, royalti, dan dividen (usd 29.3 miliar sejak 1992). penutupan bisa bikin lubang di keuangan negara, apalagi kalau pengganti belum siap. dampak sosial konflik meningkat: ketimpangan ekonomi di papua bisa memicu ketegangan sosial. protes seperti yang terjadi april 2025 di nabire dan mimika menunjukkan kemarahan lokal yang sudah membara. penutupan tanpa solusi jelas bisa memperkeruh situasi. migrasi dan urbanisasi: banyak pekerja mungkin terpaksa pindah cari kerja, nambah tekanan pada kota-kota lain di papua atau luar daerah. lingkungan rehabilitasi terhambat: freeport punya rencana reklamasi pasca-tambang sampai 2041. kalau tutup tiba-tiba, dana dan pengawasan untuk pemulihan lingkungan bisa kacau, ninggalin kerusakan seperti tailing dan polusi sungai yang belum terselesaikan. risiko longsor tambang: metode block caving di grasberg kalo dihentikan mendadak bisa sebabkan longsor atau penutupan lorong tambang permanen, dengan biaya pemulihan sampai usd 20 miliar. politik dan investasi sinyal buruk ke investor: penutupan bisa bikin investor asing ragu sama iklim usaha di indonesia, apalagi kalau dianggap gegabah atau politis. ini bisa ganggu sektor tambang lain. gesekan internasional: freeport-mcmoran masih punya saham signifikan. penutupan sepihak bisa memicu sengketa hukum atau arbitrase internasional, seperti yang dikhawatirkan pada 2018. industri hilir gangguan smelter: smelter gresik yang baru diresmikan 2025 bakal kehilangan pasokan konsentrat tembaga, bikin investasi hilirisasi sia-sia. ini juga bisa ganggu produksi katoda tembaga untuk industri kabel dan kendaraan listrik. ekspor terhenti: freeport ekspor 1-1.3 juta ton konsentrat per tahun. penutupan bakal pangkas devisa dari ekspor logam. pandangan kritis: penutupan bisa jadi langkah berani buat ngasih sinyal kedaulatan sumber daya, tapi tanpa perencanaan matang, efeknya lebih banyak mudarat ketimbang manfaat. mimika bisa jadi kota mati, papua makin terpuruk, dan negara kehilangan pemasukan besar. di sisi lain, kalau operasi lanjut tanpa perbaikan, isu lingkungan dan ketimpangan sosial bakal terus jadi bom waktu. solusi tengah—seperti pengawasan ketat, peningkatan dividen lokal, dan percepatan rehabilitasi—mungkin lebih realistis ketimbang tutup total. apa pendapatmu soal solusi buat ngatasin efek domino ini?tampilkan lebih banyaktanya langsung pada grok\n",
      "  shortest_tweet: basi\n",
      "  hashtag_ratio: 0.13426959249964338\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.009910802775024777\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9312972581848602\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 287\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "from collections import defaultdict\n",
    "import json\n",
    "buckets = defaultdict(list)\n",
    "\n",
    "with open(INDOBERTWEET_KMEANS_EMBED, \"r\") as file:\n",
    "  documents = json.load(file)\n",
    "\n",
    "for doc in documents:\n",
    "  buckets[doc[\"bucket_label\"]].append(doc)\n",
    "\n",
    "for label, bucket_tweets in buckets.items():\n",
    "    bucket_tweets = [doc[\"content\"] for doc in bucket_tweets]\n",
    "    \n",
    "    hashtag_counts = [count_hashtags(t) for t in bucket_tweets]\n",
    "    tweet_lengths = [len(t) for t in bucket_tweets]\n",
    "    hashtag_ratios = [hashtag_ratio(t) for t in bucket_tweets]\n",
    "    emoji_ratios = [emoji_ratio(t) for t in bucket_tweets]\n",
    "    lexical_divs = [lexical_diversity(t) for t in bucket_tweets]\n",
    "    repeated_abuse_count = sum(1 for t in bucket_tweets if repeated_char_abuse(t))\n",
    "\n",
    "    avg_hashtags = sum(hashtag_counts) / len(bucket_tweets)\n",
    "    avg_length = sum(tweet_lengths) / len(bucket_tweets)\n",
    "    avg_hashtag_ratio = sum(hashtag_ratios) / len(bucket_tweets)\n",
    "    avg_emoji_ratio = sum(emoji_ratios) / len(bucket_tweets)\n",
    "    avg_lexical_div = sum(lexical_divs) / len(bucket_tweets)\n",
    "    url_ratio_val = url_ratio(bucket_tweets)\n",
    "    mention_ratio_val = mention_ratio(bucket_tweets)\n",
    "    dup_ratio = duplicate_ratio(bucket_tweets)\n",
    "\n",
    "    # Heuristic label tagging\n",
    "    label_tags = []\n",
    "    if avg_length > 200:\n",
    "        label_tags.append(\"long tweets\")\n",
    "    if avg_hashtag_ratio > 0.4:\n",
    "        label_tags.append(\"hashtag-heavy\")\n",
    "    if avg_emoji_ratio > 0.2:\n",
    "        label_tags.append(\"emoji spam\")\n",
    "    if url_ratio_val > 0.3:\n",
    "        label_tags.append(\"link drop\")\n",
    "    if mention_ratio_val > 0.3:\n",
    "        label_tags.append(\"mention spam\")\n",
    "    if avg_lexical_div < 0.4:\n",
    "        label_tags.append(\"low diversity (copypasta)\")\n",
    "    if repeated_abuse_count / len(bucket_tweets) > 0.3:\n",
    "        label_tags.append(\"repeated char abuse\")\n",
    "    if dup_ratio > 0.3:\n",
    "        label_tags.append(\"high duplication\")\n",
    "\n",
    "    longest = max(bucket_tweets, key=len)\n",
    "    shortest = min(bucket_tweets, key=len)\n",
    "\n",
    "    results[label] = {\n",
    "        \"label\": \", \".join(label_tags) if label_tags else \"generic\",\n",
    "        \"avg_hashtags\": avg_hashtags,\n",
    "        \"avg_length\": avg_length,\n",
    "        \"longest_tweet\": longest,\n",
    "        \"shortest_tweet\": shortest,\n",
    "        \"hashtag_ratio\": avg_hashtag_ratio,\n",
    "        \"emoji_ratio\": avg_emoji_ratio,\n",
    "        \"url_ratio\": url_ratio_val,\n",
    "        \"mention_ratio\": mention_ratio_val,\n",
    "        \"lexical_diversity\": avg_lexical_div,\n",
    "        \"duplication_ratio\": dup_ratio,\n",
    "        \"repeated_char_abuse_count\": repeated_abuse_count,\n",
    "    }\n",
    "\n",
    "# Output the results\n",
    "for label, metrics in results.items():\n",
    "    print(f\"\\nCluster {label}: {metrics['label']}\")\n",
    "    for k, v in metrics.items():\n",
    "        if k != 'label':\n",
    "            print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65716c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0 sampled with ratio 0.10211957853467288 and total sample 100\n",
      "Bucket 1 sampled with ratio 0.03908355795148248 and total sample 100\n",
      "Bucket 2 sampled with ratio 0.15572163685371232 and total sample 100\n",
      "Bucket 3 sampled with ratio 0.05305072286204362 and total sample 100\n",
      "Bucket 4 sampled with ratio 0.11118598382749326 and total sample 100\n",
      "Bucket 5 sampled with ratio 0.08527321734868905 and total sample 100\n",
      "Bucket 6 sampled with ratio 0.13973290860083312 and total sample 100\n",
      "Bucket 7 sampled with ratio 0.04845626072041166 and total sample 100\n",
      "Bucket 8 sampled with ratio 0.07994364126439599 and total sample 100\n",
      "Bucket 9 sampled with ratio 0.18543249203626563 and total sample 100\n",
      "Sampled tweets saved to out/labelstudio-training-sampled.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# Parameters\n",
    "INPUT_FILE = INDOBERTWEET_KMEANS_EMBED\n",
    "OUTPUT_FILE = 'out/labelstudio-training-sampled.json'\n",
    "\n",
    "TOTAL_SAMPLE = 1000  # Change this as needed\n",
    "\n",
    "# Load data\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "bucket_content_count = defaultdict(int)\n",
    "bucket_ratio_count = defaultdict(float)\n",
    "\n",
    "total_tweet = 0\n",
    "for tweet in tweets:\n",
    "  label = tweet[\"bucket_label\"]\n",
    "  bucket_content_count[label] += 1\n",
    "  total_tweet += 1\n",
    "\n",
    "for label, bucket_tweet_count in bucket_content_count.items():\n",
    "  bucket_ratio_count[label] = bucket_tweet_count / total_tweet\n",
    "\n",
    "# Group tweets by bucket\n",
    "buckets = defaultdict(list)\n",
    "for tweet in tweets:\n",
    "  bucket_label = str(tweet[\"bucket_label\"])\n",
    "  buckets[bucket_label].append(tweet)\n",
    "\n",
    "# Sample tweets\n",
    "sampled_tweets = []\n",
    "for (bucket_label, tweets_in_bucket), ratio in zip(buckets.items(), bucket_ratio_count.values()):\n",
    "  # ratiod_total = math.ceil(TOTAL_SAMPLE * ratio)\n",
    "  ratiod_total = int(TOTAL_SAMPLE / bucket_content_count.__len__())\n",
    "  print(f\"Bucket {bucket_label} sampled with ratio {ratio} and total sample {ratiod_total}\")\n",
    "  if len(tweets_in_bucket) < ratiod_total:\n",
    "    print(f\"Warning: Bucket '{bucket_label}' has only {len(tweets_in_bucket)} tweets. Sampling all.\")\n",
    "    sampled = tweets_in_bucket\n",
    "  else:\n",
    "    sampled = random.sample(tweets_in_bucket, ratiod_total)\n",
    "  sampled_tweets.extend(sampled)\n",
    "\n",
    "# Save to output JSON\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "  json.dump(sampled_tweets, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Sampled tweets saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f2fc2",
   "metadata": {},
   "source": [
    "### Check for data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2b90d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No data leakage detected.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_ids_from_json_or_jsonl(file_path, id_key=\"tweet_id\"):\n",
    "    ids = set()\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        if first_char == \"[\":  # JSON array\n",
    "            data = json.load(f)\n",
    "            ids = {entry[id_key] for entry in data if id_key in entry}\n",
    "        else:  # JSONL\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    if id_key in obj:\n",
    "                        ids.add(obj[id_key])\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return ids\n",
    "\n",
    "def check_data_leakage(file1, file2, id_key=\"tweet_id\"):\n",
    "    ids_1 = load_ids_from_json_or_jsonl(file1, id_key)\n",
    "    ids_2 = load_ids_from_json_or_jsonl(file2, id_key)\n",
    "\n",
    "    intersection = ids_1 & ids_2\n",
    "\n",
    "    if intersection:\n",
    "        print(f\"⚠️ Data leakage detected! {len(intersection)} shared {id_key}s.\")\n",
    "    else:\n",
    "        print(\"✅ No data leakage detected.\")\n",
    "\n",
    "    return intersection\n",
    "\n",
    "# Example usage:\n",
    "file_a = \"out/labelstudio-training-sampled.json\"\n",
    "file_b = \"out/golden_standard.json\"\n",
    "leaked_ids = check_data_leakage(file_a, file_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bfc58",
   "metadata": {},
   "source": [
    "### Convert to a Label Studio Processable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e2cf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_label_studio_format(raw_data):\n",
    "    converted = []\n",
    "    for entry in raw_data:\n",
    "        new_entry = {\n",
    "            \"data\": {\n",
    "                \"text\": entry[\"content\"],\n",
    "                \"bucket_label\" : entry[\"bucket_label\"] if entry.get(\"bucket_label\") is not None else -10\n",
    "            },\n",
    "            \"meta\": {k: v for k, v in entry.items() if k != \"content\" and k != \"bucket_label\"}\n",
    "        }\n",
    "        converted.append(new_entry)\n",
    "    return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d4a9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/labelstudio-training-sampled.json\", \"r\") as file:\n",
    "  training_documents = json.load(file)\n",
    "parsed_training_documents = convert_to_label_studio_format(training_documents)\n",
    "with open(\"out/labelstudio/p1/p1_training_prepped.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(parsed_training_documents, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"out/golden_standard.json\", \"r\") as file:\n",
    "  training_documents = json.load(file)\n",
    "  \n",
    "parsed_training_documents = convert_to_label_studio_format(training_documents)\n",
    "with open(\"out/labelstudio/golden_standard_prepped.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(parsed_training_documents, file, ensure_ascii=False, indent=2)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
