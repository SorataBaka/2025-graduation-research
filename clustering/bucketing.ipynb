{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "11887971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents: 201583\n",
      "Loaded golden standard: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sympy import O\n",
    "OUT_DIR =\"OUT/\"\n",
    "ASSET_DIR=\"assets/\"\n",
    "with open(ASSET_DIR + \"dump-formatted.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "  RAW_DOCUMENTS = json.load(file)\n",
    "with open(OUT_DIR + \"golden-standard.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "  GOLDEN_STANDARD = json.load(file)\n",
    "\n",
    "print(\"Loaded documents:\", len(RAW_DOCUMENTS))\n",
    "print(\"Loaded golden standard:\", len(GOLDEN_STANDARD))\n",
    "\n",
    "INDOBERT_OUT_FILE = OUT_DIR + \"indobert_embeds.jsonl\"\n",
    "INDOBERTWEET_OUT_FILE = OUT_DIR + \"indobertweet_embeds.jsonl\"\n",
    "INDOBERT_REDUCED_OUT_FILE = OUT_DIR + \"indobert_reduced_embeds.jsonl\"\n",
    "TWEET_REDUCED_OUT_FILE = OUT_DIR + \"indobertweet_reduced_embeds.jsonl\"\n",
    "INDOBERT_NORMALIZED = OUT_DIR + \"indobert_normalized.jsonl\"\n",
    "TWEET_NORMALIZED = OUT_DIR + \"indobertweet_normalized.jsonl\"\n",
    "\n",
    "INDOBERT_KMEANS_EMBED = OUT_DIR + \"indobert/indobert-kmeans-embed.json\"\n",
    "INDOBERT_KMEANS_PROPS = OUT_DIR + \"model-agnostic/mixed-kmeans-props.json\"\n",
    "INDOBERT_KMEANS_CONCAT = OUT_DIR + \"indobert/indobert-kmeans-concat.json\"\n",
    "INDOBERTWEET_KMEANS_EMBED = OUT_DIR + \"indobertweet/indobertweet-kmeans-embed.json\"\n",
    "INDOBERTWEET_KMEANS_PROPS = OUT_DIR + \"model-agnostic/mixed-kmeans-props.json\"\n",
    "INDOBERTWEET_KMEANS_CONCAT = OUT_DIR + \"indobertweet/indobertweet-kmeans-concat.json\"\n",
    "\n",
    "INDOBERT_HDBSCAN_EMBED = OUT_DIR + \"indobert/indobert-hdbscan-embed.json\"\n",
    "INDOBERT_HDBSCAN_PROPS = OUT_DIR + \"model-agnostic/mixed-hdbscan-props.json\"\n",
    "INDOBERT_HDBSCAN_CONCAT = OUT_DIR + \"indobert/indobert-hdbscan-concat.json\"\n",
    "INDOBERTWEET_HDBSCAN_EMBED = OUT_DIR + \"indobertweet/indobertweet-hdbscan-embed.json\"\n",
    "INDOBERTWEET_HDBSCAN_PROPS = OUT_DIR + \"model-agnostic/mixed-hdbscan-props.json\"\n",
    "INDOBERTWEET_HDBSCAN_CONCAT = OUT_DIR + \"indobertweet/indobertweet-hdbscan-concat.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd9c36",
   "metadata": {},
   "source": [
    "### Text Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f687730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "# ─── pre-compiled patterns ────────────────────────────────────────────────\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_REPEAT   = re.compile(r'(.)\\1{2,}')       # ≥3 of same char\n",
    "_WS       = re.compile(r'\\s+')\n",
    "\n",
    "# remove from the first token that *begins* with “kutipan” (any case) to the string-end\n",
    "_KUTI_CUT = re.compile(r'(?i)kutipan.*$', re.DOTALL)\n",
    "\n",
    "def cleantext(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = _MENTION.sub(' ', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "\n",
    "    # ⇣ one liner does all the “kutipan” work; the old _KUTI_BEF/_KUTI_AFT are no longer needed\n",
    "    text = _KUTI_CUT.sub('', text)\n",
    "\n",
    "    text = emoji.demojize(text, delimiters=(' ', ' '))\n",
    "    text = _REPEAT.sub(r'\\1\\1', text)\n",
    "    text = _WS.sub(' ', text).strip().lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8466d",
   "metadata": {},
   "source": [
    "### Apply cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9d9663d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining documents after removing golden standard: 152551\n"
     ]
    }
   ],
   "source": [
    "#Remove golden standard from the raw documents\n",
    "GOLDEN_STANDARD_DF = pd.DataFrame(GOLDEN_STANDARD)\n",
    "DOCUMENT_DF = pd.DataFrame(RAW_DOCUMENTS)\n",
    "\n",
    "DOCUMENT_DF[\"content\"] = DOCUMENT_DF[\"content\"].apply(cleantext)\n",
    "DOCUMENT_DF.drop_duplicates(subset=[\"content\"], inplace=True)\n",
    "\n",
    "#Iterate through document df and remove golden standard\n",
    "for index, row in GOLDEN_STANDARD_DF.iterrows():\n",
    "  if row[\"text\"] in DOCUMENT_DF[\"content\"].values:\n",
    "    DOCUMENT_DF = DOCUMENT_DF[DOCUMENT_DF[\"content\"] != row[\"text\"]]\n",
    "    \n",
    "print(\"Remaining documents after removing golden standard:\", len(DOCUMENT_DF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb096a8c",
   "metadata": {},
   "source": [
    "#### Check for leaks between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1692b539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No leaks found between golden standard and documents.\n"
     ]
    }
   ],
   "source": [
    "## Check for leaks between golden standard and documents\n",
    "GOLDEN_STANDARD_CONTENT = set(GOLDEN_STANDARD_DF[\"text\"].values)\n",
    "LEAKS = DOCUMENT_DF[DOCUMENT_DF[\"content\"].isin(GOLDEN_STANDARD_CONTENT)]\n",
    "if not LEAKS.empty:\n",
    "    print(\"Leaks found between golden standard and documents:\")\n",
    "    print(LEAKS[[\"tweet_id\", \"content\"]])\n",
    "else:\n",
    "    print(\"No leaks found between golden standard and documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77557b79",
   "metadata": {},
   "source": [
    "### Generate splits and golden standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "372557d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             _id             tweet_id                 time  \\\n",
      "0       6822dc79c7778784da9569f0  1911247544715514098  2025-04-13T02:38:18   \n",
      "1       6822dc79c7778784da9569f1  1915217941312033078  2025-04-24T01:35:14   \n",
      "2       6822dc83c7778784da9569f3  1912379519576731805  2025-04-16T05:36:22   \n",
      "3       6822dc83c7778784da9569f4  1916851056434164088  2025-04-28T13:44:39   \n",
      "4       6822dc8dc7778784da9569f6  1914923849457492430  2025-04-23T06:06:37   \n",
      "...                          ...                  ...                  ...   \n",
      "152546  68360fa9dc3752db9ae74d42  1669665839547822081  2023-06-16T11:18:51   \n",
      "152547  68360fabdc3752db9ae74d47  1669660827341963270  2023-06-16T10:58:56   \n",
      "152548  68360faddc3752db9ae74d4c  1669653586920484865  2023-06-16T10:30:10   \n",
      "152549  68360faddc3752db9ae74d4d  1669650029014044674  2023-06-16T10:16:02   \n",
      "152550  68360fb8dc3752db9ae74d51  1669645998317248519  2023-06-16T10:00:01   \n",
      "\n",
      "                  author                                            content  \\\n",
      "0       @expenjagajokowi  tokoh nu dan islam moderat cak menyentil kiner...   \n",
      "1       @mydearest_dream           kembalikan tentara ke barak #tolakruutni   \n",
      "2           @insuccubuss  inilah kenapa instansi yang memperbolehkan pen...   \n",
      "3        @BookBenzTravel  amazing elephants group - massai mara national...   \n",
      "4            @eesangyeon      masih ga nyangka suamiku tentara #tolakruutni   \n",
      "...                  ...                                                ...   \n",
      "152546         @SatMetro  personil sat samapta polres metro bekasi melak...   \n",
      "152547     @Anang_ssanto            besuk anda yang bakal di demo mahasiswa   \n",
      "152548      @polsekgnhl9  kapolres cimahi akbp aldi subartono,s.h.,s.i.k...   \n",
      "152549           @_haye_  karena anjing gw gede, dan waktu itu ada empat...   \n",
      "152550        @KilasMasa  1976: #sowetouprising dimulai di soweto, afrik...   \n",
      "\n",
      "        comment_count  repost_count  like_count  view_count  \\\n",
      "0                   0             5           6         253   \n",
      "1                   0             0           0         120   \n",
      "2                   0             2           2         375   \n",
      "3                   0             0           1         345   \n",
      "4                   0             1           0         373   \n",
      "...               ...           ...         ...         ...   \n",
      "152546              0             0           0          21   \n",
      "152547              0             0           0         355   \n",
      "152548              0             0           0          19   \n",
      "152549              2             0           0        3057   \n",
      "152550              0             1           0          26   \n",
      "\n",
      "                        created_at  __v  \n",
      "0       2025-05-13T05:45:29.113000    0  \n",
      "1       2025-05-13T05:45:29.113000    0  \n",
      "2       2025-05-13T05:45:39.133000    0  \n",
      "3       2025-05-13T05:45:39.133000    0  \n",
      "4       2025-05-13T05:45:49.144000    0  \n",
      "...                            ...  ...  \n",
      "152546  2025-05-27T19:16:57.503000    0  \n",
      "152547  2025-05-27T19:16:59.247000    0  \n",
      "152548  2025-05-27T19:17:01.995000    0  \n",
      "152549  2025-05-27T19:17:01.995000    0  \n",
      "152550  2025-05-27T19:17:12.006000    0  \n",
      "\n",
      "[152551 rows x 11 columns]\n",
      "Final dataset size for model training: 15255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "\n",
    "DOCUMENT_DF = DOCUMENT_DF.drop_duplicates(subset=[\"content\"]).reset_index(drop=True)\n",
    "print(DOCUMENT_DF)\n",
    "TRAIN_DF, TEST_DF = train_test_split(\n",
    "  DOCUMENT_DF,\n",
    "  test_size=0.90,\n",
    "  random_state=42,\n",
    ")\n",
    "print(\"Final dataset size for model training:\", len(TRAIN_DF))\n",
    "\n",
    "with open(\"out/training_split_general.json\", \"w\") as file:\n",
    "  json.dump(TRAIN_DF.to_dict(orient=\"records\"),file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292726f",
   "metadata": {},
   "source": [
    "### Extract hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fac8fbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hashtags found: 30783\n",
      "Total cleaned hashtags: 102\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from typing import List, Hashable, Optional\n",
    "with open(\"out/training_split_general.json\", \"r\") as file:\n",
    "  documents = json.load(file)\n",
    "texts = [doc[\"content\"] for doc in documents]\n",
    "\n",
    "hashtags = []\n",
    "\n",
    "for text in texts:\n",
    "  text_split = text.split(\" \")\n",
    "  for token in text_split:\n",
    "    if token.startswith(\"#\"):\n",
    "      hashtags.append(token)\n",
    "\n",
    "def most_common_hashtags(\n",
    "    tags: List[Hashable],\n",
    "    *,\n",
    "    top_n: Optional[int] = None,\n",
    "    min_count: Optional[int] = None,\n",
    ") -> List[Hashable]:\n",
    "    if top_n is None and min_count is None:\n",
    "        raise ValueError(\"Specify either top_n or min_count\")\n",
    "\n",
    "    freq = Counter(tags)\n",
    "    # Sort once by (-count, tag) so result is deterministic for ties\n",
    "    ranked = sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "\n",
    "    if top_n is not None:\n",
    "        selected = ranked[:top_n]\n",
    "    else:\n",
    "        selected = [kv for kv in ranked if kv[1] >= min_count]\n",
    "\n",
    "    return [tag for tag, _ in selected]\n",
    "\n",
    "\n",
    "cleaned_hashtags = most_common_hashtags(hashtags, min_count=15)\n",
    "\n",
    "print(\"Total hashtags found:\", len(hashtags))\n",
    "print(\"Total cleaned hashtags:\", len(cleaned_hashtags))\n",
    "\n",
    "with open(\"out/hashtag_list.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(cleaned_hashtags, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a05d56",
   "metadata": {},
   "source": [
    "### Initialize all models and tokenizers from IndoBERT and IndoBERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "162726b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(32025, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "indobert_model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "indobert_tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\", cache_dir=\"cache/\")\n",
    "\n",
    "tweet_model = AutoModel.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "tweet_tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\", cache_dir=\"cache/\")\n",
    "\n",
    "indobert_tokenizer.add_tokens(cleaned_hashtags)\n",
    "tweet_tokenizer.add_tokens(cleaned_hashtags)\n",
    "\n",
    "indobert_model.resize_token_embeddings(len(indobert_tokenizer))\n",
    "tweet_model.resize_token_embeddings(len(tweet_tokenizer))\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "indobert_model = indobert_model.to(device)\n",
    "tweet_model = tweet_model.to(device)\n",
    "\n",
    "#Turn on evaluation mode as default\n",
    "indobert_model.eval()\n",
    "tweet_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965c6ea",
   "metadata": {},
   "source": [
    "### Create functions to get encodings for both indobert and indobertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d2c31a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def getEncodings(textArray):\n",
    "  indobert_inputs = indobert_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  tweet_inputs = tweet_tokenizer(\n",
    "    textArray,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  indobert_inputs = {k: v.to(\"mps\") for k, v in indobert_inputs.items()}\n",
    "  tweet_inputs = {k: v.to(\"mps\") for k, v in tweet_inputs.items()}\n",
    "  with torch.no_grad():\n",
    "    indobert_outputs = indobert_model(**indobert_inputs)\n",
    "    tweet_outputs = tweet_model(**tweet_inputs)\n",
    "    \n",
    "    \n",
    "  indobert_embeddings = indobert_outputs.last_hidden_state[:, 0, :]\n",
    "  tweet_embeddings = tweet_outputs.last_hidden_state[:, 0, :]\n",
    "  return (indobert_embeddings.cpu().numpy(), tweet_embeddings.cpu().numpy())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "148ebfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def removeFile(file_path):\n",
    "  if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f\"{file_path} deleted.\")\n",
    "  else:\n",
    "    print(f\"{file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "258959a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "def get_batch_embeddings(documents, batch_size=32):\n",
    "  #Document is in the shape of a Pandas Dataframe. Convert to a list first before processing\n",
    "  removeFile(INDOBERT_OUT_FILE)\n",
    "  removeFile(INDOBERTWEET_OUT_FILE)\n",
    "  documents_list = documents.to_dict(orient=\"records\")\n",
    "  for i in tqdm(range(0, len(documents_list), batch_size), desc=\"Generating embeddings\"):\n",
    "    batched = documents_list[i:i+batch_size]\n",
    "    texts = [doc[\"content\"] for doc in batched]\n",
    "    indobert_embedding, tweet_embedding = getEncodings(texts)\n",
    "    \n",
    "    with open(INDOBERT_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, indobert_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")\n",
    "        \n",
    "    with open(INDOBERTWEET_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "      for doc, embed in zip(batched, tweet_embedding):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy[\"embedding\"] = embed.tolist()\n",
    "        file.write(json.dumps(doc_copy, ensure_ascii=False)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7d84b2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_embeds.jsonl deleted.\n",
      "OUT/indobertweet_embeds.jsonl deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 477/477 [03:18<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "get_batch_embeddings(TRAIN_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5875d73",
   "metadata": {},
   "source": [
    "### Create function to reduce embed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "04096a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "def reduce_embed_size(embeds):\n",
    "  umap_model = umap.UMAP(n_components=50)\n",
    "  reduced_embedding = umap_model.fit_transform(np.array(embeds))\n",
    "  return reduced_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80dedb8",
   "metadata": {},
   "source": [
    "### Reduce embedding size to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3ae7ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_reduced_embeds.jsonl deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_reduced_embeds.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(INDOBERT_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(INDOBERT_REDUCED_OUT_FILE)\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(INDOBERTWEET_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  embedded_documents = []\n",
    "  for line in file:\n",
    "    doc = json.loads(line)\n",
    "    embedded_documents.append(doc)\n",
    "\n",
    "embeddings = [doc[\"embedding\"] for doc in embedded_documents]\n",
    "reduced_embeddings = reduce_embed_size(embeddings)\n",
    "\n",
    "for doc, reduced in zip(embedded_documents, reduced_embeddings):\n",
    "  doc[\"embedding\"] = reduced.tolist()\n",
    "removeFile(TWEET_REDUCED_OUT_FILE)\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in embedded_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4152416",
   "metadata": {},
   "source": [
    "#### Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "34f4ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "try:\n",
    "  stopwords.words('english')\n",
    "except LookupError:\n",
    "  nltk.download('stopwords')\n",
    "\n",
    "# Read the javanese and sundanese stopwords from the assets folder\n",
    "with open(ASSET_DIR + \"local_languages_stopwords.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "  #Read headers indonesian, javanese, sundanese\n",
    "  local_stopwords = pd.read_csv(file, header=None, names=[\"indonesian\", \"javanese\", \"sundanese\"])\n",
    "\n",
    "javanese_stopwords = set(local_stopwords[\"javanese\"].dropna().tolist())\n",
    "sundanese_stopwords = set(local_stopwords[\"sundanese\"].dropna().tolist())\n",
    "\n",
    "stopwords_combined = set(stopwords.words(\"indonesian\")) | set(stopwords.words(\"english\")) | javanese_stopwords | sundanese_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c2a7d",
   "metadata": {},
   "source": [
    "#### Initialize the function to generate structural features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "98b14581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import emoji\n",
    "\n",
    "# ── pre‑compiled patterns ───────────────────────────────────────────────────────\n",
    "URL_RE   = re.compile(r\"http\\S+\")\n",
    "EMOJI_RE = re.compile(\"|\".join(re.escape(e) for e in emoji.EMOJI_DATA))\n",
    "\n",
    "def extract_structural_features(tweet: str, stopwords_set: set) -> list[float]:\n",
    "    \"\"\"\n",
    "    Return a list of lightweight structural features for a single tweet.\n",
    "\n",
    "    Features (18 → 10 after pruning):\n",
    "      0. length                – total characters\n",
    "      1. num_hashtags          – count of '#'\n",
    "      2. num_mentions          – count of '@'\n",
    "      3. num_urls              – URLs matched by URL_RE\n",
    "      4. num_emojis            – emojis matched by EMOJI_RE\n",
    "      5. num_upper             – uppercase characters\n",
    "      6. num_punct             – non‑alphanum / non‑whitespace\n",
    "      7. avg_word_len          – mean token length (0 if no tokens)\n",
    "      8. word_count            – number of whitespace‑split tokens\n",
    "      9. stopword_ratio        – fraction of tokens that are stopwords\n",
    "    \"\"\"\n",
    "    # fast counters\n",
    "    length        = len(tweet)\n",
    "    num_hashtags  = tweet.count(\"#\")\n",
    "    num_mentions  = tweet.count(\"@\")\n",
    "    num_urls      = len(URL_RE.findall(tweet))\n",
    "    num_emojis    = len(EMOJI_RE.findall(tweet))\n",
    "    num_upper     = sum(c.isupper() for c in tweet)\n",
    "    num_punct     = sum(1 for c in tweet if not c.isalnum() and not c.isspace())\n",
    "\n",
    "    # token‑level stats\n",
    "    words         = tweet.split()\n",
    "    word_lengths  = [len(w) for w in words]\n",
    "    avg_word_len  = np.mean(word_lengths) if word_lengths else 0.0\n",
    "    word_count    = len(words)\n",
    "    stopword_ratio= (\n",
    "        np.mean([w.lower() in stopwords_set for w in words]) if words else 0.0\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        length, num_hashtags, num_mentions, num_urls,\n",
    "        num_emojis, num_upper, num_punct, avg_word_len,\n",
    "        word_count, stopword_ratio,\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9316084",
   "metadata": {},
   "source": [
    "### Generate structural features of each cleaned content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a3c7ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobert_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(INDOBERT_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"content\"], stopwords_combined) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props)])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(INDOBERT_NORMALIZED)\n",
    "with open(INDOBERT_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5c335379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT/indobertweet_normalized.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "concat_scaler = StandardScaler()\n",
    "props_scaler = StandardScaler()\n",
    "with open(TWEET_REDUCED_OUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "  indobert_reduced_documents = [json.loads(line) for line in file]\n",
    "\n",
    "structural_properties = [extract_structural_features(doc[\"content\"], stopwords_combined) for doc in indobert_reduced_documents]\n",
    "\n",
    "for doc, props in zip(indobert_reduced_documents, structural_properties):\n",
    "  doc[\"structural_property\"] = props\n",
    "  doc[\"concatenated_features\"] = np.concatenate([np.array(doc[\"embedding\"]), np.array(props) * 2])\n",
    "\n",
    "props_scaled = props_scaler.fit_transform(np.array(structural_properties))\n",
    "concat_scaled = concat_scaler.fit_transform(\n",
    "    np.array([doc[\"concatenated_features\"] for doc in indobert_reduced_documents])\n",
    ")\n",
    "for doc, scaled_prop, scaled_concat in zip(indobert_reduced_documents, props_scaled, concat_scaled):\n",
    "  doc[\"structural_property\"] = scaled_prop.tolist()\n",
    "  doc[\"concatenated_features\"] = scaled_concat.tolist()\n",
    "  \n",
    "removeFile(TWEET_NORMALIZED)\n",
    "with open(TWEET_NORMALIZED, \"a\", encoding=\"utf-8\") as file:\n",
    "  for doc in indobert_reduced_documents:\n",
    "    file.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396446d",
   "metadata": {},
   "source": [
    "#### Initialize method to save clustered docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d815540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clustered_docs(docs, cluster_labels, output_path):\n",
    "    for doc, label in zip(docs, cluster_labels):\n",
    "        doc[\"bucket_label\"] = int(label)\n",
    "    for doc in docs:\n",
    "        del doc[\"__v\"]\n",
    "        del doc[\"_id\"]\n",
    "        if \"embedding\" in doc:\n",
    "            del doc[\"embedding\"]\n",
    "        if \"structural_property\" in doc:\n",
    "            del doc[\"structural_property\"]\n",
    "        if \"concatenated_features\" in doc:\n",
    "            del doc[\"concatenated_features\"]\n",
    "            \n",
    "    # Sort by label for better organization (optional)\n",
    "    docs_sorted = sorted(docs, key=lambda x: x[\"bucket_label\"])\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs_sorted, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c5c38",
   "metadata": {},
   "source": [
    "#### Generate code to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "059ae25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "\n",
    "\n",
    "# ── helpers ───────────────────────────────────────────────────────────────────\n",
    "def _load_merged(bucket_file: Union[str, Path],\n",
    "                 features_file: Union[str, Path]) -> pd.DataFrame:\n",
    "    labels = pd.read_json(Path(bucket_file))\n",
    "    labels[\"content\"] = labels[\"content\"].astype(str)\n",
    "\n",
    "    with open(features_file) as f:\n",
    "        feats = pd.DataFrame(json.loads(line) for line in f)\n",
    "\n",
    "    return labels.merge(feats, on=\"content\", how=\"inner\")\n",
    "\n",
    "\n",
    "def _gini(vals: np.ndarray) -> float:\n",
    "    v = np.sort(vals.astype(np.float64))\n",
    "    if v.size == 0:\n",
    "        return np.nan\n",
    "    v += 1e-10\n",
    "    n = v.size\n",
    "    cum = np.cumsum(v)\n",
    "    return (n + 1 - 2 * np.sum(cum) / cum[-1]) / n\n",
    "\n",
    "\n",
    "def _safe_score(func, X, y):\n",
    "    return np.nan if len(np.unique(y)) < 2 else func(X, y)\n",
    "\n",
    "\n",
    "# ── core ──────────────────────────────────────────────────────────────────────\n",
    "def evaluate_views(bucket_embed: Union[str, Path],\n",
    "                   bucket_props: Union[str, Path],\n",
    "                   bucket_concat: Union[str, Path],\n",
    "                   features_file: Union[str, Path]) -> None:\n",
    "    \"\"\"\n",
    "    For each clustering run (embeds / props / concat) print Gini, DBI, Silhouette\n",
    "    with its *own* labels and matching feature vectors.\n",
    "    \"\"\"\n",
    "    spec = [\n",
    "        (bucket_embed,  \"Embedding\",  \"embedding\"),\n",
    "        (bucket_props,  \"Properties\", \"structural_property\"),\n",
    "        (bucket_concat, \"Concat\",     \"concatenated_features\"),\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "    for bucket, label, col in spec:\n",
    "        df = _load_merged(bucket, features_file)\n",
    "        y  = df[\"bucket_label\"].to_numpy()\n",
    "        X  = np.stack(df[col].to_numpy())\n",
    "\n",
    "        counts = pd.Series(y).value_counts().sort_index()\n",
    "        rows.append({\n",
    "            \"View\":           label,\n",
    "            \"Gini\":           f\"{_gini(counts.values):.4f}\",\n",
    "            \"Clusters\":       len(counts),\n",
    "            \"Noise\":          (y == -1).sum(),\n",
    "            \"DBI\":            f\"{_safe_score(davies_bouldin_score, X, y):.4f}\",\n",
    "            \"Silhouette\":     f\"{_safe_score(silhouette_score,       X, y):.4f}\",\n",
    "        })\n",
    "\n",
    "    print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d01c4",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobert\n",
    "1. KMeans + IndoBERT Embeddings\n",
    "2. KMeans + IndoBERT Embeddings + Structure Properties\n",
    "3. KMeans + Structure Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "264d73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating KMeans on Indobert Normalized\n",
      "      View   Gini  Clusters  Noise    DBI Silhouette\n",
      " Embedding 0.3544        10      0 0.8870     0.3831\n",
      "Properties 0.5000        10      0 0.7502     0.2640\n",
      "    Concat 0.3724        10      0 1.3181     0.3238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import copy\n",
    "\n",
    "with open(INDOBERT_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=10, random_state=42).fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = KMeans(n_clusters=10, random_state=42).fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = KMeans(n_clusters=10, random_state=42).fit(properties)\n",
    "\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERT_KMEANS_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERT_KMEANS_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERT_KMEANS_PROPS)\n",
    "\n",
    "print(\"Evaluating KMeans on Indobert Normalized\")\n",
    "evaluate_views(INDOBERT_KMEANS_EMBED, INDOBERT_KMEANS_PROPS, INDOBERT_KMEANS_CONCAT, INDOBERT_NORMALIZED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1875ee",
   "metadata": {},
   "source": [
    "### Utilize KMeans and generate buckets on indobertweet\n",
    "1. KMeans + IndoBERTweet Embeddings\n",
    "2. KMeans + IndoBERTweet Embeddings + Structure Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9b408120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating KMeans on Indobertweet Normalized\n",
      "      View   Gini  Clusters  Noise    DBI Silhouette\n",
      " Embedding 0.3616        10      0 0.8610     0.3755\n",
      "Properties 0.5000        10      0 0.7502     0.2640\n",
      "    Concat 0.3310        10      0 1.2840     0.3026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import copy\n",
    "\n",
    "with open(TWEET_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "kmeans_embed = KMeans(n_clusters=10, random_state=42).fit(embeds)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "kmeans_concat = KMeans(n_clusters=10, random_state=42).fit(concats)\n",
    "\n",
    "# 3. Props only\n",
    "kmeans_props = KMeans(n_clusters=10, random_state=42).fit(properties)\n",
    "\n",
    "labels_embed = kmeans_embed.labels_\n",
    "labels_concat = kmeans_concat.labels_\n",
    "labels_props = kmeans_props.labels_\n",
    "\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERTWEET_KMEANS_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERTWEET_KMEANS_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERTWEET_KMEANS_PROPS)\n",
    "\n",
    "print(\"Evaluating KMeans on Indobertweet Normalized\")\n",
    "evaluate_views(INDOBERTWEET_KMEANS_EMBED, INDOBERTWEET_KMEANS_PROPS, INDOBERTWEET_KMEANS_CONCAT, TWEET_NORMALIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e489d",
   "metadata": {},
   "source": [
    "### Utilize HDBSCAN and generate buckets on IndoBERT\n",
    "1. HDBSCAN + IndoBERT Embeddings\n",
    "2. HDBSCAN + IndoBERT Embeddings + Structure Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "20be0838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      View   Gini  Clusters  Noise    DBI Silhouette\n",
      " Embedding 0.8575         8     12 1.1851     0.0830\n",
      "Properties 0.7518        10   6682 1.4964    -0.0508\n",
      "    Concat 0.8953        11     16 1.5901     0.2624\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import copy\n",
    "import hdbscan\n",
    "\n",
    "with open(INDOBERT_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only\n",
    "hdbscan_embed = hdbscan.HDBSCAN(min_cluster_size=8, metric=\"euclidean\").fit(embeds)\n",
    "\n",
    "# 3. Props only\n",
    "hdbscan_props = hdbscan.HDBSCAN(min_cluster_size=30, metric=\"euclidean\").fit(properties)\n",
    "\n",
    "# 2. Embeddings + Props\n",
    "hdbscan_concat = hdbscan.HDBSCAN(min_cluster_size=3, metric=\"euclidean\").fit(concats)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = hdbscan_embed.labels_\n",
    "labels_concat = hdbscan_concat.labels_\n",
    "labels_props = hdbscan_props.labels_\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERT_HDBSCAN_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERT_HDBSCAN_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERT_HDBSCAN_PROPS)\n",
    "\n",
    "evaluate_views(INDOBERT_HDBSCAN_EMBED, INDOBERT_HDBSCAN_PROPS, INDOBERT_HDBSCAN_CONCAT, INDOBERT_NORMALIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "d9e59b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      View   Gini  Clusters  Noise    DBI Silhouette\n",
      " Embedding 0.8329         7     53 0.7990     0.2109\n",
      "Properties 0.7696        11   6265 1.5427    -0.0434\n",
      "    Concat 0.8771        11    157 1.3879     0.2162\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import copy\n",
    "import hdbscan\n",
    "\n",
    "with open(TWEET_NORMALIZED, \"r\", encoding=\"utf-8\") as file:\n",
    "  normalized_indobert_documents = [json.loads(line) for line in file]\n",
    "\n",
    "embeds = [doc[\"embedding\"] for doc in normalized_indobert_documents]\n",
    "properties = [doc[\"structural_property\"] for doc in normalized_indobert_documents]\n",
    "concats = [doc[\"concatenated_features\"] for doc in normalized_indobert_documents]\n",
    "\n",
    "# 1. Embeddings only #before waas  18\n",
    "hdbscan_embed = hdbscan.HDBSCAN(min_cluster_size=29, metric=\"euclidean\").fit(embeds)\n",
    "\n",
    "# 3. Props only\n",
    "hdbscan_props = hdbscan.HDBSCAN(min_cluster_size=27, metric=\"euclidean\").fit(properties)\n",
    "\n",
    "# 2. Embeddings + Props #before was 17\n",
    "hdbscan_concat = hdbscan.HDBSCAN(min_cluster_size=20, metric=\"euclidean\").fit(concats)\n",
    "\n",
    "# Cluster label predictions\n",
    "labels_embed = hdbscan_embed.labels_\n",
    "labels_concat = hdbscan_concat.labels_\n",
    "labels_props = hdbscan_props.labels_\n",
    "  \n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_embed, INDOBERTWEET_HDBSCAN_EMBED)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_concat, INDOBERTWEET_HDBSCAN_CONCAT)\n",
    "save_clustered_docs(copy.deepcopy(normalized_indobert_documents), labels_props, INDOBERTWEET_HDBSCAN_PROPS)\n",
    "\n",
    "evaluate_views(INDOBERTWEET_HDBSCAN_EMBED, INDOBERTWEET_HDBSCAN_PROPS, INDOBERTWEET_HDBSCAN_CONCAT, TWEET_NORMALIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c1606",
   "metadata": {},
   "source": [
    "### Do a bit of bucket analization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "18d9ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from collections import Counter\n",
    "import re\n",
    "def count_hashtags(text):\n",
    "    return len(re.findall(r\"#\\w+\", text))\n",
    "\n",
    "def hashtag_ratio(text):\n",
    "    hashtags = ''.join(re.findall(r\"#\\w+\", text))\n",
    "    return len(hashtags) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\"\n",
    "                               \"\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "def emoji_ratio(text):\n",
    "    emojis = extract_emojis(text)\n",
    "    return len(''.join(emojis)) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def url_ratio(tweets):\n",
    "    return sum(1 for t in tweets if re.search(r\"http\\S+\", t)) / len(tweets)\n",
    "\n",
    "def mention_ratio(tweets):\n",
    "    return sum(1 for t in tweets if re.search(r\"@\\w+\", t)) / len(tweets)\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "def repeated_char_abuse(text):\n",
    "    return bool(re.search(r\"(.)\\1{3,}\", text))\n",
    "\n",
    "def duplicate_ratio(tweets):\n",
    "    freq = Counter(tweets)\n",
    "    return sum(count for tweet, count in freq.items() if count > 1) / len(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "c8f84332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0: long tweets\n",
      "  avg_hashtags: 1.0710659898477157\n",
      "  avg_length: 230.65989847715736\n",
      "  longest_tweet: buat yang suka demo atau protes, mending kalian serang yang ini. ada puluhan ribu triliun di situ. rebut, kaya jaman penjajahan belanda duludaftar perusahaan asing yang menguasai sda indonesia dan jumlah keuntungannyaanswer by grokberikut adalah daftar beberapa perusahaan asing yang mengelola sumber daya alam (sda) di indonesia, khususnya di sektor pertambangan dan migas, berdasarkan informasi yang tersedia. namun, data spesifik mengenai jumlah keuntungan sering kali tidak diungkap secara rinci dalam sumber publik karena bersifat rahasia perusahaan atau hanya dilaporkan secara agregat. saya akan mencantumkan perusahaan-perusahaan yang dikenal memiliki peran besar dalam pengelolaan sda indonesia beserta informasi keuntungan yang tersedia dari sumber terpercaya. jika data keuntungan tidak tersedia, saya akan menjelaskan kontribusi atau operasi mereka. pt freeport indonesia (freeport-mcmoran, amerika serikat) sektor: tambang emas, tembaga, dan perak di grasberg, papua. operasi: mengelola salah satu tambang emas dan tembaga terbesar di dunia. freeport indonesia telah beroperasi sejak 1967. pemerintah indonesia melalui inalum memiliki 51,23% saham sejak 2018, sedangkan freeport-mcmoran memiliki sisanya. keuntungan: laporan spesifik tentang keuntungan tahunan freeport indonesia sulit didapat karena data keuangan biasanya dilaporkan oleh freeport-mcmoran secara global. pada 2012, pendapatan bersih freeport-mcmoran secara global mencapai miliaran dolar, tetapi tidak ada angka spesifik untuk indonesia. freeport mengalokasikan investasi besar, seperti $17,3 miliar untuk pengembangan tambang bawah tanah dan smelter. catatan: meskipun indonesia kini memiliki mayoritas saham, sebagian besar hasil tambang diekspor sebagai konsentrat, dan keuntungan besar masih mengalir ke pihak asing sebelum divestasi. chevron corporation (amerika serikat) sektor: minyak dan gas (migas). operasi: melalui pt chevron pacific indonesia, chevron mengelola blok rokan dan siak di sumatra (produksi minyak terbesar di indonesia, sekitar 35-40% dari total produksi nasional) serta blok migas lepas pantai di kalimantan timur dan papua. keuntungan: tidak ada data spesifik keuntungan tahunan chevron pacific indonesia di sumber yang tersedia. namun, sebagai produsen minyak mentah terbesar di indonesia, kontribusinya signifikan terhadap devisa ekspor migas. chevron juga mengelola proyek geothermal di gunung salak, jawa barat. catatan: chevron telah beroperasi di indonesia sejak 1952 (dulunya caltex). total e&p indonesie (totalenergies, prancis) sektor: migas. operasi: mengelola blok mahakam di kalimantan timur, salah satu blok gas terbesar di indonesia. keuntungan: data keuntungan spesifik untuk total e&p indonesie tidak tersedia dalam sumber. namun, blok mahakam menghasilkan gas alam dalam jumlah besar, yang sebagian diekspor. catatan: totalenergies bekerja sama dengan pertamina dalam pengelolaan blok ini, tetapi keuntungan besar mengalir ke perusahaan asing melalui bagi hasil. british petroleum (bp, inggris) sektor: migas. operasi: menguasai 37,16% saham di proyek tangguh, papua, yang menghasilkan 7,6 juta ton lng per tahun, sebagian diekspor ke amerika serikat dan china. bp juga mengelola blok migas baru melalui kontrak bagi hasil. keuntungan: tidak ada data spesifik keuntungan proyek tangguh. namun, cadangan gas sebesar 14,4 triliun kaki kubik menunjukkan skala operasi yang besar dan profitabilitas tinggi. catatan: bp berjanji mengalokasikan sebagian gas untuk kebutuhan domestik (pln) setelah pengembangan kilang lng train 3 dan 4. exxonmobil (amerika serikat) sektor: migas. operasi: melalui mobil cepu ltd, exxonmobil mengelola blok cepu di jawa tengah, dengan cadangan minyak mentah 1,4 miliar barel dan gas 8,14 miliar kaki kubik (lapangan banyu urip). keuntungan: tidak ada data keuntungan spesifik untuk indonesia. namun, skala cadangan menunjukkan potensi keuntungan besar. catatan: exxonmobil mengambil alih blok ini dari royal dutch shell dan bekerja sama dengan pertamina. pt newmont nusa tenggara (newmont corporation, amerika serikat) sektor: tambang emas dan tembaga. operasi: mengelola tambang emas dan tembaga di nusa tenggara timur (ntt) dan nusa tenggara barat (ntb), terutama di tambang batu hijau. keuntungan: pada 2012, newmont mencatat pendapatan bersih $4,17 juta (sekitar rp58 miliar) dari operasi di ntb. catatan: newmont telah menghadapi kritik karena dampak lingkungan, seperti limbah tailing di sungai-sungai papua. niko resources (kanada) sektor: migas. operasi: mengelola 12 blok migas di indonesia, dengan 58% lahan eksplorasinya berada di indonesia. keuntungan: tidak ada data keuntungan spesifik dalam sumber. catatan: fokus pada pengeboran lepas pantai, bekerja sama dengan perusahaan lokal seperti indika energy. perusahaan tambang batubara (berau coal, rognar holding b.v, belanda; sojitz corp, jepang) sektor: batubara. operasi: mengoperasikan tambang batubara binunan lati dan sambarata di kalimantan timur, menghasilkan batubara termal dan metalurgi. keuntungan: tidak ada data keuntungan spesifik. catatan: berau coal bekerja sama dengan perusahaan indonesia, tetapi saham mayoritas dipegang asing. essar steel (india, terdaftar di inggris) sektor: batubara dan baja. operasi: menguasai tambang batubara aries di indonesia untuk mendukung pembangkit listrik dan produksi baja. menguasai 35% pangsa pasar baja domestik di indonesia. keuntungan: tidak ada data spesifik. catatan: fokus pada pengamanan pasokan batubara untuk operasi mereka. gmr energy (india) sektor: batubara. operasi: membeli 100% saham pt barasentosa lestari, yang mengelola dua blok batubara di sumatra selatan. keuntungan: tidak ada data spesifik. catatan: bagian dari strategi gmr untuk mendukung pembangkit listrik global mereka. catatan penting: keterbatasan data keuntungan**: banyak perusahaan asing tidak merilis data keuntungan spesifik per negara karena pelaporan keuangan dilakukan secara global atau melalui anak perusahaan. keuntungan sering kali dihitung dalam bagi hasil dengan pemerintah indonesia (melalui kontrak karya atau psc untuk migas). dampak dan kritik**: pengelolaan sda oleh perusahaan asing sering dikritik karena eksploitasi berlebihan, pencemaran lingkungan (misalnya limbah tailing), dan keuntungan yang tidak sebanding dengan kesejahteraan masyarakat lokal. uu no. 4 tahun 2009 tentang pertambangan mineral dan batubara mewajibkan perusahaan asing membangun smelter untuk mengolah bahan mentah di dalam negeri, tetapi implementasinya masih terbatas. regulasi**: pasal 33 uud 1945 menegaskan bahwa sda harus dikuasai negara untuk kemakmuran rakyat. pemerintah indonesia telah mendorong divestasi saham (misalnya freeport) dan melarang ekspor mineral mentah untuk meningkatkan nilai tambah dalam negeri. saran: untuk data keuntungan yang lebih akurat, anda dapat merujuk laporan tahunan perusahaan di bursa saham (jika publik) atau laporan badan koordinasi penanaman modal (bkpm). sumber seperti kementerian esdm atau skk migas juga dapat memberikan gambaran lebih rinci tentang bagi hasil migas. jika anda ingin informasi lebih spesifik tentang perusahaan tertentu, silakan beri tahu, dan saya bisa mencoba mencari data tambahan!tampilkan lebih banyaktanya langsung pada grok\n",
      "  shortest_tweet: mhdhh bestiy (ak)\n",
      "  hashtag_ratio: 0.07118141261975154\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.00036258158085569254\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.8949311426235029\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 1: hashtag-heavy\n",
      "  avg_hashtags: 2.455389007851535\n",
      "  avg_length: 76.06423982869379\n",
      "  longest_tweet: sbenernya literatur tentang demonstrasi & mobilisasi punya ruang sendiri. sayangnya gak banyak scholars indo yang sistematis dan konsisten nyemplung di sini. \"vivid political events should be important catalysts because they can have significant effects\"meridian.allenpress.comthe independent psychological effects of participation in demonstrationsstreet demonstrations have received the lion's share of scholarly attention to collective action. this article starts by returning to this research in order to raise some methodological questions..\n",
      "  shortest_tweet: tag\n",
      "  hashtag_ratio: 0.5223042774769733\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0007137758743754461\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9708089426554464\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 1\n",
      "\n",
      "Cluster 2: generic\n",
      "  avg_hashtags: 0.2267864783910997\n",
      "  avg_length: 193.43945228925975\n",
      "  longest_tweet: \"ibu ketua umum (megawati) mendukung revisi ini setelah melihat bahwa isinya sudah disesuaikan dengan harapan reformasi dan tidak membuka ruang bagi dwifungsi,\" ujar puan dalam konferensi pers di senayan.news.detik.compuan ungkap megawati dukung revisi uu tni: sesuai yang diharapkanpuan mengungkap sikap megawati mendukung revisi uu tni. puan menyebut isi perubahan uu tni itu sesuai dengan apa yang diharapkan.\n",
      "  shortest_tweet: ril?\n",
      "  hashtag_ratio: 0.020170591698760468\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9231468527047004\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 1\n",
      "\n",
      "Cluster 3: hashtag-heavy\n",
      "  avg_hashtags: 10.345679012345679\n",
      "  avg_length: 183.78306878306879\n",
      "  longest_tweet: acab #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap #indonesiagelap\n",
      "  shortest_tweet: #cabutuutni #tolakruupolri #indonesiagelap #tolakrevisiuutni !!\n",
      "  hashtag_ratio: 0.8785067694193518\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9384418546735396\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 4: generic\n",
      "  avg_hashtags: 2.0289532293986636\n",
      "  avg_length: 135.7521476296532\n",
      "  longest_tweet: soal ruu tni, ksad tepis bikin orde baru lagi: jangan berpikir masa lalu \"bukannya kita mau bikin orde baru lagi. kok pikirnya ke situ terus? orang yang berpikir-berpikir seperti ini maaf ya, jangan jadi berpikir masa lalu lah.\" >> <url> #detikcomnews.detik.comsoal ruu tni, ksad tepis bikin orde baru lagi: jangan berpikir masa lalu\"bukannya kita mau bikin orde baru lagi. kok pikirnya ke situ terus? orang yang berpikir-berpikir seperti ini maaf ya, jangan jadi berpikir masa lalu lah.\"\n",
      "  shortest_tweet: stop gajelas bs\n",
      "  hashtag_ratio: 0.25254656747146503\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9583264749070686\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 5: hashtag-heavy\n",
      "  avg_hashtags: 7.292093023255814\n",
      "  avg_length: 154.91627906976746\n",
      "  longest_tweet: bukannya bersyukur ada artis luar yg bersuara soal indo mlh dikatain target pasar.artis indo noh pda diem-diem bae #gagalkanruutni #cabutruutni #peringatandarurat #indonesiagelap #tolakruutni #tolakrevisiuutni #tolakdwifungsiabri #supremasisipil #tolakruupolri #tolakruukejaksaan\n",
      "  shortest_tweet: spill mereka!! #indonesiagelap\n",
      "  hashtag_ratio: 0.7420128896284973\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9767873332202504\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 6: generic\n",
      "  avg_hashtags: 1.7712962962962964\n",
      "  avg_length: 102.48333333333333\n",
      "  longest_tweet: aksi seperti #indonesiagelap kerap dimanfaatkan oleh kelompok tertentu untuk memecah belah masyarakat. mari kita hadapi dengan bijak, hindari menyebarkan konten yang bersifat provokatif & terus menjaga persatuan serta kesatuan nkri. #indonesiamaju #indonesiacerah #tolakajakandemo\n",
      "  shortest_tweet: fokus ruu tni\n",
      "  hashtag_ratio: 0.21804761747407375\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9703918618198425\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 7: generic\n",
      "  avg_hashtags: 0.2737567700640079\n",
      "  avg_length: 73.18266863613984\n",
      "  longest_tweet: kita mana bisa begini wkwkwk, bsk kita mah bayar beacukai berkali-kali lipat dari yang udh ditetapin sekarang. gws lu pada yang masih pada buta politik anj. #tolakpilkadaakal2an #tolakpolitikdinastijokowi #peringatandarurat #indonesiaemergencydemocracydari aqwam fiazmi hanifan\n",
      "  shortest_tweet: aku\n",
      "  hashtag_ratio: 0.057361278116902586\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9708793963846148\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 1\n",
      "\n",
      "Cluster 8: generic\n",
      "  avg_hashtags: 1.5161676646706588\n",
      "  avg_length: 145.32814371257484\n",
      "  longest_tweet: salam.. dari pada komentar tidak jelas lebih baik bapak protes dan demonstrasi ke kedubes mesir , jordan , saudi , turki , jolani..! nafis aprsaya tdk setuju evakuasi warga gaza ke indonesia krn masalahnya bukan warga gaza tapi krn srael. maka srael yg diberhentikan menyerangnya. apa ada jaminan mereka keluar bisa balik? bukankah mereka sengaja dikeluarkan utk memasukan srael ke palestina . <url>\n",
      "  shortest_tweet: #tolakruutni <url>\n",
      "  hashtag_ratio: 0.13109392424538557\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 0.9626283994914214\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n",
      "\n",
      "Cluster 9: generic\n",
      "  avg_hashtags: 4.464285714285714\n",
      "  avg_length: 158.46428571428572\n",
      "  longest_tweet: hasil pengeluaran new york evening hari ini senin, 24-2-2025 result :2884 selamat kepada pemenang #lotiongemoyg2g #kamibersamasukatani #indonesiagelap #galaxywatchultra #gaza #kaburajadulu\n",
      "  shortest_tweet: hasil pengeluaran oregon 12 hari ini senin, 31-3-2025 result : 9805 selamat kepada pemenang #siapwd #cabutuutni #indonesiagelap #tolakruupolri\n",
      "  hashtag_ratio: 0.34655496229225075\n",
      "  emoji_ratio: 0.0\n",
      "  url_ratio: 0.0\n",
      "  mention_ratio: 0.0\n",
      "  lexical_diversity: 1.0\n",
      "  duplication_ratio: 0.0\n",
      "  repeated_char_abuse_count: 0\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "from collections import defaultdict\n",
    "import json\n",
    "buckets = defaultdict(list)\n",
    "\n",
    "with open(INDOBERT_KMEANS_EMBED, \"r\") as file:\n",
    "  documents = json.load(file)\n",
    "\n",
    "for doc in documents:\n",
    "  buckets[doc[\"bucket_label\"]].append(doc)\n",
    "\n",
    "for label, bucket_tweets in buckets.items():\n",
    "    bucket_tweets = [doc[\"content\"] for doc in bucket_tweets]\n",
    "    \n",
    "    hashtag_counts = [count_hashtags(t) for t in bucket_tweets]\n",
    "    tweet_lengths = [len(t) for t in bucket_tweets]\n",
    "    hashtag_ratios = [hashtag_ratio(t) for t in bucket_tweets]\n",
    "    emoji_ratios = [emoji_ratio(t) for t in bucket_tweets]\n",
    "    lexical_divs = [lexical_diversity(t) for t in bucket_tweets]\n",
    "    repeated_abuse_count = sum(1 for t in bucket_tweets if repeated_char_abuse(t))\n",
    "\n",
    "    avg_hashtags = sum(hashtag_counts) / len(bucket_tweets)\n",
    "    avg_length = sum(tweet_lengths) / len(bucket_tweets)\n",
    "    avg_hashtag_ratio = sum(hashtag_ratios) / len(bucket_tweets)\n",
    "    avg_emoji_ratio = sum(emoji_ratios) / len(bucket_tweets)\n",
    "    avg_lexical_div = sum(lexical_divs) / len(bucket_tweets)\n",
    "    url_ratio_val = url_ratio(bucket_tweets)\n",
    "    mention_ratio_val = mention_ratio(bucket_tweets)\n",
    "    dup_ratio = duplicate_ratio(bucket_tweets)\n",
    "\n",
    "    # Heuristic label tagging\n",
    "    label_tags = []\n",
    "    if avg_length > 200:\n",
    "        label_tags.append(\"long tweets\")\n",
    "    if avg_hashtag_ratio > 0.4:\n",
    "        label_tags.append(\"hashtag-heavy\")\n",
    "    if avg_emoji_ratio > 0.2:\n",
    "        label_tags.append(\"emoji spam\")\n",
    "    if url_ratio_val > 0.3:\n",
    "        label_tags.append(\"link drop\")\n",
    "    if mention_ratio_val > 0.3:\n",
    "        label_tags.append(\"mention spam\")\n",
    "    if avg_lexical_div < 0.4:\n",
    "        label_tags.append(\"low diversity (copypasta)\")\n",
    "    if repeated_abuse_count / len(bucket_tweets) > 0.3:\n",
    "        label_tags.append(\"repeated char abuse\")\n",
    "    if dup_ratio > 0.3:\n",
    "        label_tags.append(\"high duplication\")\n",
    "\n",
    "    longest = max(bucket_tweets, key=len)\n",
    "    shortest = min(bucket_tweets, key=len)\n",
    "\n",
    "    results[label] = {\n",
    "        \"label\": \", \".join(label_tags) if label_tags else \"generic\",\n",
    "        \"avg_hashtags\": avg_hashtags,\n",
    "        \"avg_length\": avg_length,\n",
    "        \"longest_tweet\": longest,\n",
    "        \"shortest_tweet\": shortest,\n",
    "        \"hashtag_ratio\": avg_hashtag_ratio,\n",
    "        \"emoji_ratio\": avg_emoji_ratio,\n",
    "        \"url_ratio\": url_ratio_val,\n",
    "        \"mention_ratio\": mention_ratio_val,\n",
    "        \"lexical_diversity\": avg_lexical_div,\n",
    "        \"duplication_ratio\": dup_ratio,\n",
    "        \"repeated_char_abuse_count\": repeated_abuse_count,\n",
    "    }\n",
    "\n",
    "# Output the results\n",
    "for label, metrics in results.items():\n",
    "    print(f\"\\nCluster {label}: {metrics['label']}\")\n",
    "    for k, v in metrics.items():\n",
    "        if k != 'label':\n",
    "            print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e65716c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0 sampled with ratio 0.18079318256309407 and total sample 110\n",
      "Bucket 1 sampled with ratio 0.09183874139626352 and total sample 110\n",
      "Bucket 2 sampled with ratio 0.15319567354965585 and total sample 110\n",
      "Bucket 3 sampled with ratio 0.03716814159292035 and total sample 110\n",
      "Bucket 4 sampled with ratio 0.20603080957063258 and total sample 110\n",
      "Bucket 5 sampled with ratio 0.07046869878728286 and total sample 110\n",
      "Bucket 6 sampled with ratio 0.07079646017699115 and total sample 110\n",
      "Bucket 7 sampled with ratio 0.13313667649950836 and total sample 110\n",
      "Bucket 8 sampled with ratio 0.05473615208128482 and total sample 110\n",
      "Bucket 9 sampled with ratio 0.0018354637823664373 and total sample 110\n",
      "Warning: Bucket '9' has only 28 tweets. Sampling all.\n",
      "Sampled tweets saved to out/labelstudio-training-sampled.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "# Parameters\n",
    "INPUT_FILE = INDOBERT_KMEANS_EMBED\n",
    "OUTPUT_FILE = 'out/labelstudio-training-sampled.json'\n",
    "\n",
    "TOTAL_SAMPLE = 1100  # Change this as needed\n",
    "\n",
    "# Load data\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "bucket_content_count = defaultdict(int)\n",
    "bucket_ratio_count = defaultdict(float)\n",
    "\n",
    "total_tweet = 0\n",
    "for tweet in tweets:\n",
    "  label = tweet[\"bucket_label\"]\n",
    "  bucket_content_count[label] += 1\n",
    "  total_tweet += 1\n",
    "\n",
    "for label, bucket_tweet_count in bucket_content_count.items():\n",
    "  bucket_ratio_count[label] = bucket_tweet_count / total_tweet\n",
    "\n",
    "# Group tweets by bucket\n",
    "buckets = defaultdict(list)\n",
    "for tweet in tweets:\n",
    "  bucket_label = str(tweet[\"bucket_label\"])\n",
    "  buckets[bucket_label].append(tweet)\n",
    "\n",
    "# Sample tweets\n",
    "sampled_tweets = []\n",
    "for (bucket_label, tweets_in_bucket), ratio in zip(buckets.items(), bucket_ratio_count.values()):\n",
    "  # ratiod_total = math.ceil(TOTAL_SAMPLE * ratio)\n",
    "  ratiod_total = int(TOTAL_SAMPLE / bucket_content_count.__len__())\n",
    "  print(f\"Bucket {bucket_label} sampled with ratio {ratio} and total sample {ratiod_total}\")\n",
    "  if len(tweets_in_bucket) < ratiod_total:\n",
    "    print(f\"Warning: Bucket '{bucket_label}' has only {len(tweets_in_bucket)} tweets. Sampling all.\")\n",
    "    sampled = tweets_in_bucket\n",
    "  else:\n",
    "    sampled = random.sample(tweets_in_bucket, ratiod_total)\n",
    "  sampled_tweets.extend(sampled)\n",
    "\n",
    "# Save to output JSON\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "  json.dump(sampled_tweets, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Sampled tweets saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f2fc2",
   "metadata": {},
   "source": [
    "### Check for data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "4a2b90d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No data leakage detected.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_ids_from_json_or_jsonl(file_path, id_key=\"tweet_id\"):\n",
    "    ids = set()\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        if first_char == \"[\":  # JSON array\n",
    "            data = json.load(f)\n",
    "            ids = {entry[id_key] for entry in data if id_key in entry}\n",
    "        else:  # JSONL\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    if id_key in obj:\n",
    "                        ids.add(obj[id_key])\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return ids\n",
    "\n",
    "def check_data_leakage(file1, file2, id_key=\"tweet_id\"):\n",
    "    ids_1 = load_ids_from_json_or_jsonl(file1, id_key)\n",
    "    ids_2 = load_ids_from_json_or_jsonl(file2, id_key)\n",
    "\n",
    "    intersection = ids_1 & ids_2\n",
    "\n",
    "    if intersection:\n",
    "        print(f\"⚠️ Data leakage detected! {len(intersection)} shared {id_key}s.\")\n",
    "    else:\n",
    "        print(\"✅ No data leakage detected.\")\n",
    "\n",
    "    return intersection\n",
    "\n",
    "# Example usage:\n",
    "file_a = \"out/labelstudio-training-sampled.json\"\n",
    "file_b = \"out/golden-standard.json\"\n",
    "leaked_ids = check_data_leakage(file_a, file_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bfc58",
   "metadata": {},
   "source": [
    "### Convert to a Label Studio Processable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4e2cf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_label_studio_format(raw_data):\n",
    "    converted = []\n",
    "    for entry in raw_data:\n",
    "        new_entry = {\n",
    "            \"data\": {\n",
    "                \"text\": entry[\"content\"],\n",
    "                \"bucket_label\" : entry[\"bucket_label\"] if entry.get(\"bucket_label\") is not None else -10\n",
    "            },\n",
    "            \"meta\": {k: v for k, v in entry.items() if k != \"content\" and k != \"bucket_label\"}\n",
    "        }\n",
    "        converted.append(new_entry)\n",
    "    return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "0d4a9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/labelstudio-training-sampled.json\", \"r\") as file:\n",
    "  training_documents = json.load(file)\n",
    "parsed_training_documents = convert_to_label_studio_format(training_documents)\n",
    "with open(\"out/labelstudio/p1/p1_training_prepped.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "  json.dump(parsed_training_documents, file, ensure_ascii=False, indent=2)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
