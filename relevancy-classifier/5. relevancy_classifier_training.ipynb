{
 "cells": [
  {
   "cell_type": "code",
   "id": "73d29f87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:18:36.270350Z",
     "start_time": "2025-11-19T05:18:31.549118Z"
    }
   },
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "ds = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"/data/cache/\")\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]\n",
    "# Split into 80% train, 20% temp (for eval + test)\n",
    "train_ds_split= train_ds.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_ds = train_ds_split[\"train\"]\n",
    "eval_ds = train_ds_split[\"test\"]\n",
    "\n",
    "print(train_ds, eval_ds, test_ds)\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Eval: {len(eval_ds)}, Test: {len(test_ds)}\")\n",
    "\n",
    "class_labels = ClassLabel(names=list(set(train_ds[\"relevant\"])))\n",
    "\n",
    "train_ds = train_ds.cast_column(\"relevant\", class_labels)\n",
    "eval_ds = eval_ds.cast_column(\"relevant\", class_labels)\n",
    "test_ds = test_ds.cast_column(\"relevant\", class_labels)\n",
    "\n",
    "cleaned_train_ds = train_ds.select_columns([\"content\", \"relevant\"])\n",
    "cleaned_test_ds = test_ds.select_columns([\"content\", \"relevant\"])\n",
    "cleaned_eval_ds = eval_ds.select_columns([\"content\", \"relevant\"])\n",
    "cleaned_train_ds = cleaned_train_ds.rename_column(\"relevant\", \"label\")\n",
    "cleaned_eval_ds = cleaned_eval_ds.rename_column(\"relevant\", \"label\")\n",
    "cleaned_test_ds = cleaned_test_ds.rename_column(\"relevant\", \"label\")\n",
    "\n",
    "cleaned_train_ds = cleaned_train_ds.rename_column(\"content\", \"text\")\n",
    "cleaned_eval_ds = cleaned_eval_ds.rename_column(\"content\", \"text\")\n",
    "cleaned_test_ds = cleaned_test_ds.rename_column(\"content\", \"text\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating source_stage_1 split:   0%|          | 0/201583 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "035caabf8f0748759a96b44e52c1572d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating source_stage_2 split:   0%|          | 0/247820 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9453be4ad174bd38beb392256122a20"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating cleaned split:   0%|          | 0/195952 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39aa0518fc7245c8a4245aaf685b28bd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ee311a797e743a3a8e39a2027cd1455"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/19999 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40e50996dcdd4ccc8346821afdb88cae"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
      "    num_rows: 15999\n",
      "}) Dataset({\n",
      "    features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
      "    num_rows: 4000\n",
      "}) Dataset({\n",
      "    features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Train: 15999, Eval: 4000, Test: 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/15999 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae09db4103b84e7b9ba8a6acf0202e2b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2a8341707134f66860b21f24cc7cd6b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a536c576a70e4991abfcb392c118831a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "51b63de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:18:43.433107Z",
     "start_time": "2025-11-19T05:18:43.427790Z"
    }
   },
   "source": [
    "print(cleaned_train_ds, cleaned_eval_ds, cleaned_test_ds)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 15999\n",
      "}) Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 4000\n",
      "}) Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "8721e370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:18:45.974824Z",
     "start_time": "2025-11-19T05:18:45.948400Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Extract all labels\n",
    "labels = cleaned_train_ds[\"label\"]\n",
    "\n",
    "# Count occurrences\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "num_false = label_counts.get(0, 0)  # count of label 0 / False\n",
    "num_true = label_counts.get(1, 0)   # count of label 1 / True\n",
    "\n",
    "print(f\"False: {num_false}, True: {num_true}\")\n",
    "\n",
    "print(label_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: 6384, True: 9615\n",
      "Counter({1: 9615, 0: 6384})\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "8a713479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:19:02.293352Z",
     "start_time": "2025-11-19T05:18:55.968398Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"indolem/indobertweet-base-uncased\",\n",
    "    cache_dir=\"/data/cache/\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"indolem/indobertweet-base-uncased\",\n",
    "    cache_dir=\"/data/cache/\"\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17e8ef8bbc2a47f3931778eb6ffa057c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a08b216a4a23404b9eb772428a7007af"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobertweet-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32c6e28dc11943f7843d797633f756b6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d7dadbf308441d192ad974bf3a1f5f6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9829f036acd545d2b251565ad3eb9123"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65840157d4b0443aa5b6892c5affb724"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "9cec70fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:19:06.881870Z",
     "start_time": "2025-11-19T05:19:06.878260Z"
    }
   },
   "source": [
    "def tokenize(key):\n",
    "    def callback(row):\n",
    "        return tokenizer(\n",
    "            row[key],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "        )\n",
    "    return callback"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "731425d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:19:12.103514Z",
     "start_time": "2025-11-19T05:19:09.220546Z"
    }
   },
   "source": [
    "tokenizer_callback = tokenize(\"text\")\n",
    "encoded_train_ds = cleaned_train_ds.map(tokenizer_callback, batch_size=256, batched=True)\n",
    "encoded_eval_ds = cleaned_eval_ds.map(tokenizer_callback, batch_size=256, batched=True)\n",
    "encoded_test_ds = cleaned_test_ds.map(tokenizer_callback, batch_size=256, batched=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/15999 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f171a26ca1304aab9034d646a56aebba"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51636f317e3e47c882951d16734a253f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb1413811a554ff6a0ec537c7451e346"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "2a6f8b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:19:12.827830Z",
     "start_time": "2025-11-19T05:19:12.824015Z"
    }
   },
   "source": [
    "encoded_train_ds.set_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])\n",
    "encoded_eval_ds.set_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])\n",
    "encoded_test_ds.set_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "001ef791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:19:14.878038Z",
     "start_time": "2025-11-19T05:19:14.870175Z"
    }
   },
   "source": [
    "from transformers import Trainer\n",
    "from torch import nn\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        weights = self.class_weights.to(logits.device)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
    "                        labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "c195eb1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:19:21.054363Z",
     "start_time": "2025-11-19T05:19:20.980346Z"
    }
   },
   "source": [
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(class_names):\n",
    "    num_classes = len(class_names)\n",
    "    def callback(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "        if isinstance(labels, torch.Tensor):\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average=\"macro\", zero_division=0\n",
    "        )\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        p_cls, r_cls, f1_cls, support_cls = precision_recall_fscore_support(\n",
    "            labels, \n",
    "            preds, \n",
    "            average=None,\n",
    "            zero_division=0,\n",
    "            labels=list(range(num_classes))\n",
    "        )\n",
    "        metrics = {\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": macro_f1,\n",
    "            \"macro_precision\": macro_p,\n",
    "            \"macro_recall\": macro_r,\n",
    "        }\n",
    "        for idx, name in enumerate(class_names):\n",
    "            metrics[f\"{name}_precision\"] = p_cls[idx] #type: ignore\n",
    "            metrics[f\"{name}_recall\"]    = r_cls[idx]  #type: ignore\n",
    "            metrics[f\"{name}_f1\"]        = f1_cls[idx]  #type: ignore\n",
    "            metrics[f\"{name}_support\"]   = int(support_cls[idx])  #type: ignore\n",
    "        return metrics\n",
    "    return callback\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\",     # evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",           # save checkpoint at the end of each epoch\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=256,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.05,\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.01,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    test_dataset,\n",
    "    input_class_names,\n",
    "    class_weights\n",
    "):\n",
    "    compute_callback = compute_metrics(input_class_names)\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=compute_callback,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training complete. Evaluating...\")\n",
    "    return trainer.evaluate(eval_dataset = test_dataset), trainer.model"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "8ef7064f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:23:43.679071Z",
     "start_time": "2025-11-19T05:19:24.975470Z"
    }
   },
   "source": [
    "import os\n",
    "num_false = label_counts.get(False, 0)\n",
    "num_true = label_counts.get(True, 0)\n",
    "total = num_false + num_true\n",
    "\n",
    "my_weights = torch.tensor([num_true / total,  # weight for class 0 (False)\n",
    "                           num_false / total], # weight for class 1 (True)\n",
    "                           dtype=torch.float)\n",
    "results, trained_model = train_model(model, encoded_train_ds, encoded_eval_ds, encoded_test_ds, class_labels.names, my_weights)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 04:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>False Precision</th>\n",
       "      <th>False Recall</th>\n",
       "      <th>False F1</th>\n",
       "      <th>False Support</th>\n",
       "      <th>True Precision</th>\n",
       "      <th>True Recall</th>\n",
       "      <th>True F1</th>\n",
       "      <th>True Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.258017</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>0.904438</td>\n",
       "      <td>0.902179</td>\n",
       "      <td>0.907270</td>\n",
       "      <td>0.869332</td>\n",
       "      <td>0.906095</td>\n",
       "      <td>0.887333</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.935026</td>\n",
       "      <td>0.908445</td>\n",
       "      <td>0.921544</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.242157</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>0.907881</td>\n",
       "      <td>0.906160</td>\n",
       "      <td>0.909890</td>\n",
       "      <td>0.878019</td>\n",
       "      <td>0.904229</td>\n",
       "      <td>0.890931</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.934300</td>\n",
       "      <td>0.915552</td>\n",
       "      <td>0.924831</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.239839</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>0.908485</td>\n",
       "      <td>0.906470</td>\n",
       "      <td>0.910920</td>\n",
       "      <td>0.876351</td>\n",
       "      <td>0.907960</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.936590</td>\n",
       "      <td>0.913880</td>\n",
       "      <td>0.925095</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.247108</td>\n",
       "      <td>0.909500</td>\n",
       "      <td>0.905788</td>\n",
       "      <td>0.906213</td>\n",
       "      <td>0.905375</td>\n",
       "      <td>0.889862</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.887087</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.922565</td>\n",
       "      <td>0.926421</td>\n",
       "      <td>0.924489</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.245701</td>\n",
       "      <td>0.908750</td>\n",
       "      <td>0.906436</td>\n",
       "      <td>0.902793</td>\n",
       "      <td>0.913003</td>\n",
       "      <td>0.852524</td>\n",
       "      <td>0.934701</td>\n",
       "      <td>0.891724</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.953062</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.921149</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.253736</td>\n",
       "      <td>0.909500</td>\n",
       "      <td>0.906346</td>\n",
       "      <td>0.904572</td>\n",
       "      <td>0.908432</td>\n",
       "      <td>0.875754</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.889161</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.933390</td>\n",
       "      <td>0.913880</td>\n",
       "      <td>0.923532</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.262532</td>\n",
       "      <td>0.909250</td>\n",
       "      <td>0.906237</td>\n",
       "      <td>0.903993</td>\n",
       "      <td>0.909039</td>\n",
       "      <td>0.871642</td>\n",
       "      <td>0.907960</td>\n",
       "      <td>0.889430</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.936344</td>\n",
       "      <td>0.910117</td>\n",
       "      <td>0.923044</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.268980</td>\n",
       "      <td>0.909250</td>\n",
       "      <td>0.906255</td>\n",
       "      <td>0.903959</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.871199</td>\n",
       "      <td>0.908582</td>\n",
       "      <td>0.889498</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.936720</td>\n",
       "      <td>0.909699</td>\n",
       "      <td>0.923012</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.273791</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>0.904646</td>\n",
       "      <td>0.901841</td>\n",
       "      <td>0.908493</td>\n",
       "      <td>0.864118</td>\n",
       "      <td>0.913557</td>\n",
       "      <td>0.888150</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.939565</td>\n",
       "      <td>0.903428</td>\n",
       "      <td>0.921142</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.276891</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>0.904420</td>\n",
       "      <td>0.902212</td>\n",
       "      <td>0.907168</td>\n",
       "      <td>0.869773</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.887264</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.934652</td>\n",
       "      <td>0.908863</td>\n",
       "      <td>0.921577</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "55ac867b46e7ed27341f1d663ee002bc"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "9a090eab5fa80d4225d0ee624ed2f09e"
     }
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:25:43.980110Z",
     "start_time": "2025-11-19T05:25:43.973203Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "1ac708e9a3d380a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24182546138763428,\n",
       " 'eval_accuracy': 0.9082,\n",
       " 'eval_macro_f1': 0.9037855219555779,\n",
       " 'eval_macro_precision': 0.9010565164892763,\n",
       " 'eval_macro_recall': 0.9070816456317661,\n",
       " 'eval_False_precision': 0.8649052841475573,\n",
       " 'eval_False_recall': 0.9022360894435777,\n",
       " 'eval_False_f1': 0.8831763807584627,\n",
       " 'eval_False_support': 1923,\n",
       " 'eval_True_precision': 0.9372077488309953,\n",
       " 'eval_True_recall': 0.9119272018199545,\n",
       " 'eval_True_f1': 0.9243946631526931,\n",
       " 'eval_True_support': 3077,\n",
       " 'eval_runtime': 2.0899,\n",
       " 'eval_samples_per_second': 2392.416,\n",
       " 'eval_steps_per_second': 9.57,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "c6ad8aae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:26:34.924930Z",
     "start_time": "2025-11-19T05:26:32.591196Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# We don't need a full training setup, just a place to output predictions.\n",
    "# This creates a dummy TrainingArguments object.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data\",\n",
    "    per_device_eval_batch_size=64, # Use a large batch size for fast eval\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "\n",
    "    model=trained_model,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"Running predictions on test_ds...\")\n",
    "\n",
    "# --- 2. Run Predictions ---\n",
    "# The .predict() method runs inference on test_ds and returns a PredictionOutput object\n",
    "prediction_output = trainer.predict(encoded_test_ds)\n",
    "\n",
    "print(\"Predictions complete.\")\n",
    "\n",
    "# The predictions are logits (raw model scores). We need the class index (0 or 1).\n",
    "predicted_labels = np.argmax(prediction_output.predictions, axis=-1)\n",
    "\n",
    "# The true labels are also in the output, just to be safe\n",
    "true_labels = prediction_output.label_ids\n",
    "\n",
    "# --- 3. Create DataFrame for Review ---\n",
    "\n",
    "# Get the original text from the dataset\n",
    "# This is why your test_ds *must* have the 'text' column\n",
    "try:\n",
    "    original_texts = encoded_test_ds['text']\n",
    "except KeyError:\n",
    "    print(\"=\"*50)\n",
    "    print(\"ERROR: Your 'test_ds' does not have a 'text' column.\")\n",
    "    print(\"Please reload your dataset without removing the 'text' column.\")\n",
    "    print(\"=\"*50)\n",
    "    # Stop execution if the text column is missing\n",
    "    raise\n",
    "\n",
    "# Create the main DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': original_texts,\n",
    "    'true_label': true_labels,\n",
    "    'predicted_label': predicted_labels\n",
    "})\n",
    "\n",
    "# --- 4. Isolate and Categorize Errors ---\n",
    "\n",
    "# Filter to get only the rows where the model was wrong\n",
    "errors_df = df[df['true_label'] != df['predicted_label']].copy()\n",
    "\n",
    "# Add a new column to categorize the error type\n",
    "def get_error_type(row):\n",
    "    if row['true_label'] == 0 and row['predicted_label'] == 1:\n",
    "        # Model predicted 1 (Relevant), but it was 0 (Not Relevant)\n",
    "        return 'False Positive (FP)'\n",
    "    elif row['true_label'] == 1 and row['predicted_label'] == 0:\n",
    "        # Model predicted 0 (Not Relevant), but it was 1 (Relevant)\n",
    "        return 'False Negative (FN)'\n",
    "\n",
    "errors_df['error_type'] = errors_df.apply(get_error_type, axis=1)\n",
    "\n",
    "print(f\"\\nFound {len(errors_df)} misclassified samples out of {len(df)} total.\")\n",
    "\n",
    "# --- 5. Display the Wrong Answers for Review ---\n",
    "\n",
    "# Set pandas display options for better text viewing in the notebook\n",
    "pd.set_option('display.max_colwidth', 300) # Show more text\n",
    "pd.set_option('display.max_rows', 100)     # Show more rows\n",
    "\n",
    "print(\"\\n--- ðŸ”´ FALSE POSITIVES (Model said 'Relevant', but was 'Not Relevant') ---\")\n",
    "display(errors_df[errors_df['error_type'] == 'False Positive (FP)'])\n",
    "\n",
    "print(\"\\n--- ðŸ”µ FALSE NEGATIVES (Model said 'Not Relevant', but was 'Relevant') ---\")\n",
    "display(errors_df[errors_df['error_type'] == 'False Negative (FN)'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions on test_ds...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "c730124352fcbe183274ed8cbce1e0fb"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions complete.\n",
      "\n",
      "Found 459 misclassified samples out of 5000 total.\n",
      "\n",
      "--- ðŸ”´ FALSE POSITIVES (Model said 'Relevant', but was 'Not Relevant') ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                                                                                                                                             text  \\\n",
       "21                                        Btw di kantor gue ada anak magang dari kampus pemerintahan basis tni. Tapi hari ini mereka izin masuk 1/2 hari karna mau ada acara bukber sm dekan nya Mau ngerayain abis disahin ruu tni y? abis ini makin bisa banyak rangkap jabatan   \n",
       "22                                                                                                                                                                                                                                                  udh bukan dwifungsi lg lu pek   \n",
       "27                                                                                                                                                                                                                                      Jadi? RUU TNI yg disahkan tuh yg mana ya?   \n",
       "99                            Institusi ini gak punya integritas, tapi malah mau ditambahin kewenangan oleh DPR. Bikin heboh media dengan beberapa kasus besar seperti kasus Pertamina, tapi cuma nangkep bawahan2nya aja dan kasus nya pun gak jelas gimana...#TolakRUUKejaksaan   \n",
       "161                                                                                                                                                                 Entar univ univ lain juga diginiin...gabakal ada gerakan demo mahasiswa, udah dipotong sebelum keluar kampus.   \n",
       "...                                                                                                                                                                                                                                                                           ...   \n",
       "4873                                                                                                                                                                                                                                                             Dwifungsi Damkar   \n",
       "4898                                                                                                                                                                                                    Framing supaya demo mahasiswa dinilai ditunggangi dan sebagainya. <url> .   \n",
       "4899  Apa sih yang mngkawatirkan dengan demo buruh.buruh mnntut gaji maksimal seperti buruh di negara2 lain di luar negri,blm tnt juga di kabulin DPR,hingga aparat berbuat melebihi batas kek gini.?... Knp GK di temuin aja pimpinan organisasinya di ajak dialog & di arahkan.   \n",
       "4961                                                                  Keluarga bokap gue hampir semua tni. Dari alm. kakek, om, tante gue hampir semua-muanya tentara. Semalem liat postingan sepupu dia bilang si om jg menolak ruu tni. Alhamdulillah keluarga gue masih waras.   \n",
       "4974                                                                                                                                                        Presiden Prabowo buktiin komitmen ke TNI lewat revisi UU, posisi makin aman dan penuh profesionalisme tanpa dwifungsi   \n",
       "\n",
       "      true_label  predicted_label           error_type  \n",
       "21             0                1  False Positive (FP)  \n",
       "22             0                1  False Positive (FP)  \n",
       "27             0                1  False Positive (FP)  \n",
       "99             0                1  False Positive (FP)  \n",
       "161            0                1  False Positive (FP)  \n",
       "...          ...              ...                  ...  \n",
       "4873           0                1  False Positive (FP)  \n",
       "4898           0                1  False Positive (FP)  \n",
       "4899           0                1  False Positive (FP)  \n",
       "4961           0                1  False Positive (FP)  \n",
       "4974           0                1  False Positive (FP)  \n",
       "\n",
       "[188 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Btw di kantor gue ada anak magang dari kampus pemerintahan basis tni. Tapi hari ini mereka izin masuk 1/2 hari karna mau ada acara bukber sm dekan nya Mau ngerayain abis disahin ruu tni y? abis ini makin bisa banyak rangkap jabatan</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>udh bukan dwifungsi lg lu pek</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Jadi? RUU TNI yg disahkan tuh yg mana ya?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Institusi ini gak punya integritas, tapi malah mau ditambahin kewenangan oleh DPR. Bikin heboh media dengan beberapa kasus besar seperti kasus Pertamina, tapi cuma nangkep bawahan2nya aja dan kasus nya pun gak jelas gimana...#TolakRUUKejaksaan</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Entar univ univ lain juga diginiin...gabakal ada gerakan demo mahasiswa, udah dipotong sebelum keluar kampus.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>Dwifungsi Damkar</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4898</th>\n",
       "      <td>Framing supaya demo mahasiswa dinilai ditunggangi dan sebagainya. &lt;url&gt; .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4899</th>\n",
       "      <td>Apa sih yang mngkawatirkan dengan demo buruh.buruh mnntut gaji maksimal seperti buruh di negara2 lain di luar negri,blm tnt juga di kabulin DPR,hingga aparat berbuat melebihi batas kek gini.?... Knp GK di temuin aja pimpinan organisasinya di ajak dialog &amp; di arahkan.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4961</th>\n",
       "      <td>Keluarga bokap gue hampir semua tni. Dari alm. kakek, om, tante gue hampir semua-muanya tentara. Semalem liat postingan sepupu dia bilang si om jg menolak ruu tni. Alhamdulillah keluarga gue masih waras.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>Presiden Prabowo buktiin komitmen ke TNI lewat revisi UU, posisi makin aman dan penuh profesionalisme tanpa dwifungsi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸ”µ FALSE NEGATIVES (Model said 'Not Relevant', but was 'Relevant') ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                                                                                                                                                           text  \\\n",
       "10                                                                                                                                                                                        Masa depan tak pasti. TNI harus pasti kuat. UU TNI adalah bekal menuju ke sana. #TNIKuatIndonesiaAman   \n",
       "44                                                                                                                                                                                                                                         awas bangkitnya #DwiFungsiABRI bangkit kembali <url>   \n",
       "74                                                                                                                                                                                                           Mahasiswa Demo di Balai Kota Solo, Balas Aksi Unjuk Rasa yang Ditemui Gibran <url>   \n",
       "76    Demonstrasi mcmni penting sebenarnya untuk ajar ke budak-budak dan orang dewasa. Sebab kalau setakat terang secara teori tapi tak tunjuk situasi sebenar org mudah lupa apa yang sepatutnya buat tambah-tambah bila dah kalut dan panik. Cumanya kenapa tak padam-padam abang bomba ui...   \n",
       "81                   Teror Terhadap Jurnalis, Teror Terhadap Papua Koalisi Advokasi Keadilan dan Keselamatan Jurnalis di Tanah Papua yang terdiri dari para jurnalis dan pembela HAM menggelar demonstrasi damai di depan Mapolda Papua, di Kota Jayapura, Papua pada Selasa, 17 Desember 2024.   \n",
       "...                                                                                                                                                                                                                                                                                         ...   \n",
       "4949          In syaa Allah, butuh perubahan yang mendasar mulai dari pemimpinnya... Tidak dalam waktu dekat, kita berharap pencalonan Anies Baswedan menjadi pintu masuk perubahan yang mendasar ini. Namun upaya penjegalan akan terus diusahakan oleh mereka. Jangan biarkan #IndonesiaGelap   \n",
       "4951                                       Aksi masyarakat Kota Sorong menyerukan dukungan terhadap Undang-Undang (UU) TNI melalui aksi sosial pembagian takjil yang digelar pada. Minggu (23/3/2025). <url> #TNIADbekerjadenganhati #TNIADDihatiRakyat #TNIADPrima #TNIADBerjuangBersamaRakyat   \n",
       "4963                                                                                                                                                                Mana mungkin dipublikasi dengan menunjukan ijazahnya,dan mana mungkin kepolisian melawan junjungannya,salam #indonesiagelap   \n",
       "4968                                                                                                                                                                                                                                                        Ini unjuk rasa masalah apa lagi ya?   \n",
       "4986                                                                                                                 Segerakan revisi UU Kejagung #timah #babel #kejagungarogan #kejagungoverbody #savebabel #timnasday #300t #ekonomibabel #tni #BUMN #Baktiuntuknegeri #kejagungri #jampidsus   \n",
       "\n",
       "      true_label  predicted_label           error_type  \n",
       "10             1                0  False Negative (FN)  \n",
       "44             1                0  False Negative (FN)  \n",
       "74             1                0  False Negative (FN)  \n",
       "76             1                0  False Negative (FN)  \n",
       "81             1                0  False Negative (FN)  \n",
       "...          ...              ...                  ...  \n",
       "4949           1                0  False Negative (FN)  \n",
       "4951           1                0  False Negative (FN)  \n",
       "4963           1                0  False Negative (FN)  \n",
       "4968           1                0  False Negative (FN)  \n",
       "4986           1                0  False Negative (FN)  \n",
       "\n",
       "[271 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Masa depan tak pasti. TNI harus pasti kuat. UU TNI adalah bekal menuju ke sana. #TNIKuatIndonesiaAman</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>awas bangkitnya #DwiFungsiABRI bangkit kembali &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Mahasiswa Demo di Balai Kota Solo, Balas Aksi Unjuk Rasa yang Ditemui Gibran &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Demonstrasi mcmni penting sebenarnya untuk ajar ke budak-budak dan orang dewasa. Sebab kalau setakat terang secara teori tapi tak tunjuk situasi sebenar org mudah lupa apa yang sepatutnya buat tambah-tambah bila dah kalut dan panik. Cumanya kenapa tak padam-padam abang bomba ui...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Teror Terhadap Jurnalis, Teror Terhadap Papua Koalisi Advokasi Keadilan dan Keselamatan Jurnalis di Tanah Papua yang terdiri dari para jurnalis dan pembela HAM menggelar demonstrasi damai di depan Mapolda Papua, di Kota Jayapura, Papua pada Selasa, 17 Desember 2024.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>In syaa Allah, butuh perubahan yang mendasar mulai dari pemimpinnya... Tidak dalam waktu dekat, kita berharap pencalonan Anies Baswedan menjadi pintu masuk perubahan yang mendasar ini. Namun upaya penjegalan akan terus diusahakan oleh mereka. Jangan biarkan #IndonesiaGelap</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>Aksi masyarakat Kota Sorong menyerukan dukungan terhadap Undang-Undang (UU) TNI melalui aksi sosial pembagian takjil yang digelar pada. Minggu (23/3/2025). &lt;url&gt; #TNIADbekerjadenganhati #TNIADDihatiRakyat #TNIADPrima #TNIADBerjuangBersamaRakyat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>Mana mungkin dipublikasi dengan menunjukan ijazahnya,dan mana mungkin kepolisian melawan junjungannya,salam #indonesiagelap</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>Ini unjuk rasa masalah apa lagi ya?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>Segerakan revisi UU Kejagung #timah #babel #kejagungarogan #kejagungoverbody #savebabel #timnasday #300t #ekonomibabel #tni #BUMN #Baktiuntuknegeri #kejagungri #jampidsus</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "de17ff5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:27:26.620108Z",
     "start_time": "2025-11-19T05:27:26.611954Z"
    }
   },
   "source": [
    "def calculate_weighted_metrics(eval_metrics):\n",
    "    \"\"\"\n",
    "    Calculates the weighted-average F1, precision, and recall\n",
    "    from a Hugging Face eval metrics dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Get F1 and Support ---\n",
    "    f1_false = eval_metrics.get('eval_False_f1')\n",
    "    support_false = eval_metrics.get('eval_False_support')\n",
    "    f1_true = eval_metrics.get('eval_True_f1')\n",
    "    support_true = eval_metrics.get('eval_True_support')\n",
    "    \n",
    "    # --- Get Precision ---\n",
    "    precision_false = eval_metrics.get('eval_False_precision')\n",
    "    precision_true = eval_metrics.get('eval_True_precision')\n",
    "    \n",
    "    # --- Get Recall ---\n",
    "    recall_false = eval_metrics.get('eval_False_recall')\n",
    "    recall_true = eval_metrics.get('eval_True_recall')\n",
    "\n",
    "    # Check that we have the minimum required keys\n",
    "    if None in [f1_false, support_false, f1_true, support_true]:\n",
    "        print(\"Error: Missing required keys for F1/support.\")\n",
    "        return {}\n",
    "\n",
    "    # --- Calculate Total Support ---\n",
    "    total_support = support_false + support_true\n",
    "    if total_support == 0:\n",
    "        print(\"Error: Total support is zero.\")\n",
    "        return {}\n",
    "\n",
    "    # --- Calculate Weighted Averages ---\n",
    "    weighted_f1 = ( (f1_false * support_false) + (f1_true * support_true) ) / total_support\n",
    "    \n",
    "    weighted_precision = ( (precision_false * support_false) + (precision_true * support_true) ) / total_support\n",
    "    \n",
    "    weighted_recall = ( (recall_false * support_false) + (recall_true * support_true) ) / total_support\n",
    "\n",
    "    return {\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "        \"weighted_precision\": weighted_precision,\n",
    "        \"weighted_recall\": weighted_recall,\n",
    "        \"total_support\": total_support\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "ef6b1f02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:27:28.358546Z",
     "start_time": "2025-11-19T05:27:28.353765Z"
    }
   },
   "source": [
    "print(calculate_weighted_metrics(results))\n",
    "print(results)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weighted_f1': 0.9085421117438722, 'weighted_precision': 0.9094002209137451, 'weighted_recall': 0.9082, 'total_support': 5000}\n",
      "{'eval_loss': 0.24182546138763428, 'eval_accuracy': 0.9082, 'eval_macro_f1': 0.9037855219555779, 'eval_macro_precision': 0.9010565164892763, 'eval_macro_recall': 0.9070816456317661, 'eval_False_precision': 0.8649052841475573, 'eval_False_recall': 0.9022360894435777, 'eval_False_f1': 0.8831763807584627, 'eval_False_support': 1923, 'eval_True_precision': 0.9372077488309953, 'eval_True_recall': 0.9119272018199545, 'eval_True_f1': 0.9243946631526931, 'eval_True_support': 3077, 'eval_runtime': 2.0899, 'eval_samples_per_second': 2392.416, 'eval_steps_per_second': 9.57, 'epoch': 10.0}\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:33:40.835665Z",
     "start_time": "2025-11-19T05:33:40.780592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_WS       = re.compile(r'\\s+')\n",
    "_KUTI_CUT = re.compile(r'(?i)kutipan.*$', re.DOTALL)\n",
    "\n",
    "# --- (MODIFIED) ---\n",
    "# Catches \"word\" + \"dari\" + \"domain.com\" -> replaces with \"word\"\n",
    "# Changed \\w+ to \\S+ to include punctuation like '!'\n",
    "_DARI_URL_ATTACHED = re.compile(r'(\\S+)dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# Catches \" dari \" + \"domain.com\" -> replaces with empty string\n",
    "_DARI_URL_SPACED = re.compile(r'\\s+dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# --- (NEW) ---\n",
    "# Catches any word ending in \"dari\" (e.g., \"anarko!dari\", \"negaradari\")\n",
    "_DARI_STUCK = re.compile(r'(\\S+)dari\\b', re.I)\n",
    "\n",
    "def cleantext(row: str):\n",
    "    text = row[\"content\"] #type: ignore\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "    text = text.replace(\"tanya grok\", \" \")\n",
    "    text = text.replace(\"grokproductivitypasang\", \" \")\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "\n",
    "    # Handle standard URLs first\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = text.replace('ini tidak tersedia', ' ')\n",
    "\n",
    "    text = _MENTION.sub('@USER', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "    text = _KUTI_CUT.sub('', text)\n",
    "\n",
    "    # text = _DARI_URL_ATTACHED.sub(r'\\1', text)\n",
    "    # text = _DARI_URL_SPACED.sub('', text)\n",
    "    # text = _DARI_STUCK.sub(r'\\1', text)\n",
    "\n",
    "    text = _WS.sub(' ', text).strip()\n",
    "    row[\"content\"] = text #type: ignore\n",
    "    return row"
   ],
   "id": "a73adf3412aca395",
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "ff9b5df7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:49:55.576731Z",
     "start_time": "2025-11-19T05:47:36.748218Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from scipy.special import softmax # Or use torch.nn.functional.softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "trained_model.eval()     # Put model in evaluation mode (turns off dropout)\n",
    "\n",
    "# 3. Prepare your new data\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"/data/cache\")\n",
    "raw_ds = dataset[\"source_stage_2\"]\n",
    "raw_ds = raw_ds.map(cleantext, num_proc=30)\n",
    "\n",
    "new_texts = raw_ds[\"content\"]\n",
    "\n",
    "batch_size = 256  # <-- Adjust this based on your GPU memory. Try 16, 32, or 64.\n",
    "\n",
    "# --- 5. NEW: Batch Prediction Loop ---\n",
    "all_probabilities = []\n",
    "all_predicted_indices = []\n",
    "\n",
    "print(f\"Starting prediction on {len(new_texts)} texts in batches of {batch_size}...\")\n",
    "\n",
    "# Use torch.no_grad() for the whole loop\n",
    "with torch.no_grad():\n",
    "    # Loop over the new_texts in chunks\n",
    "    for i in tqdm(range(0, len(new_texts), batch_size)):\n",
    "        \n",
    "        # Get the current batch of texts\n",
    "        batch_texts = new_texts[i : i + batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True,       # Pad to the longest text *in this batch*\n",
    "            truncation=True,    # Truncate to model's max length\n",
    "            max_length=512,     # Explicitly set max length\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move batch to GPU\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = trained_model(**inputs)\n",
    "        \n",
    "        # Get logits and convert to probabilities\n",
    "        logits = outputs.logits\n",
    "        probabilities = softmax(logits.cpu().numpy(), axis=1)\n",
    "        \n",
    "        # Get predicted indices\n",
    "        predicted_indices = np.argmax(probabilities, axis=1)\n",
    "        \n",
    "        # Store results\n",
    "        all_probabilities.append(probabilities)\n",
    "        all_predicted_indices.append(predicted_indices)\n",
    "\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- 6. Consolidate Results ---\n",
    "# Combine the results from all batches\n",
    "all_probabilities = np.concatenate(all_probabilities, axis=0)\n",
    "all_predicted_indices = np.concatenate(all_predicted_indices, axis=0)\n",
    "\n",
    "# --- 7. Package into DataFrame ---\n",
    "results_data = []\n",
    "for i in range(len(new_texts)):\n",
    "    results_data.append({\n",
    "        'text': new_texts[i],\n",
    "        'predicted_label': class_labels.names[all_predicted_indices[i]],\n",
    "        'confidence': all_probabilities[i].max(),\n",
    "        'prob_False': all_probabilities[i][0],\n",
    "        'prob_True': all_probabilities[i][1]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results_data)\n",
    "print(df)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2fd503d5f8946aeb1e70f56193bdbf0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "data/source_labeled-00000-of-00001.parqu(â€¦):   0%|          | 0.00/32.3M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f84470168214f80be1730fef820176c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating source_stage_1 split:   0%|          | 0/201583 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5b9e0ee50c14a6191a78a833aa6ec48"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating source_stage_2 split:   0%|          | 0/247820 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "105e972e9e8242d59447d1fb0050d52f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating cleaned split:   0%|          | 0/195952 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b79faf6d8064ea48af9f233548e4781"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80b75ac0b61d48daa1bdc07b5e08999d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/19999 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afd9f25f2bbc4b758ab67f58b898d442"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating source_labeled split:   0%|          | 0/247820 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea75c0f90674420bb846c3dce3976892"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=30):   0%|          | 0/247820 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0acb39eb801d4fe5a3969a9d08d42ee5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction on 247820 texts in batches of 256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 969/969 [01:56<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction complete.\n",
      "                                                                                                                                                                                                                                                                                     text  \\\n",
      "0                                                                                                                                                                                                                                                Kembalikan Tentara ke barak #TolakRUUTNI   \n",
      "1                                                                                                                                                                                                                                           Masih ga nyangka suamiku tentara #tolakruutni   \n",
      "2                                                                                                                                                                                                         Beberapa zine yg akan saya bawa saat melapak nanti #TolakRUUTNI #SupremasiSipil   \n",
      "3                     Sebagai mahasiswa hukum, kami memiliki tanggung jawab akademik untuk mengawal proses pembentukan undang-undang agar memiliki proses partisipasi yg bermakna! berjuang turun ke jalan, bahkan berjuang di ruang sidang, semua sah dilakukan. yg penting #tolakruutni   \n",
      "4                                                                                                                                                                                                                          sekali-kali ga nyebelin ye kite #FallingInLoveEra #TolakRUUTNI   \n",
      "...                                                                                                                                                                                                                                                                                   ...   \n",
      "247815                                                                              Muncul isu Jaksa Agung di copot ? Isu pencopotan ini muncul beriringan dengan perintah pengamanan TNI untuk Kejaksaan. ? Terlalu banyak isu untuk mengaburkan kasus kasus besar?? TOLAK RUU KEJAKSAAN   \n",
      "247816  Setelah ada perintah pengamanan TNI untuk Kejaksaan, tiba tiba beredar isu soal Pencopotan Jaksa Agung ST Burhanuddin, Ada apakah..?? Besok Senin, kita tggu aja klanjutan beritanya Genkz. Persebaya Projo Jambu MasyaAllah PPATK Amanda Dewtee minggu malam Tolak RUU kejaksaan   \n",
      "247817             Bahkan Ketua DPR RI Puan Maharani pun meminta agar TNI memberikan penjelasan resmi terkait pengamanan di kejati dan kejari. Karena penjelasan yg transparan sgt penting agar tidak menimbulkan dugaan atau prasangka negatif di tengah masyarakat. Tolak RUU Kejaksaan   \n",
      "247818                                Daku kurang setuju jika prajurit TNI ditugaskan Ngawal Pejabat Kejaksaan, kesannya prajurit TNI sejak 1945 - 2025 ngganggur gitu loh, ga ada kerjaan? Mbok ya daripada prajurit mending pendekar PSHT saja pak JAGUNG ? GLODOK! TOLAK RUU KEJAKSAAN   \n",
      "247819                                                   Belum reda pertanyaan Publik Soal Alasan Mengapa Prajurit TNI Menjaga Kantor2 Kejaksaan , Kini justru berhembus berita kalau Jaksa Agung ST Burhanudin akan di \"Geser\" dari Jabatannya ? wow..ada apa ini..? Tolak RUU Kejaksaan   \n",
      "\n",
      "        predicted_label  confidence  prob_False  prob_True  \n",
      "0                  True    0.994413    0.005587   0.994413  \n",
      "1                  True    0.990188    0.009812   0.990188  \n",
      "2                  True    0.992837    0.007163   0.992837  \n",
      "3                  True    0.978631    0.021369   0.978631  \n",
      "4                  True    0.982490    0.017510   0.982490  \n",
      "...                 ...         ...         ...        ...  \n",
      "247815             True    0.818374    0.181626   0.818374  \n",
      "247816            False    0.773131    0.773131   0.226869  \n",
      "247817             True    0.647854    0.352146   0.647854  \n",
      "247818             True    0.881053    0.118947   0.881053  \n",
      "247819             True    0.674479    0.325521   0.674479  \n",
      "\n",
      "[247820 rows x 5 columns]\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "6830114f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:50:19.737929Z",
     "start_time": "2025-11-19T05:50:19.437989Z"
    }
   },
   "source": [
    "source_ds = ds[\"source_stage_2\"]\n",
    "predictions_list = df[\"predicted_label\"].tolist()\n",
    "# 3. Define the function that .map() will run on each row\n",
    "def update_row(example, idx):\n",
    "    \"\"\"\n",
    "    Takes a single row (example) and its index (idx).\n",
    "    It replaces the value in 'relevant' with the new prediction.\n",
    "    \"\"\"\n",
    "    example['relevant'] = predictions_list[idx]\n",
    "    return example\n",
    "\n",
    "# 4. Apply the function to the entire dataset\n",
    "# .map() returns a new dataset, so we must re-assign it\n",
    "print(\"Starting to update 'relevant' column...\")\n",
    "source_ds = source_ds.map(update_row, with_indices=True)\n",
    "\n",
    "print(\"Column updated successfully!\")\n",
    "print(source_ds)\n",
    "\n",
    "# Check the first row to see the change\n",
    "# You should now see your string label (e.g., \"True\" or \"False\")\n",
    "# instead of the old boolean (e.g., True or False)\n",
    "print(source_ds[5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to update 'relevant' column...\n",
      "Column updated successfully!\n",
      "Dataset({\n",
      "    features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
      "    num_rows: 247820\n",
      "})\n",
      "{'tweet_id': '1909950840775020808', 'time': '2025-04-09T12:45:40.000', 'author': '@lightfuryy_', 'content': 'Presiden kita ini udah KOSONG dari pas zaman debat pilpres. Dan skrg negara kita di pimpin oleh 2 manusia KOSONG bin DONGO \\n\\n#TolakRUUTNI #SupremasiSipil', 'comment_count': 0, 'repost_count': 2, 'like_count': 1, 'view_count': 463, 'relevant': True, 'sentiment': 0}\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "2b694358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:50:24.372948Z",
     "start_time": "2025-11-19T05:50:23.870819Z"
    }
   },
   "source": [
    "view_ds = source_ds.select_columns([\"content\", \"relevant\"])\n",
    "for row in view_ds.to_list()[:10]:\n",
    "    print(row)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Kembalikan Tentara ke barak\\n#TolakRUUTNI', 'relevant': True}\n",
      "{'content': 'Masih ga nyangka suamiku tentara #tolakruutni', 'relevant': True}\n",
      "{'content': 'Beberapa zine yg akan saya bawa saat melapak nanti\\n\\n#TolakRUUTNI \\n#SupremasiSipil', 'relevant': True}\n",
      "{'content': 'Sebagai mahasiswa hukum, kami memiliki tanggung jawab akademik untuk mengawal proses pembentukan undang-undang agar memiliki proses partisipasi yg bermakna!\\n\\nberjuang turun ke jalan, bahkan berjuang di ruang sidang, semua sah dilakukan. yg penting #tolakruutni', 'relevant': True}\n",
      "{'content': 'sekali-kali ga nyebelin ye kite #FallingInLoveEra #TolakRUUTNI', 'relevant': True}\n",
      "{'content': 'Presiden kita ini udah KOSONG dari pas zaman debat pilpres. Dan skrg negara kita di pimpin oleh 2 manusia KOSONG bin DONGO \\n\\n#TolakRUUTNI #SupremasiSipil', 'relevant': True}\n",
      "{'content': 'Kalo bisa menteri dan pejabat negara lainnya ikut disandera juga.. wkwkwkkwk\\n\\n#TolakRUUTNI QRIS', 'relevant': True}\n",
      "{'content': 'Great \\nECCO CHI Ses\\nBingx3\\n#shailenzo #zelena #yulioli #grandefratello #zeudiners #ENHYPEN #ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆ #Trump #TolakRUUTNI', 'relevant': False}\n",
      "{'content': 'Kalau mau bertani jangan jadi tentara.\\n#TolakRUUTNI', 'relevant': True}\n",
      "{'content': '\"Whenever I lay down to close my eyesâ€¦ thatâ€™s when the nightmares begin. And lately, theyâ€™ve been getting worse.\"\\n-Dave Torres\\n\\n#TolakUUTNI #TolakRUUTNI #TolakRevisiUUTNI #PeringatanDarurat #IndonesiaGelap #TolakRUUPolri #TolakRKUHAP', 'relevant': True}\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "id": "b5834fec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:50:26.524604Z",
     "start_time": "2025-11-19T05:50:26.518212Z"
    }
   },
   "source": "ds[\"source_labeled\"] = source_ds",
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "id": "9f5fe511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:50:37.899947Z",
     "start_time": "2025-11-19T05:50:29.381976Z"
    }
   },
   "source": "ds.push_to_hub(\"tianharjuno/twitter-parse\", commit_message=\"Labeled the source ds\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e59e9b84b72f462f97180916f3169224"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/202 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7108d98908214216b5fad80b6e825f3c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9af4fd8d53c742d1954c0b33b7abf821"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d51e620951dd4ea1812e853eba7540e9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8db5c40457fb4eeeb1274b86a3abd2f2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/196 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fabf3ef4278494fb98be226657fc95e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d3c5011ef07470fbf6302905210873d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb1396b2c09d468490655e4bcc1e8c43"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd996e601b9f4fce92c49daeeca45537"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1771fa0097b5434e92abdba73fe478ff"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a99fa332f754837bcbff341993f34b4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "406a636c9bd447c8bbcfd766329cb77e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tianharjuno/twitter-parse/commit/72c6aa86902d8e64ae2f44c3192c13817609d206', commit_message='Labeled the source ds', commit_description='', oid='72c6aa86902d8e64ae2f44c3192c13817609d206', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tianharjuno/twitter-parse', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tianharjuno/twitter-parse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "id": "a3069c68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T05:50:52.391933Z",
     "start_time": "2025-11-19T05:50:43.252405Z"
    }
   },
   "source": [
    "trained_model.push_to_hub(\"tianharjuno/ruu-tni-relevancy-classification-p1\", commit_description=\"Changed training pipeline\")\n",
    "tokenizer.push_to_hub(\"tianharjuno/ruu-tni-relevancy-classification-p1\", commit_description=\"Changed training pipeline\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89576a38ce6945cd8349057f49dd0ef1"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56c77a28fb424a6c9347ac846ad4eddf"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  /tmp/tmp9urvown6/model.safetensors    :   8%|7         | 33.5MB /  442MB            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34791d8693dc455e93b08b39a875c139"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tianharjuno/ruu-tni-relevancy-classification-p1/commit/577a485da72ce07eded842332f974dd60fb7b73a', commit_message='Upload tokenizer', commit_description='Changed training pipeline', oid='577a485da72ce07eded842332f974dd60fb7b73a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tianharjuno/ruu-tni-relevancy-classification-p1', endpoint='https://huggingface.co', repo_type='model', repo_id='tianharjuno/ruu-tni-relevancy-classification-p1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
