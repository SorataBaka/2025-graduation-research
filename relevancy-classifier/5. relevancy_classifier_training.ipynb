{
 "cells": [
  {
   "cell_type": "code",
   "id": "73d29f87",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-25T08:00:43.917417Z"
    }
   },
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "ds = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"/data/cache/\")\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]\n",
    "# Split into 80% train, 20% temp (for eval + test)\n",
    "train_ds_split= train_ds.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_ds = train_ds_split[\"train\"]\n",
    "eval_ds = train_ds_split[\"test\"]\n",
    "\n",
    "print(train_ds, eval_ds, test_ds)\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Eval: {len(eval_ds)}, Test: {len(test_ds)}\")\n",
    "\n",
    "class_labels = ClassLabel(names=list(set(train_ds[\"relevant\"])))\n",
    "\n",
    "train_ds = train_ds.cast_column(\"relevant\", class_labels)\n",
    "eval_ds = eval_ds.cast_column(\"relevant\", class_labels)\n",
    "test_ds = test_ds.cast_column(\"relevant\", class_labels)\n",
    "\n",
    "cleaned_train_ds = train_ds.select_columns([\"content\", \"relevant\"])\n",
    "cleaned_test_ds = test_ds.select_columns([\"content\", \"relevant\"])\n",
    "cleaned_eval_ds = eval_ds.select_columns([\"content\", \"relevant\"])\n",
    "cleaned_train_ds = cleaned_train_ds.rename_column(\"relevant\", \"label\")\n",
    "cleaned_eval_ds = cleaned_eval_ds.rename_column(\"relevant\", \"label\")\n",
    "cleaned_test_ds = cleaned_test_ds.rename_column(\"relevant\", \"label\")\n",
    "\n",
    "cleaned_train_ds = cleaned_train_ds.rename_column(\"content\", \"text\")\n",
    "cleaned_eval_ds = cleaned_eval_ds.rename_column(\"content\", \"text\")\n",
    "cleaned_test_ds = cleaned_test_ds.rename_column(\"content\", \"text\")"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/data'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_dataset, ClassLabel\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m ds = \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtianharjuno/twitter-parse\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/data/cache/\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m train_ds = ds[\u001B[33m\"\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m      4\u001B[39m test_ds = ds[\u001B[33m\"\u001B[39m\u001B[33mtest\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/sentiment/lib/python3.11/site-packages/datasets/load.py:1397\u001B[39m, in \u001B[36mload_dataset\u001B[39m\u001B[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001B[39m\n\u001B[32m   1392\u001B[39m verification_mode = VerificationMode(\n\u001B[32m   1393\u001B[39m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode.BASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode.ALL_CHECKS\n\u001B[32m   1394\u001B[39m )\n\u001B[32m   1396\u001B[39m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1397\u001B[39m builder_instance = \u001B[43mload_dataset_builder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1398\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1399\u001B[39m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1400\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1401\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1402\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1403\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1404\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1405\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1406\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1407\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1408\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1409\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1410\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1412\u001B[39m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[32m   1413\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/sentiment/lib/python3.11/site-packages/datasets/load.py:1171\u001B[39m, in \u001B[36mload_dataset_builder\u001B[39m\u001B[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001B[39m\n\u001B[32m   1169\u001B[39m builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\n\u001B[32m   1170\u001B[39m \u001B[38;5;66;03m# Instantiate the dataset builder\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1171\u001B[39m builder_instance: DatasetBuilder = \u001B[43mbuilder_cls\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1172\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1173\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1174\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconfig_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1175\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1176\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1177\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mhash\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mdataset_module\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1178\u001B[39m \u001B[43m    \u001B[49m\u001B[43minfo\u001B[49m\u001B[43m=\u001B[49m\u001B[43minfo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1179\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1180\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1181\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mbuilder_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1184\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1185\u001B[39m builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n\u001B[32m   1187\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/sentiment/lib/python3.11/site-packages/datasets/builder.py:386\u001B[39m, in \u001B[36mDatasetBuilder.__init__\u001B[39m\u001B[34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, config_id, **config_kwargs)\u001B[39m\n\u001B[32m    384\u001B[39m \u001B[38;5;28mself\u001B[39m._cache_dir = \u001B[38;5;28mself\u001B[39m._build_cache_dir()\n\u001B[32m    385\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_remote_url(\u001B[38;5;28mself\u001B[39m._cache_dir_root):\n\u001B[32m--> \u001B[39m\u001B[32m386\u001B[39m     \u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_cache_dir_root\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    387\u001B[39m     lock_path = os.path.join(\n\u001B[32m    388\u001B[39m         \u001B[38;5;28mself\u001B[39m._cache_dir_root, Path(\u001B[38;5;28mself\u001B[39m._cache_dir).as_posix().replace(\u001B[33m\"\u001B[39m\u001B[33m/\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m_\u001B[39m\u001B[33m\"\u001B[39m) + \u001B[33m\"\u001B[39m\u001B[33m.lock\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    389\u001B[39m     )\n\u001B[32m    390\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m FileLock(lock_path):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen os>:215\u001B[39m, in \u001B[36mmakedirs\u001B[39m\u001B[34m(name, mode, exist_ok)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen os>:225\u001B[39m, in \u001B[36mmakedirs\u001B[39m\u001B[34m(name, mode, exist_ok)\u001B[39m\n",
      "\u001B[31mOSError\u001B[39m: [Errno 30] Read-only file system: '/data'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "51b63de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:03:27.689287Z",
     "start_time": "2025-11-19T15:03:27.685690Z"
    }
   },
   "source": [
    "print(cleaned_train_ds, cleaned_eval_ds, cleaned_test_ds)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 15999\n",
      "}) Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 4000\n",
      "}) Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "8721e370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:03:32.456717Z",
     "start_time": "2025-11-19T15:03:32.429853Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Extract all labels\n",
    "labels = cleaned_train_ds[\"label\"]\n",
    "\n",
    "# Count occurrences\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "num_false = label_counts.get(0, 0)  # count of label 0 / False\n",
    "num_true = label_counts.get(1, 0)   # count of label 1 / True\n",
    "\n",
    "print(f\"False: {num_false}, True: {num_true}\")\n",
    "\n",
    "print(label_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: 6384, True: 9615\n",
      "Counter({1: 9615, 0: 6384})\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8a713479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:03:55.571696Z",
     "start_time": "2025-11-19T15:03:48.157438Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"indolem/indobertweet-base-uncased\",\n",
    "    cache_dir=\"cache/\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"indolem/indobertweet-base-uncased\",\n",
    "    cache_dir=\"cache/\"\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88af05856e3542e5878cc883f90cdb1b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a1334be8c934bb7a5d50d1948eeb76c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobertweet-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08f35e343d25411b938204d4a78300e2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ee0d7b48d074f6e9fd0eddd7004dd37"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0637966113c84511b1ba25793f5311da"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b1deabf46b9449d930deba1fc3779b0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9cec70fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:03:57.834746Z",
     "start_time": "2025-11-19T15:03:57.829252Z"
    }
   },
   "source": [
    "def tokenize(key):\n",
    "    def callback(row):\n",
    "        return tokenizer(\n",
    "            row[key],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "        )\n",
    "    return callback"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "731425d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:04:01.471552Z",
     "start_time": "2025-11-19T15:03:59.108267Z"
    }
   },
   "source": [
    "tokenizer_callback = tokenize(\"text\")\n",
    "encoded_train_ds = cleaned_train_ds.map(tokenizer_callback, batch_size=256, batched=True)\n",
    "encoded_eval_ds = cleaned_eval_ds.map(tokenizer_callback, batch_size=256, batched=True)\n",
    "encoded_test_ds = cleaned_test_ds.map(tokenizer_callback, batch_size=256, batched=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/15999 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4623fc2b0819441584a4be5d356aac85"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60342eb8eff44c9c875d20a21ec77c73"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a58c6ffb4564ce99d140eb6cdeaba3b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "2a6f8b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:04:02.795632Z",
     "start_time": "2025-11-19T15:04:02.788760Z"
    }
   },
   "source": [
    "encoded_train_ds.set_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])\n",
    "encoded_eval_ds.set_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])\n",
    "encoded_test_ds.set_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "001ef791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:04:04.640569Z",
     "start_time": "2025-11-19T15:04:04.557502Z"
    }
   },
   "source": [
    "from transformers import Trainer\n",
    "from torch import nn\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        weights = self.class_weights.to(logits.device)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
    "                        labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "c195eb1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:04:06.550177Z",
     "start_time": "2025-11-19T15:04:06.483403Z"
    }
   },
   "source": [
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(class_names):\n",
    "    num_classes = len(class_names)\n",
    "    def callback(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "        if isinstance(labels, torch.Tensor):\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average=\"macro\", zero_division=0\n",
    "        )\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        p_cls, r_cls, f1_cls, support_cls = precision_recall_fscore_support(\n",
    "            labels, \n",
    "            preds, \n",
    "            average=None,\n",
    "            zero_division=0,\n",
    "            labels=list(range(num_classes))\n",
    "        )\n",
    "        metrics = {\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": macro_f1,\n",
    "            \"macro_precision\": macro_p,\n",
    "            \"macro_recall\": macro_r,\n",
    "        }\n",
    "        for idx, name in enumerate(class_names):\n",
    "            metrics[f\"{name}_precision\"] = p_cls[idx] #type: ignore\n",
    "            metrics[f\"{name}_recall\"]    = r_cls[idx]  #type: ignore\n",
    "            metrics[f\"{name}_f1\"]        = f1_cls[idx]  #type: ignore\n",
    "            metrics[f\"{name}_support\"]   = int(support_cls[idx])  #type: ignore\n",
    "        return metrics\n",
    "    return callback\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\",     # evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",           # save checkpoint at the end of each epoch\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=256,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.05,\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.01,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    test_dataset,\n",
    "    input_class_names,\n",
    "    class_weights\n",
    "):\n",
    "    compute_callback = compute_metrics(input_class_names)\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=compute_callback,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training complete. Evaluating...\")\n",
    "    return trainer.evaluate(eval_dataset = test_dataset), trainer.model"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "8ef7064f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:08:35.502397Z",
     "start_time": "2025-11-19T15:04:15.589347Z"
    }
   },
   "source": [
    "import os\n",
    "num_false = label_counts.get(False, 0)\n",
    "num_true = label_counts.get(True, 0)\n",
    "total = num_false + num_true\n",
    "\n",
    "my_weights = torch.tensor([num_true / total,  # weight for class 0 (False)\n",
    "                           num_false / total], # weight for class 1 (True)\n",
    "                           dtype=torch.float)\n",
    "results, trained_model = train_model(model, encoded_train_ds, encoded_eval_ds, encoded_test_ds, class_labels.names, my_weights)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 04:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>False Precision</th>\n",
       "      <th>False Recall</th>\n",
       "      <th>False F1</th>\n",
       "      <th>False Support</th>\n",
       "      <th>True Precision</th>\n",
       "      <th>True Recall</th>\n",
       "      <th>True F1</th>\n",
       "      <th>True Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.258527</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.900655</td>\n",
       "      <td>0.898906</td>\n",
       "      <td>0.902713</td>\n",
       "      <td>0.869119</td>\n",
       "      <td>0.896144</td>\n",
       "      <td>0.882425</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.928693</td>\n",
       "      <td>0.909281</td>\n",
       "      <td>0.918885</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.241200</td>\n",
       "      <td>0.242097</td>\n",
       "      <td>0.907250</td>\n",
       "      <td>0.904027</td>\n",
       "      <td>0.902234</td>\n",
       "      <td>0.906143</td>\n",
       "      <td>0.872815</td>\n",
       "      <td>0.900498</td>\n",
       "      <td>0.886440</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.931653</td>\n",
       "      <td>0.911789</td>\n",
       "      <td>0.921614</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.213700</td>\n",
       "      <td>0.237483</td>\n",
       "      <td>0.909750</td>\n",
       "      <td>0.906788</td>\n",
       "      <td>0.904439</td>\n",
       "      <td>0.909762</td>\n",
       "      <td>0.871352</td>\n",
       "      <td>0.909826</td>\n",
       "      <td>0.890173</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.937527</td>\n",
       "      <td>0.909699</td>\n",
       "      <td>0.923403</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.248366</td>\n",
       "      <td>0.908750</td>\n",
       "      <td>0.905075</td>\n",
       "      <td>0.905200</td>\n",
       "      <td>0.904952</td>\n",
       "      <td>0.887227</td>\n",
       "      <td>0.885572</td>\n",
       "      <td>0.886399</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.923173</td>\n",
       "      <td>0.924331</td>\n",
       "      <td>0.923752</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>0.248582</td>\n",
       "      <td>0.903750</td>\n",
       "      <td>0.901435</td>\n",
       "      <td>0.897778</td>\n",
       "      <td>0.908618</td>\n",
       "      <td>0.843732</td>\n",
       "      <td>0.933458</td>\n",
       "      <td>0.886330</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.951824</td>\n",
       "      <td>0.883779</td>\n",
       "      <td>0.916540</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.250295</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.901945</td>\n",
       "      <td>0.899460</td>\n",
       "      <td>0.905179</td>\n",
       "      <td>0.864176</td>\n",
       "      <td>0.906095</td>\n",
       "      <td>0.884639</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.934745</td>\n",
       "      <td>0.904264</td>\n",
       "      <td>0.919252</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.258336</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>0.905935</td>\n",
       "      <td>0.903824</td>\n",
       "      <td>0.908524</td>\n",
       "      <td>0.872455</td>\n",
       "      <td>0.906095</td>\n",
       "      <td>0.888957</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.935193</td>\n",
       "      <td>0.910953</td>\n",
       "      <td>0.922914</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.265349</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.903174</td>\n",
       "      <td>0.900847</td>\n",
       "      <td>0.906123</td>\n",
       "      <td>0.867183</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.885914</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.934511</td>\n",
       "      <td>0.906773</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.269125</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.901003</td>\n",
       "      <td>0.898306</td>\n",
       "      <td>0.904649</td>\n",
       "      <td>0.860849</td>\n",
       "      <td>0.907960</td>\n",
       "      <td>0.883777</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.935764</td>\n",
       "      <td>0.901338</td>\n",
       "      <td>0.918228</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>0.272528</td>\n",
       "      <td>0.906750</td>\n",
       "      <td>0.903546</td>\n",
       "      <td>0.901640</td>\n",
       "      <td>0.905827</td>\n",
       "      <td>0.871317</td>\n",
       "      <td>0.901119</td>\n",
       "      <td>0.885968</td>\n",
       "      <td>1608</td>\n",
       "      <td>0.931964</td>\n",
       "      <td>0.910535</td>\n",
       "      <td>0.921125</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "ac2a647861188fd710252f16597716b0"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "f195ff1ae58dbb423a85f4278600e4c3"
     }
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:09:36.506049Z",
     "start_time": "2025-11-19T15:09:36.499335Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "1ac708e9a3d380a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.18780359625816345,\n",
       " 'eval_accuracy': 0.9408,\n",
       " 'eval_macro_f1': 0.9390974171379209,\n",
       " 'eval_macro_precision': 0.9417771093712233,\n",
       " 'eval_macro_recall': 0.9369391378843959,\n",
       " 'eval_False_precision': 0.9471106758080313,\n",
       " 'eval_False_recall': 0.9114043355325165,\n",
       " 'eval_False_f1': 0.9289145052833814,\n",
       " 'eval_False_support': 2122,\n",
       " 'eval_True_precision': 0.9364435429344151,\n",
       " 'eval_True_recall': 0.9624739402362752,\n",
       " 'eval_True_f1': 0.9492803289924606,\n",
       " 'eval_True_support': 2878,\n",
       " 'eval_runtime': 2.0912,\n",
       " 'eval_samples_per_second': 2390.922,\n",
       " 'eval_steps_per_second': 9.564,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "c6ad8aae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:09:42.992286Z",
     "start_time": "2025-11-19T15:09:40.739766Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# We don't need a full training setup, just a place to output predictions.\n",
    "# This creates a dummy TrainingArguments object.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data\",\n",
    "    per_device_eval_batch_size=64, # Use a large batch size for fast eval\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "\n",
    "    model=trained_model,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"Running predictions on test_ds...\")\n",
    "\n",
    "# --- 2. Run Predictions ---\n",
    "# The .predict() method runs inference on test_ds and returns a PredictionOutput object\n",
    "prediction_output = trainer.predict(encoded_test_ds)\n",
    "\n",
    "print(\"Predictions complete.\")\n",
    "\n",
    "# The predictions are logits (raw model scores). We need the class index (0 or 1).\n",
    "predicted_labels = np.argmax(prediction_output.predictions, axis=-1)\n",
    "\n",
    "# The true labels are also in the output, just to be safe\n",
    "true_labels = prediction_output.label_ids\n",
    "\n",
    "# --- 3. Create DataFrame for Review ---\n",
    "\n",
    "# Get the original text from the dataset\n",
    "# This is why your test_ds *must* have the 'text' column\n",
    "try:\n",
    "    original_texts = encoded_test_ds['text']\n",
    "except KeyError:\n",
    "    print(\"=\"*50)\n",
    "    print(\"ERROR: Your 'test_ds' does not have a 'text' column.\")\n",
    "    print(\"Please reload your dataset without removing the 'text' column.\")\n",
    "    print(\"=\"*50)\n",
    "    # Stop execution if the text column is missing\n",
    "    raise\n",
    "\n",
    "# Create the main DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': original_texts,\n",
    "    'true_label': true_labels,\n",
    "    'predicted_label': predicted_labels\n",
    "})\n",
    "\n",
    "# --- 4. Isolate and Categorize Errors ---\n",
    "\n",
    "# Filter to get only the rows where the model was wrong\n",
    "errors_df = df[df['true_label'] != df['predicted_label']].copy()\n",
    "\n",
    "# Add a new column to categorize the error type\n",
    "def get_error_type(row):\n",
    "    if row['true_label'] == 0 and row['predicted_label'] == 1:\n",
    "        # Model predicted 1 (Relevant), but it was 0 (Not Relevant)\n",
    "        return 'False Positive (FP)'\n",
    "    elif row['true_label'] == 1 and row['predicted_label'] == 0:\n",
    "        # Model predicted 0 (Not Relevant), but it was 1 (Relevant)\n",
    "        return 'False Negative (FN)'\n",
    "\n",
    "errors_df['error_type'] = errors_df.apply(get_error_type, axis=1)\n",
    "\n",
    "print(f\"\\nFound {len(errors_df)} misclassified samples out of {len(df)} total.\")\n",
    "\n",
    "# --- 5. Display the Wrong Answers for Review ---\n",
    "\n",
    "# Set pandas display options for better text viewing in the notebook\n",
    "pd.set_option('display.max_colwidth', 300) # Show more text\n",
    "pd.set_option('display.max_rows', 100)     # Show more rows\n",
    "\n",
    "print(\"\\n--- ðŸ”´ FALSE POSITIVES (Model said 'Relevant', but was 'Not Relevant') ---\")\n",
    "display(errors_df[errors_df['error_type'] == 'False Positive (FP)'])\n",
    "\n",
    "print(\"\\n--- ðŸ”µ FALSE NEGATIVES (Model said 'Not Relevant', but was 'Relevant') ---\")\n",
    "display(errors_df[errors_df['error_type'] == 'False Negative (FN)'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions on test_ds...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "b7b13359328786b6405dc872eccc8480"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions complete.\n",
      "\n",
      "Found 296 misclassified samples out of 5000 total.\n",
      "\n",
      "--- ðŸ”´ FALSE POSITIVES (Model said 'Relevant', but was 'Not Relevant') ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                                                                                                                                                          text  \\\n",
       "0                                                                                                                                                                                                                                           Thanks for the raffle! #CabutRUUTNI #TolakRUUPolri   \n",
       "22                                                                                                                                                                                                                                                               udh bukan dwifungsi lg lu pek   \n",
       "27                                                                                                                                                                                                                                                   Jadi? RUU TNI yg disahkan tuh yg mana ya?   \n",
       "53                                                                                                                                                                                                                                     Ini ga ada yg mau gitu unjuk rasa buat reformasi polri?   \n",
       "78                                                       dari kemaren gatekeep aksi terhadap suatu perwakilan (yg even doing harmless and nothing wrong) terus skrg ada yg kontra dgn opininya langsung ngataiin lonta lonte wow such a coli ego moment doang aksi #IndonesiaGelap ini buat lo   \n",
       "...                                                                                                                                                                                                                                                                                        ...   \n",
       "4811                                                                                                                                   Demonstrasi rakyat menjadi manifestasi ketidaksetujuan terhadap Gemoysian yang terlibat dalam kecurangan pilpres, menggugah ingatan akan tahun 98 <url>   \n",
       "4868  Unjuk rasa itu hrsnya dijawab dg instrospeksi dan koreksi diri. Bukan dijawab dg Power apalagi ditakutkan, dihindari, dijauhi. Mrk itu manusia yg ingin dihargai, didengar, dihadapi, diajak diskusi. Jgn dianggap musuh ngr, mrk pemilik ngr yg sesungguhnya, pemilik hak n kedaulatan.   \n",
       "4873                                                                                                                                                                                                                                                                          Dwifungsi Damkar   \n",
       "4899               Apa sih yang mngkawatirkan dengan demo buruh.buruh mnntut gaji maksimal seperti buruh di negara2 lain di luar negri,blm tnt juga di kabulin DPR,hingga aparat berbuat melebihi batas kek gini.?... Knp GK di temuin aja pimpinan organisasinya di ajak dialog & di arahkan.   \n",
       "4919          Kalau dilihat demonstrasi (Aksi turun ke jalan) ini agak aneh. Demo dilakukan utk menyuarakan pendapat saat saluran bicara kpd penguasa/pihak2 \"dominan\" sdh tertutup. Lha ini? apa gerakan mhs gen z & era \"hilirnya digital\" apakah spt ini? #ngawurnya jawa tengah dirty vote   \n",
       "\n",
       "      true_label  predicted_label           error_type  \n",
       "0              0                1  False Positive (FP)  \n",
       "22             0                1  False Positive (FP)  \n",
       "27             0                1  False Positive (FP)  \n",
       "53             0                1  False Positive (FP)  \n",
       "78             0                1  False Positive (FP)  \n",
       "...          ...              ...                  ...  \n",
       "4811           0                1  False Positive (FP)  \n",
       "4868           0                1  False Positive (FP)  \n",
       "4873           0                1  False Positive (FP)  \n",
       "4899           0                1  False Positive (FP)  \n",
       "4919           0                1  False Positive (FP)  \n",
       "\n",
       "[188 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thanks for the raffle! #CabutRUUTNI #TolakRUUPolri</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>udh bukan dwifungsi lg lu pek</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Jadi? RUU TNI yg disahkan tuh yg mana ya?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Ini ga ada yg mau gitu unjuk rasa buat reformasi polri?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>dari kemaren gatekeep aksi terhadap suatu perwakilan (yg even doing harmless and nothing wrong) terus skrg ada yg kontra dgn opininya langsung ngataiin lonta lonte wow such a coli ego moment doang aksi #IndonesiaGelap ini buat lo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4811</th>\n",
       "      <td>Demonstrasi rakyat menjadi manifestasi ketidaksetujuan terhadap Gemoysian yang terlibat dalam kecurangan pilpres, menggugah ingatan akan tahun 98 &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>Unjuk rasa itu hrsnya dijawab dg instrospeksi dan koreksi diri. Bukan dijawab dg Power apalagi ditakutkan, dihindari, dijauhi. Mrk itu manusia yg ingin dihargai, didengar, dihadapi, diajak diskusi. Jgn dianggap musuh ngr, mrk pemilik ngr yg sesungguhnya, pemilik hak n kedaulatan.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>Dwifungsi Damkar</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4899</th>\n",
       "      <td>Apa sih yang mngkawatirkan dengan demo buruh.buruh mnntut gaji maksimal seperti buruh di negara2 lain di luar negri,blm tnt juga di kabulin DPR,hingga aparat berbuat melebihi batas kek gini.?... Knp GK di temuin aja pimpinan organisasinya di ajak dialog &amp; di arahkan.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>Kalau dilihat demonstrasi (Aksi turun ke jalan) ini agak aneh. Demo dilakukan utk menyuarakan pendapat saat saluran bicara kpd penguasa/pihak2 \"dominan\" sdh tertutup. Lha ini? apa gerakan mhs gen z &amp; era \"hilirnya digital\" apakah spt ini? #ngawurnya jawa tengah dirty vote</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False Positive (FP)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸ”µ FALSE NEGATIVES (Model said 'Not Relevant', but was 'Relevant') ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                                                                                                                                                           text  \\\n",
       "10                                                                                                                                                                                        Masa depan tak pasti. TNI harus pasti kuat. UU TNI adalah bekal menuju ke sana. #TNIKuatIndonesiaAman   \n",
       "29              Bahkan Demonstrasi ini saya tidak percaya, mungkin saja ini gerakan pesanan dari penguasa agar terlihat demokratis dari kecacatan proses demokrasi yang berlangsung... Disclaimer!!, Ini hanya dugaan, syukur jika ini benar2 gerakan murni, dan mudah2an ini menjadi massive..   \n",
       "44                                                                                                                                                                                                                                         awas bangkitnya #DwiFungsiABRI bangkit kembali <url>   \n",
       "72                                                                                                                                                            guys aku berjuang sendiri guys wkwkwk adekku pendukung berat prabowo, pendukung ruu tni guys, aku gabisa berbicara banyak d rumah   \n",
       "76    Demonstrasi mcmni penting sebenarnya untuk ajar ke budak-budak dan orang dewasa. Sebab kalau setakat terang secara teori tapi tak tunjuk situasi sebenar org mudah lupa apa yang sepatutnya buat tambah-tambah bila dah kalut dan panik. Cumanya kenapa tak padam-padam abang bomba ui...   \n",
       "...                                                                                                                                                                                                                                                                                         ...   \n",
       "4681                                                                                                                                                                                           Buat apa ada MK jika keputusannya dianulir? #PeringatanDarurat #kawalkeputusanMK #kawaldemokrasi   \n",
       "4781                                                                                                                                                                           Pantas saja lowongan penyuluh pertanian beberapa tahun belakangan sering kosong, udah diisi dari militer rupanya   \n",
       "4848                                                                                                                                                                                                               Salken Mas ee , emg rejeki kadang tidak tepat sasaran mas, sama spt UU TNI ~   \n",
       "4951                                       Aksi masyarakat Kota Sorong menyerukan dukungan terhadap Undang-Undang (UU) TNI melalui aksi sosial pembagian takjil yang digelar pada. Minggu (23/3/2025). <url> #TNIADbekerjadenganhati #TNIADDihatiRakyat #TNIADPrima #TNIADBerjuangBersamaRakyat   \n",
       "4986                                                                                                                 Segerakan revisi UU Kejagung #timah #babel #kejagungarogan #kejagungoverbody #savebabel #timnasday #300t #ekonomibabel #tni #BUMN #Baktiuntuknegeri #kejagungri #jampidsus   \n",
       "\n",
       "      true_label  predicted_label           error_type  \n",
       "10             1                0  False Negative (FN)  \n",
       "29             1                0  False Negative (FN)  \n",
       "44             1                0  False Negative (FN)  \n",
       "72             1                0  False Negative (FN)  \n",
       "76             1                0  False Negative (FN)  \n",
       "...          ...              ...                  ...  \n",
       "4681           1                0  False Negative (FN)  \n",
       "4781           1                0  False Negative (FN)  \n",
       "4848           1                0  False Negative (FN)  \n",
       "4951           1                0  False Negative (FN)  \n",
       "4986           1                0  False Negative (FN)  \n",
       "\n",
       "[108 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Masa depan tak pasti. TNI harus pasti kuat. UU TNI adalah bekal menuju ke sana. #TNIKuatIndonesiaAman</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Bahkan Demonstrasi ini saya tidak percaya, mungkin saja ini gerakan pesanan dari penguasa agar terlihat demokratis dari kecacatan proses demokrasi yang berlangsung... Disclaimer!!, Ini hanya dugaan, syukur jika ini benar2 gerakan murni, dan mudah2an ini menjadi massive..</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>awas bangkitnya #DwiFungsiABRI bangkit kembali &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>guys aku berjuang sendiri guys wkwkwk adekku pendukung berat prabowo, pendukung ruu tni guys, aku gabisa berbicara banyak d rumah</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Demonstrasi mcmni penting sebenarnya untuk ajar ke budak-budak dan orang dewasa. Sebab kalau setakat terang secara teori tapi tak tunjuk situasi sebenar org mudah lupa apa yang sepatutnya buat tambah-tambah bila dah kalut dan panik. Cumanya kenapa tak padam-padam abang bomba ui...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>Buat apa ada MK jika keputusannya dianulir? #PeringatanDarurat #kawalkeputusanMK #kawaldemokrasi</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4781</th>\n",
       "      <td>Pantas saja lowongan penyuluh pertanian beberapa tahun belakangan sering kosong, udah diisi dari militer rupanya</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4848</th>\n",
       "      <td>Salken Mas ee , emg rejeki kadang tidak tepat sasaran mas, sama spt UU TNI ~</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>Aksi masyarakat Kota Sorong menyerukan dukungan terhadap Undang-Undang (UU) TNI melalui aksi sosial pembagian takjil yang digelar pada. Minggu (23/3/2025). &lt;url&gt; #TNIADbekerjadenganhati #TNIADDihatiRakyat #TNIADPrima #TNIADBerjuangBersamaRakyat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>Segerakan revisi UU Kejagung #timah #babel #kejagungarogan #kejagungoverbody #savebabel #timnasday #300t #ekonomibabel #tni #BUMN #Baktiuntuknegeri #kejagungri #jampidsus</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False Negative (FN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "de17ff5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:09:56.008576Z",
     "start_time": "2025-11-19T15:09:55.999037Z"
    }
   },
   "source": [
    "def calculate_weighted_metrics(eval_metrics):\n",
    "    \"\"\"\n",
    "    Calculates the weighted-average F1, precision, and recall\n",
    "    from a Hugging Face eval metrics dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Get F1 and Support ---\n",
    "    f1_false = eval_metrics.get('eval_False_f1')\n",
    "    support_false = eval_metrics.get('eval_False_support')\n",
    "    f1_true = eval_metrics.get('eval_True_f1')\n",
    "    support_true = eval_metrics.get('eval_True_support')\n",
    "    \n",
    "    # --- Get Precision ---\n",
    "    precision_false = eval_metrics.get('eval_False_precision')\n",
    "    precision_true = eval_metrics.get('eval_True_precision')\n",
    "    \n",
    "    # --- Get Recall ---\n",
    "    recall_false = eval_metrics.get('eval_False_recall')\n",
    "    recall_true = eval_metrics.get('eval_True_recall')\n",
    "\n",
    "    # Check that we have the minimum required keys\n",
    "    if None in [f1_false, support_false, f1_true, support_true]:\n",
    "        print(\"Error: Missing required keys for F1/support.\")\n",
    "        return {}\n",
    "\n",
    "    # --- Calculate Total Support ---\n",
    "    total_support = support_false + support_true\n",
    "    if total_support == 0:\n",
    "        print(\"Error: Total support is zero.\")\n",
    "        return {}\n",
    "\n",
    "    # --- Calculate Weighted Averages ---\n",
    "    weighted_f1 = ( (f1_false * support_false) + (f1_true * support_true) ) / total_support\n",
    "    \n",
    "    weighted_precision = ( (precision_false * support_false) + (precision_true * support_true) ) / total_support\n",
    "    \n",
    "    weighted_recall = ( (recall_false * support_false) + (recall_true * support_true) ) / total_support\n",
    "\n",
    "    return {\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "        \"weighted_precision\": weighted_precision,\n",
    "        \"weighted_recall\": weighted_recall,\n",
    "        \"total_support\": total_support\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "ef6b1f02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:10:00.445317Z",
     "start_time": "2025-11-19T15:10:00.439413Z"
    }
   },
   "source": [
    "print(calculate_weighted_metrics(results))\n",
    "print(results)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weighted_f1': 0.9406370734103273, 'weighted_precision': 0.9409706741259779, 'weighted_recall': 0.9408, 'total_support': 5000}\n",
      "{'eval_loss': 0.18780359625816345, 'eval_accuracy': 0.9408, 'eval_macro_f1': 0.9390974171379209, 'eval_macro_precision': 0.9417771093712233, 'eval_macro_recall': 0.9369391378843959, 'eval_False_precision': 0.9471106758080313, 'eval_False_recall': 0.9114043355325165, 'eval_False_f1': 0.9289145052833814, 'eval_False_support': 2122, 'eval_True_precision': 0.9364435429344151, 'eval_True_recall': 0.9624739402362752, 'eval_True_f1': 0.9492803289924606, 'eval_True_support': 2878, 'eval_runtime': 2.0912, 'eval_samples_per_second': 2390.922, 'eval_steps_per_second': 9.564, 'epoch': 10.0}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:12:14.221813Z",
     "start_time": "2025-11-19T15:12:14.157677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_WS       = re.compile(r'\\s+')\n",
    "_KUTI_CUT = re.compile(r'(?i)kutipan.*$', re.DOTALL)\n",
    "\n",
    "# --- (MODIFIED) ---\n",
    "# Catches \"word\" + \"dari\" + \"domain.com\" -> replaces with \"word\"\n",
    "# Changed \\w+ to \\S+ to include punctuation like '!'\n",
    "_DARI_URL_ATTACHED = re.compile(r'(\\S+)dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# Catches \" dari \" + \"domain.com\" -> replaces with empty string\n",
    "_DARI_URL_SPACED = re.compile(r'\\s+dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# --- (NEW) ---\n",
    "# Catches any word ending in \"dari\" (e.g., \"anarko!dari\", \"negaradari\")\n",
    "_DARI_STUCK = re.compile(r'(\\S+)dari\\b', re.I)\n",
    "\n",
    "def cleantext(row: str):\n",
    "    text = row[\"content\"] #type: ignore\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "    text = text.replace(\"tanya grok\", \" \")\n",
    "    text = text.replace(\"grokproductivitypasang\", \" \")\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "\n",
    "    # Handle standard URLs first\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = text.replace('ini tidak tersedia', ' ')\n",
    "\n",
    "    text = _MENTION.sub('@USER', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "    text = _KUTI_CUT.sub('', text)\n",
    "\n",
    "    # text = _DARI_URL_ATTACHED.sub(r'\\1', text)\n",
    "    # text = _DARI_URL_SPACED.sub('', text)\n",
    "    # text = _DARI_STUCK.sub(r'\\1', text)\n",
    "\n",
    "    text = _WS.sub(' ', text).strip()\n",
    "    row[\"content\"] = text #type: ignore\n",
    "    return row"
   ],
   "id": "a73adf3412aca395",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "ff9b5df7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:14:20.505452Z",
     "start_time": "2025-11-19T15:12:16.120023Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from scipy.special import softmax # Or use torch.nn.functional.softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "trained_model.eval()     # Put model in evaluation mode (turns off dropout)\n",
    "\n",
    "# 3. Prepare your new data\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"/data/cache\")\n",
    "raw_ds = dataset[\"source_stage_2\"]\n",
    "raw_ds = raw_ds.map(cleantext, num_proc=30)\n",
    "\n",
    "new_texts = raw_ds[\"content\"]\n",
    "\n",
    "batch_size = 256  # <-- Adjust this based on your GPU memory. Try 16, 32, or 64.\n",
    "\n",
    "# --- 5. NEW: Batch Prediction Loop ---\n",
    "all_probabilities = []\n",
    "all_predicted_indices = []\n",
    "\n",
    "print(f\"Starting prediction on {len(new_texts)} texts in batches of {batch_size}...\")\n",
    "\n",
    "# Use torch.no_grad() for the whole loop\n",
    "with torch.no_grad():\n",
    "    # Loop over the new_texts in chunks\n",
    "    for i in tqdm(range(0, len(new_texts), batch_size)):\n",
    "        \n",
    "        # Get the current batch of texts\n",
    "        batch_texts = new_texts[i : i + batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True,       # Pad to the longest text *in this batch*\n",
    "            truncation=True,    # Truncate to model's max length\n",
    "            max_length=512,     # Explicitly set max length\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move batch to GPU\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = trained_model(**inputs)\n",
    "        \n",
    "        # Get logits and convert to probabilities\n",
    "        logits = outputs.logits\n",
    "        probabilities = softmax(logits.cpu().numpy(), axis=1)\n",
    "        \n",
    "        # Get predicted indices\n",
    "        predicted_indices = np.argmax(probabilities, axis=1)\n",
    "        \n",
    "        # Store results\n",
    "        all_probabilities.append(probabilities)\n",
    "        all_predicted_indices.append(predicted_indices)\n",
    "\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- 6. Consolidate Results ---\n",
    "# Combine the results from all batches\n",
    "all_probabilities = np.concatenate(all_probabilities, axis=0)\n",
    "all_predicted_indices = np.concatenate(all_predicted_indices, axis=0)\n",
    "\n",
    "# --- 7. Package into DataFrame ---\n",
    "results_data = []\n",
    "for i in range(len(new_texts)):\n",
    "    results_data.append({\n",
    "        'text': new_texts[i],\n",
    "        'predicted_label': class_labels.names[all_predicted_indices[i]],\n",
    "        'confidence': all_probabilities[i].max(),\n",
    "        'prob_False': all_probabilities[i][0],\n",
    "        'prob_True': all_probabilities[i][1]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results_data)\n",
    "print(df)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=30):   0%|          | 0/247820 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55e15173dea94c4bb98d82fa2b219904"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction on 247820 texts in batches of 256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 969/969 [01:54<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction complete.\n",
      "                                                                                                                                                                                                                                                                                     text  \\\n",
      "0                                                                                                                                                                                                                                                Kembalikan Tentara ke barak #TolakRUUTNI   \n",
      "1                                                                                                                                                                                                                                           Masih ga nyangka suamiku tentara #tolakruutni   \n",
      "2                                                                                                                                                                                                         Beberapa zine yg akan saya bawa saat melapak nanti #TolakRUUTNI #SupremasiSipil   \n",
      "3                     Sebagai mahasiswa hukum, kami memiliki tanggung jawab akademik untuk mengawal proses pembentukan undang-undang agar memiliki proses partisipasi yg bermakna! berjuang turun ke jalan, bahkan berjuang di ruang sidang, semua sah dilakukan. yg penting #tolakruutni   \n",
      "4                                                                                                                                                                                                                          sekali-kali ga nyebelin ye kite #FallingInLoveEra #TolakRUUTNI   \n",
      "...                                                                                                                                                                                                                                                                                   ...   \n",
      "247815                                                                              Muncul isu Jaksa Agung di copot ? Isu pencopotan ini muncul beriringan dengan perintah pengamanan TNI untuk Kejaksaan. ? Terlalu banyak isu untuk mengaburkan kasus kasus besar?? TOLAK RUU KEJAKSAAN   \n",
      "247816  Setelah ada perintah pengamanan TNI untuk Kejaksaan, tiba tiba beredar isu soal Pencopotan Jaksa Agung ST Burhanuddin, Ada apakah..?? Besok Senin, kita tggu aja klanjutan beritanya Genkz. Persebaya Projo Jambu MasyaAllah PPATK Amanda Dewtee minggu malam Tolak RUU kejaksaan   \n",
      "247817             Bahkan Ketua DPR RI Puan Maharani pun meminta agar TNI memberikan penjelasan resmi terkait pengamanan di kejati dan kejari. Karena penjelasan yg transparan sgt penting agar tidak menimbulkan dugaan atau prasangka negatif di tengah masyarakat. Tolak RUU Kejaksaan   \n",
      "247818                                Daku kurang setuju jika prajurit TNI ditugaskan Ngawal Pejabat Kejaksaan, kesannya prajurit TNI sejak 1945 - 2025 ngganggur gitu loh, ga ada kerjaan? Mbok ya daripada prajurit mending pendekar PSHT saja pak JAGUNG ? GLODOK! TOLAK RUU KEJAKSAAN   \n",
      "247819                                                   Belum reda pertanyaan Publik Soal Alasan Mengapa Prajurit TNI Menjaga Kantor2 Kejaksaan , Kini justru berhembus berita kalau Jaksa Agung ST Burhanudin akan di \"Geser\" dari Jabatannya ? wow..ada apa ini..? Tolak RUU Kejaksaan   \n",
      "\n",
      "        predicted_label  confidence  prob_False  prob_True  \n",
      "0                  True    0.993902    0.006098   0.993902  \n",
      "1                  True    0.989470    0.010530   0.989470  \n",
      "2                  True    0.994626    0.005374   0.994626  \n",
      "3                  True    0.978915    0.021085   0.978915  \n",
      "4                  True    0.979492    0.020508   0.979492  \n",
      "...                 ...         ...         ...        ...  \n",
      "247815             True    0.922552    0.077448   0.922552  \n",
      "247816            False    0.570212    0.570212   0.429788  \n",
      "247817             True    0.694872    0.305128   0.694872  \n",
      "247818             True    0.889625    0.110375   0.889625  \n",
      "247819             True    0.763396    0.236604   0.763396  \n",
      "\n",
      "[247820 rows x 5 columns]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "6830114f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:15:19.655196Z",
     "start_time": "2025-11-19T15:14:58.157208Z"
    }
   },
   "source": [
    "source_ds = ds[\"source_stage_2\"]\n",
    "predictions_list = df[\"predicted_label\"].tolist()\n",
    "# 3. Define the function that .map() will run on each row\n",
    "def update_row(example, idx):\n",
    "    \"\"\"\n",
    "    Takes a single row (example) and its index (idx).\n",
    "    It replaces the value in 'relevant' with the new prediction.\n",
    "    \"\"\"\n",
    "    example['relevant'] = predictions_list[idx]\n",
    "    return example\n",
    "\n",
    "# 4. Apply the function to the entire dataset\n",
    "# .map() returns a new dataset, so we must re-assign it\n",
    "print(\"Starting to update 'relevant' column...\")\n",
    "source_ds = source_ds.map(update_row, with_indices=True)\n",
    "\n",
    "print(\"Column updated successfully!\")\n",
    "print(source_ds)\n",
    "\n",
    "# Check the first row to see the change\n",
    "# You should now see your string label (e.g., \"True\" or \"False\")\n",
    "# instead of the old boolean (e.g., True or False)\n",
    "print(source_ds[5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to update 'relevant' column...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/247820 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e23a0d554b9044d097746671966d21ac"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column updated successfully!\n",
      "Dataset({\n",
      "    features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
      "    num_rows: 247820\n",
      "})\n",
      "{'tweet_id': '1909950840775020808', 'time': '2025-04-09T12:45:40.000', 'author': '@lightfuryy_', 'content': 'Presiden kita ini udah KOSONG dari pas zaman debat pilpres. Dan skrg negara kita di pimpin oleh 2 manusia KOSONG bin DONGO \\n\\n#TolakRUUTNI #SupremasiSipil', 'comment_count': 0, 'repost_count': 2, 'like_count': 1, 'view_count': 463, 'relevant': True, 'sentiment': 0}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "2b694358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:15:21.602907Z",
     "start_time": "2025-11-19T15:15:21.108585Z"
    }
   },
   "source": [
    "view_ds = source_ds.select_columns([\"content\", \"relevant\"])\n",
    "for row in view_ds.to_list()[:10]:\n",
    "    print(row)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Kembalikan Tentara ke barak\\n#TolakRUUTNI', 'relevant': True}\n",
      "{'content': 'Masih ga nyangka suamiku tentara #tolakruutni', 'relevant': True}\n",
      "{'content': 'Beberapa zine yg akan saya bawa saat melapak nanti\\n\\n#TolakRUUTNI \\n#SupremasiSipil', 'relevant': True}\n",
      "{'content': 'Sebagai mahasiswa hukum, kami memiliki tanggung jawab akademik untuk mengawal proses pembentukan undang-undang agar memiliki proses partisipasi yg bermakna!\\n\\nberjuang turun ke jalan, bahkan berjuang di ruang sidang, semua sah dilakukan. yg penting #tolakruutni', 'relevant': True}\n",
      "{'content': 'sekali-kali ga nyebelin ye kite #FallingInLoveEra #TolakRUUTNI', 'relevant': True}\n",
      "{'content': 'Presiden kita ini udah KOSONG dari pas zaman debat pilpres. Dan skrg negara kita di pimpin oleh 2 manusia KOSONG bin DONGO \\n\\n#TolakRUUTNI #SupremasiSipil', 'relevant': True}\n",
      "{'content': 'Kalo bisa menteri dan pejabat negara lainnya ikut disandera juga.. wkwkwkkwk\\n\\n#TolakRUUTNI QRIS', 'relevant': True}\n",
      "{'content': 'Great \\nECCO CHI Ses\\nBingx3\\n#shailenzo #zelena #yulioli #grandefratello #zeudiners #ENHYPEN #ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆ #Trump #TolakRUUTNI', 'relevant': False}\n",
      "{'content': 'Kalau mau bertani jangan jadi tentara.\\n#TolakRUUTNI', 'relevant': True}\n",
      "{'content': '\"Whenever I lay down to close my eyesâ€¦ thatâ€™s when the nightmares begin. And lately, theyâ€™ve been getting worse.\"\\n-Dave Torres\\n\\n#TolakUUTNI #TolakRUUTNI #TolakRevisiUUTNI #PeringatanDarurat #IndonesiaGelap #TolakRUUPolri #TolakRKUHAP', 'relevant': True}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "b5834fec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:15:24.505450Z",
     "start_time": "2025-11-19T15:15:24.500476Z"
    }
   },
   "source": "ds[\"source_labeled\"] = source_ds",
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "9f5fe511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:15:39.613338Z",
     "start_time": "2025-11-19T15:15:25.683453Z"
    }
   },
   "source": "ds.push_to_hub(\"tianharjuno/twitter-parse\", commit_message=\"Labeled the source ds\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e693def6929c49f89771f75ceb32b155"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/202 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b8c33cc74b141c5ba8fd40dd345f403"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdbc93725f8f4060b1d586cdaffa854d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22d6f92b9437466193ad26ba73275e09"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3952815396a441a944e6d1b9b0fb0e2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/196 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f63a01ccd6f44f9bc271a4b8befa902"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57c8b92e902a406598c847e5514f6b70"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "974729cbd00f442fa6ab54b3d0e525b0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8d1301823bd4be3b3cce2e1c7276eed"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf4753b174924bf49a1cfa08e3ffc3ef"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4490f82343a045c79e34b2957c0645c8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6ce3399274d47638836259a3f4152d0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tianharjuno/twitter-parse/commit/1725d6fc4a0aec55fe5d567afcfbc69068ea6a59', commit_message='Labeled the source ds', commit_description='', oid='1725d6fc4a0aec55fe5d567afcfbc69068ea6a59', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tianharjuno/twitter-parse', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tianharjuno/twitter-parse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "a3069c68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:16:22.935536Z",
     "start_time": "2025-11-19T15:15:43.669166Z"
    }
   },
   "source": [
    "trained_model.push_to_hub(\"tianharjuno/ruu-tni-relevancy-classification-p1\", commit_description=\"Changed training pipeline\")\n",
    "tokenizer.push_to_hub(\"tianharjuno/ruu-tni-relevancy-classification-p1\", commit_description=\"Changed training pipeline\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dd2f6f624f947b5bc8980f66defab7b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76037a645d734e34990af6dd0d059409"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  /tmp/tmpy4a1iv05/model.safetensors    :   0%|          |  558kB /  442MB            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f9c490b2a814f96b254e58b6f3803a8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tianharjuno/ruu-tni-relevancy-classification-p1/commit/9be30636bcc8da7e582f516b6841a055c4341ffa', commit_message='Upload tokenizer', commit_description='Changed training pipeline', oid='9be30636bcc8da7e582f516b6841a055c4341ffa', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tianharjuno/ruu-tni-relevancy-classification-p1', endpoint='https://huggingface.co', repo_type='model', repo_id='tianharjuno/ruu-tni-relevancy-classification-p1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
