{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d01a598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 201583/201583 [00:00<00:00, 1336457.34 examples/s]\n",
      "Generating cleaned split: 100%|██████████| 201583/201583 [00:00<00:00, 2207872.57 examples/s]\n",
      "Generating sampled_1000 split: 100%|██████████| 999/999 [00:00<00:00, 148369.74 examples/s]\n",
      "Generating sampled_2000 split: 100%|██████████| 2002/2002 [00:00<00:00, 495251.94 examples/s]\n",
      "Generating sampled_3000 split: 100%|██████████| 2999/2999 [00:00<00:00, 615066.14 examples/s]\n",
      "Generating sampled_4000 split: 100%|██████████| 4000/4000 [00:00<00:00, 1024312.60 examples/s]\n",
      "Generating sampled_5000 split: 100%|██████████| 4999/4999 [00:00<00:00, 1225227.94 examples/s]\n",
      "Generating sampled_6000 split: 100%|██████████| 6002/6002 [00:00<00:00, 801547.83 examples/s]\n",
      "Generating sampled_7000 split: 100%|██████████| 7001/7001 [00:00<00:00, 1182186.17 examples/s]\n",
      "Generating sampled_8000 split: 100%|██████████| 8000/8000 [00:00<00:00, 1060473.18 examples/s]\n",
      "Generating sampled_9000 split: 100%|██████████| 9000/9000 [00:00<00:00, 1185799.33 examples/s]\n",
      "Generating sampled_10000 split: 100%|██████████| 10000/10000 [00:00<00:00, 1109134.76 examples/s]\n",
      "Generating sampled_11000 split: 100%|██████████| 11000/11000 [00:00<00:00, 1098795.97 examples/s]\n",
      "Generating sampled_12000 split: 100%|██████████| 12001/12001 [00:00<00:00, 1678197.05 examples/s]\n",
      "Generating sampled_13000 split: 100%|██████████| 12998/12998 [00:00<00:00, 1560900.26 examples/s]\n",
      "Generating sampled_14000 split: 100%|██████████| 14001/14001 [00:00<00:00, 1560533.88 examples/s]\n",
      "Generating sampled_15000 split: 100%|██████████| 15000/15000 [00:00<00:00, 1592007.89 examples/s]\n",
      "Generating sampled_16000 split: 100%|██████████| 16002/16002 [00:00<00:00, 1534108.63 examples/s]\n",
      "Generating sampled_17000 split: 100%|██████████| 16999/16999 [00:00<00:00, 1291133.49 examples/s]\n",
      "Generating sampled_18000 split: 100%|██████████| 18000/18000 [00:00<00:00, 1620186.96 examples/s]\n",
      "Generating sampled_19000 split: 100%|██████████| 19001/19001 [00:00<00:00, 1458698.09 examples/s]\n",
      "Generating sampled_20000 split: 100%|██████████| 19999/19999 [00:00<00:00, 1324081.48 examples/s]\n",
      "Generating test split: 100%|██████████| 1999/1999 [00:00<00:00, 291024.43 examples/s]\n",
      "Generating sampled_20000_labeled split: 100%|██████████| 19998/19998 [00:00<00:00, 2124077.37 examples/s]\n",
      "Generating cleaned_labeled split: 100%|██████████| 201583/201583 [00:00<00:00, 3069546.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ef0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = dataset[\"cleaned_labeled\"]\n",
    "relevant_dataset = cleaned_dataset.filter(lambda row: row[\"related\"] == True)\n",
    "irrelevant_dataset = cleaned_dataset.filter(lambda row: row[\"related\"] == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1e992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_text = relevant_dataset[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4a15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def get_top_n_hashtags(list_of_texts, n=10):\n",
    "    \"\"\"\n",
    "    Extracts all hashtags from a list of text strings, counts them,\n",
    "    and returns the top N most frequent hashtags.\n",
    "\n",
    "    Args:\n",
    "        list_of_texts (list): A list of strings to process.\n",
    "        n (int): The number of top hashtags to return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (hashtag, count) tuples, sorted by count descending.\n",
    "    \"\"\"\n",
    "    all_hashtags = []\n",
    "    \n",
    "    # Regex pattern: finds a '#' followed by one or more\n",
    "    # \"word\" characters (letters, numbers, or underscore).\n",
    "    pattern = r\"#\\w+\"\n",
    "    \n",
    "    for text in list_of_texts:\n",
    "        # Ensure the item is a string before processing\n",
    "        if isinstance(text, str):\n",
    "            # Find all matches and convert to lowercase for consistent counting\n",
    "            found_hashtags = re.findall(pattern, text.lower())\n",
    "            all_hashtags.extend(found_hashtags)\n",
    "            \n",
    "    # Count the frequency of all found hashtags\n",
    "    hashtag_counts = Counter(all_hashtags)\n",
    "    \n",
    "    # Get the top N most common\n",
    "    top_n = hashtag_counts.most_common(n)\n",
    "    \n",
    "    return top_n\n",
    "\n",
    "# 2. Set how many top hashtags you want\n",
    "top_n = 100\n",
    "\n",
    "# 3. Run the function\n",
    "top_hashtags = get_top_n_hashtags(relevant_text, n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef23b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christianharjuno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621 total stopwords\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import requests\n",
    "\n",
    "# Sastrawi\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords_id = set(factory.get_stop_words())\n",
    "\n",
    "# English\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "# IndoNLU\n",
    "indonlu_url = \"https://raw.githubusercontent.com/IndoNLP/indonlu/master/utils/stopwordbahasa.txt\"\n",
    "stopwords_indonlu = set(requests.get(indonlu_url).text.split())\n",
    "\n",
    "# IDN stopwords\n",
    "idn_url = \"https://raw.githubusercontent.com/Alir3z4/stop-words/master/indonesian.txt\"\n",
    "stopwords_idn = set(requests.get(idn_url).text.split())\n",
    "\n",
    "# Colloquial extensions (dialect + slang)\n",
    "extra_slang = {\n",
    "    \"ga\",\"gak\",\"nggak\",\"ngga\",\"aja\",\"nih\",\"dong\",\"deh\",\"lah\",\"loh\",\"kok\",\n",
    "    \"kan\",\"nya\",\"ya\",\"emang\",\"tau\",\"yg\",\"pd\",\"trs\",\"pls\",\"plis\",\"thx\",\"makasih\",\n",
    "    \"makasi\",\"terimakasih\",\"bgt\",\"bngt\",\"bener\",\"btw\",\"rt\",\"dm\",\"gw\",\"gue\",\"lu\",\n",
    "    \"loe\",\"lo\",\"gua\",\"ny\",\"nya\",\"jd\",\"jadi\",\"trs\",\"sih\",\"kayak\",\"kek\",\"oke\",\n",
    "    \"ok\",\"bro\",\"sis\",\"min\",\"kalo\",\"kalau\",\"dgn\",\"dengan\",\"bikin\",\"nih\",\"dong\"\n",
    "}\n",
    "\n",
    "# Combine and lowercase normalize\n",
    "combined_stopwords = set(\n",
    "    word.lower() for word in (\n",
    "        stopwords_id |\n",
    "        stopwords_en |\n",
    "        stopwords_indonlu |\n",
    "        stopwords_idn |\n",
    "        extra_slang\n",
    "    )\n",
    ")\n",
    "\n",
    "print(len(combined_stopwords), \"total stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eae0a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordsegment model loaded.\n",
      "Loaded 111949 lines to process.\n",
      "Processing and filtering hashtags in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111949/111949 [10:29<00:00, 177.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating results...\n",
      "\n",
      "Filtered candidate count: 29121\n",
      "Filtered 'meaning' doc sample: 'dan memb uk tik an pertamina ber jalan seperti yang bukan se kedar kembali kan ten tara barak ken apa inst ansi yang mem per boleh kan tidak dengan ran ah masih engg ak men yang ka ten tara beberapa y...'\n",
      "\n",
      "Running KeyBERT on filtered data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 50 Semantically-Ranked (and Filtered) Hashtags ---\n",
      "  - 'negara2an' (Score: 0.4902)\n",
      "  - 'negara' (Score: 0.4331)\n",
      "  - 'debat' (Score: 0.4330)\n",
      "  - 'penjahat2' (Score: 0.4272)\n",
      "  - 'negara2' (Score: 0.4215)\n",
      "  - 'debate' (Score: 0.4155)\n",
      "  - 'pemerintahyg' (Score: 0.4122)\n",
      "  - 'kebangsaan' (Score: 0.4111)\n",
      "  - 'kesewenangwenangan' (Score: 0.4081)\n",
      "  - 'kerana' (Score: 0.4033)\n",
      "  - 'treasonous' (Score: 0.4028)\n",
      "  - 'masyarakat' (Score: 0.4001)\n",
      "  - 'authoritarianism' (Score: 0.3983)\n",
      "  - 'lagipemrintah' (Score: 0.3968)\n",
      "  - 'pemerintahanpeneliti' (Score: 0.3909)\n",
      "  - 'pemerintahnya' (Score: 0.3893)\n",
      "  - 'penyelewengannya' (Score: 0.3876)\n",
      "  - 'ada' (Score: 0.3859)\n",
      "  - 'pokoknyaaw' (Score: 0.3856)\n",
      "  - 'bandit' (Score: 0.3836)\n",
      "  - 'kesewenang2an' (Score: 0.3825)\n",
      "  - 'anarchism' (Score: 0.3816)\n",
      "  - 'menyangkutpautkan' (Score: 0.3780)\n",
      "  - 'menyebabkannya' (Score: 0.3779)\n",
      "  - 'ketidakterbukaan' (Score: 0.3763)\n",
      "  - 'ketidaktahuannya' (Score: 0.3743)\n",
      "  - 'penyebabnya' (Score: 0.3725)\n",
      "  - 'kenapa2' (Score: 0.3720)\n",
      "  - 'sebab' (Score: 0.3704)\n",
      "  - 'liberal' (Score: 0.3704)\n",
      "  - 'musuh2' (Score: 0.3691)\n",
      "  - 'segelap2nya' (Score: 0.3690)\n",
      "  - 'kericuhanmahasiswa' (Score: 0.3684)\n",
      "  - 'egalitarianism' (Score: 0.3682)\n",
      "  - 'country' (Score: 0.3680)\n",
      "  - 'pemerintahmu' (Score: 0.3679)\n",
      "  - 'hypocrite' (Score: 0.3677)\n",
      "  - 'mengkondusifkan' (Score: 0.3670)\n",
      "  - 'nations' (Score: 0.3669)\n",
      "  - 'sebagai' (Score: 0.3656)\n",
      "  - 'diharapkanpuan' (Score: 0.3653)\n",
      "  - 'ngawujudkeun' (Score: 0.3619)\n",
      "  - 'pemerintah2' (Score: 0.3615)\n",
      "  - 'menteri2nya' (Score: 0.3614)\n",
      "  - 'menyelewengkan' (Score: 0.3611)\n",
      "  - 'dikambinghitamkan' (Score: 0.3610)\n",
      "  - 'ngarujudakan' (Score: 0.3603)\n",
      "  - 'inggris' (Score: 0.3596)\n",
      "  - 'bukti2nya' (Score: 0.3594)\n",
      "  - 'rakyat' (Score: 0.3578)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import wordsegment\n",
    "from keybert import KeyBERT\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os # Added for ProcessPoolExecutor\n",
    "\n",
    "# --- 1. Set Global Constants ---\n",
    "MIN_AVG_WORD_LEN = 2.5\n",
    "\n",
    "# --- 2. Define Worker Function (MUST be at top level) ---\n",
    "# This function will be sent to each CPU core.\n",
    "def process_line(text_line):\n",
    "    \"\"\"\n",
    "    Processes a single line of text to find valid, non-noise hashtags\n",
    "    and their segmented \"meaning\".\n",
    "    \"\"\"\n",
    "    local_doc_parts = []\n",
    "    local_candidates = set()\n",
    "    \n",
    "    # Ensure input is a string\n",
    "    if not isinstance(text_line, str):\n",
    "        return local_doc_parts, local_candidates\n",
    "\n",
    "    hashtags = text_line.split()\n",
    "\n",
    "    for tag in hashtags:\n",
    "        # 1. Segment the hashtag\n",
    "        segmented_words = wordsegment.segment(tag)\n",
    "        \n",
    "        if not segmented_words:\n",
    "            continue\n",
    "            \n",
    "        # 2. Calculate average word length\n",
    "        avg_len = sum(len(w) for w in segmented_words) / len(segmented_words)\n",
    "        \n",
    "        # 3. Apply the filter\n",
    "        if avg_len >= MIN_AVG_WORD_LEN:\n",
    "            segmented_sentence = \" \".join(segmented_words)\n",
    "            \n",
    "            # 4. Optional: Language check\n",
    "            try:\n",
    "                lang = detect(segmented_sentence)\n",
    "                if lang not in ['id', 'en']:\n",
    "                    continue\n",
    "            except:\n",
    "                pass \n",
    "                \n",
    "            # If all filters pass, add the data\n",
    "            local_doc_parts.append(segmented_sentence)\n",
    "            local_candidates.add(tag)\n",
    "            \n",
    "    return local_doc_parts, local_candidates\n",
    "\n",
    "# --- 3. Main execution block (REQUIRED for multiprocessing) ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Load Wordsegment Model (once in main process) ---\n",
    "    try:\n",
    "        wordsegment.load()\n",
    "        print(\"Wordsegment model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load wordsegment data: {e}\")\n",
    "        print(\"Please run 'pip install wordsegment' and ensure you have internet.\")\n",
    "        raise e\n",
    "    \n",
    "    print(f\"Loaded {len(relevant_text)} lines to process.\")\n",
    "\n",
    "    # --- 4. Process Data in Parallel ---\n",
    "    doc_for_embedding_parts = []\n",
    "    hashtag_candidates = set()\n",
    "\n",
    "    print(\"Processing and filtering hashtags in parallel...\")\n",
    "    \n",
    "    # Use ProcessPoolExecutor to run CPU-bound tasks in parallel\n",
    "    # It will automatically use all available CPU cores\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        # 'executor.map' runs 'process_line' on each item in 'relevant_text'\n",
    "        # We wrap the map iterator in tqdm to get a single, clean progress bar\n",
    "        results = list(tqdm(executor.map(process_line, relevant_text), total=len(relevant_text)))\n",
    "\n",
    "    # --- 5. Aggregate Results from All Processes ---\n",
    "    print(\"Aggregating results...\")\n",
    "    for doc_parts, candidates in results:\n",
    "        doc_for_embedding_parts.extend(doc_parts)\n",
    "        hashtag_candidates.update(candidates)\n",
    "\n",
    "    # --- 6. Check if We Have Any Data Left ---\n",
    "    doc_for_embedding = \" \".join(doc_for_embedding_parts)\n",
    "    hashtag_candidates_list = list(hashtag_candidates)\n",
    "\n",
    "    if not doc_for_embedding or not hashtag_candidates_list:\n",
    "        print(\"\\nError: After filtering, no valid hashtags or corpus content was found.\")\n",
    "        print(\"Your dataset might be 100% noise, or your MIN_AVG_WORD_LEN is too high.\")\n",
    "    else:\n",
    "        print(f\"\\nFiltered candidate count: {len(hashtag_candidates_list)}\")\n",
    "        print(f\"Filtered 'meaning' doc sample: '{doc_for_embedding[:200]}...'\")\n",
    "\n",
    "        # --- 7. Load KeyBERT and Extract Keywords ---\n",
    "        model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        kw_model = KeyBERT(model=model_name)\n",
    "\n",
    "        print(\"\\nRunning KeyBERT on filtered data...\")\n",
    "        \n",
    "        # **FIX**: Use 'doc=' for a single string, not 'docs='\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            docs=doc_for_embedding,      \n",
    "            candidates=hashtag_candidates_list, \n",
    "            top_n=50\n",
    "        )\n",
    "\n",
    "        # --- 8. View Your (Now Correct) Results ---\n",
    "        print(\"\\n--- Top 50 Semantically-Ranked (and Filtered) Hashtags ---\")\n",
    "        for keyword, score in keywords:\n",
    "            print(f\"  - '{keyword}' (Score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "025496a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['404'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 200 most used tokens (words and hashtags):\n",
      "  #indonesiagelap: 54270 times\n",
      "  #tolakruupolri: 45941 times\n",
      "  #peringatandarurat: 39188 times\n",
      "  tni: 37127 times\n",
      "  #tolakdwifungsiabri: 36801 times\n",
      "  #tolakrevisiuutni: 36717 times\n",
      "  #peringatandarurat #indonesiagelap: 34013 times\n",
      "  #tolakruutni: 33754 times\n",
      "  ruu: 27235 times\n",
      "  #cabutruutni: 25443 times\n",
      "  #supremasisipil: 24765 times\n",
      "  ruu tni: 23497 times\n",
      "  #tolakruukejaksaan: 23470 times\n",
      "  #cabutuutni: 23239 times\n",
      "  #indonesiagelap #tolakdwifungsiabri: 23145 times\n",
      "  #tolakrevisiuutni #peringatandarurat: 21731 times\n",
      "  #peringatandarurat #indonesiagelap #tolakdwifungsiabri: 21497 times\n",
      "  #tolakrevisiuutni #peringatandarurat #indonesiagelap: 21101 times\n",
      "  #tolakruupolri #tolakruukejaksaan: 20598 times\n",
      "  #tolakrevisiuutni #peringatandarurat #indonesiagelap #tolakdwifungsiabri: 20294 times\n",
      "  #tolakdwifungsiabri #supremasisipil: 19547 times\n",
      "  #tolakruutni #tolakrevisiuutni: 18881 times\n",
      "  #indonesiagelap #tolakdwifungsiabri #supremasisipil: 16611 times\n",
      "  #tolakuutni: 16532 times\n",
      "  #peringatandarurat #indonesiagelap #tolakdwifungsiabri #supremasisipil: 16278 times\n",
      "  #supremasisipil #tolakruupolri: 16137 times\n",
      "  #tolakrevisiuutni #peringatandarurat #indonesiagelap #tolakdwifungsiabri #supremasisipil: 15931 times\n",
      "  #tolakdwifungsiabri #supremasisipil #tolakruupolri: 15784 times\n",
      "  #gagalkanruutni: 14031 times\n",
      "  #indonesiagelap #tolakdwifungsiabri #supremasisipil #tolakruupolri: 13545 times\n",
      "  #peringatandarurat #indonesiagelap #tolakdwifungsiabri #supremasisipil #tolakruupolri: 13497 times\n",
      "  #gagalkanruutni #cabutruutni: 13206 times\n",
      "  #supremasisipil #tolakruupolri #tolakruukejaksaan: 13115 times\n",
      "  #tolakdwifungsiabri #supremasisipil #tolakruupolri #tolakruukejaksaan: 12969 times\n",
      "  dwifungsi: 12759 times\n",
      "  #tolakruutni #tolakrevisiuutni #peringatandarurat: 12579 times\n",
      "  #tolakruutni #tolakrevisiuutni #peringatandarurat #indonesiagelap: 12211 times\n",
      "  #tolakruutni #tolakrevisiuutni #peringatandarurat #indonesiagelap #tolakdwifungsiabri: 11628 times\n",
      "  #indonesiagelap #tolakdwifungsiabri #supremasisipil #tolakruupolri #tolakruukejaksaan: 10875 times\n",
      "  #cabutuutni #tolakuutni: 10295 times\n",
      "  #tolakuutni #tolakrevisiuutni: 10179 times\n",
      "  #cabutruutni #peringatandarurat: 9723 times\n",
      "  #gagalkanuutni: 9551 times\n",
      "  #indonesiagelap #tolakruutni: 9422 times\n",
      "  #cabutuutni #tolakuutni #tolakrevisiuutni: 9400 times\n",
      "  #cabutruutni #peringatandarurat #indonesiagelap: 9396 times\n",
      "  #gagalkanruutni #cabutruutni #peringatandarurat: 9141 times\n",
      "  #peringatandarurat #indonesiagelap #tolakruutni: 9124 times\n",
      "  #indonesiagelap #tolakruutni #tolakrevisiuutni: 8908 times\n",
      "  #gagalkanruutni #cabutruutni #peringatandarurat #indonesiagelap: 8844 times\n",
      "  #cabutruutni #peringatandarurat #indonesiagelap #tolakruutni: 8789 times\n",
      "  #gagalkanuutni #cabutuutni: 8740 times\n",
      "  #peringatandarurat #indonesiagelap #tolakruutni #tolakrevisiuutni: 8736 times\n",
      "  #tolakuutni #tolakrevisiuutni #peringatandarurat: 8524 times\n",
      "  #tolakuutni #tolakrevisiuutni #peringatandarurat #indonesiagelap: 8493 times\n",
      "  #cabutruutni #peringatandarurat #indonesiagelap #tolakruutni #tolakrevisiuutni: 8425 times\n",
      "  #gagalkanruutni #cabutruutni #peringatandarurat #indonesiagelap #tolakruutni: 8357 times\n",
      "  #tolakuutni #tolakrevisiuutni #peringatandarurat #indonesiagelap #tolakdwifungsiabri: 8332 times\n",
      "  indonesia: 8280 times\n",
      "  #gagalkanuutni #cabutuutni #tolakuutni: 8129 times\n",
      "  #cabutuutni #tolakuutni #tolakrevisiuutni #peringatandarurat: 8083 times\n",
      "  #cabutuutni #tolakuutni #tolakrevisiuutni #peringatandarurat #indonesiagelap: 8054 times\n",
      "  #gagalkanuutni #cabutuutni #tolakuutni #tolakrevisiuutni: 7952 times\n",
      "  #gagalkanuutni #cabutuutni #tolakuutni #tolakrevisiuutni #peringatandarurat: 7745 times\n",
      "  #tolakdwifungsiabri #tolakruupolri: 6858 times\n",
      "  rakyat: 6692 times\n",
      "  #indonesiagelap #tolakruutni #tolakrevisiuutni #peringatandarurat: 6213 times\n",
      "  demo: 6204 times\n",
      "  #peringatandarurat #indonesiagelap #tolakruutni #tolakrevisiuutni #peringatandarurat: 6195 times\n",
      "  #indonesiagelap #tolakruutni #tolakrevisiuutni #peringatandarurat #indonesiagelap: 6107 times\n",
      "  #cabutruutni #tolakruupolri: 5915 times\n",
      "  uu: 5911 times\n",
      "  #tolakrevisiuutni #tolakdwifungsiabri: 5834 times\n",
      "  #cabutuutni #tolakruupolri: 5152 times\n",
      "  abri: 5148 times\n",
      "  orang: 5076 times\n",
      "  negara: 5040 times\n",
      "  #tolakdwifungsiabri #tolakruupolri #tolakruukejaksaan: 5004 times\n",
      "  banget: 4683 times\n",
      "  dwifungsi abri: 4643 times\n",
      "  #cabutruutni #tolakruutni: 4554 times\n",
      "  #tolakruutni #tolakrevisiuutni #tolakdwifungsiabri: 4482 times\n",
      "  #kembalikantnikebarak: 4106 times\n",
      "  #cabutruutni #tolakruutni #tolakrevisiuutni: 4101 times\n",
      "  mahasiswa: 4039 times\n",
      "  sipil: 4035 times\n",
      "  #indonesiagelap #tolakdwifungsiabri #tolakruupolri: 4009 times\n",
      "  dpr: 3928 times\n",
      "  #indonesiagelap #tolakdwifungsiabri #tolakruupolri #tolakruukejaksaan: 3824 times\n",
      "  uu tni: 3818 times\n",
      "  pemerintah: 3807 times\n",
      "  aksi: 3716 times\n",
      "  polri: 3707 times\n",
      "  demonstrasi: 3386 times\n",
      "  #gagalkanruutni #cabutruutni #tolakruutni: 3309 times\n",
      "  terus: 3289 times\n",
      "  #gagalkanruutni #cabutruutni #tolakruutni #tolakrevisiuutni: 3246 times\n",
      "  #cabutruutni #tolakruutni #tolakrevisiuutni #peringatandarurat: 3226 times\n",
      "  #cabutruutni #tolakruutni #tolakrevisiuutni #peringatandarurat #indonesiagelap: 3179 times\n",
      "  #tolakruukejaksaan #kembalikantnikebarak: 3115 times\n",
      "  tetap: 3021 times\n",
      "  #tolakruupolri #tolakruukejaksaan #kembalikantnikebarak: 3019 times\n",
      "  url: 2990 times\n",
      "  #peringatandarurat #indonesiagelap #tolakdwifungsiabri #tolakruupolri: 2973 times\n",
      "  #tolakruupolri #indonesiagelap: 2930 times\n",
      "  #tolakruutni #tolakruupolri: 2912 times\n",
      "  #supremasisipil #tolakruupolri #tolakruukejaksaan #kembalikantnikebarak: 2892 times\n",
      "  #tolakdwifungsiabri #supremasisipil #tolakruupolri #tolakruukejaksaan #kembalikantnikebarak: 2876 times\n",
      "  #makzulkanprabowogibran: 2847 times\n",
      "  revisi: 2834 times\n",
      "  #peringatandarurat #indonesiagelap #tolakdwifungsiabri #tolakruupolri #tolakruukejaksaan: 2832 times\n",
      "  #tolakrevisiuutni #peringatandarurat #indonesiagelap #tolakdwifungsiabri #tolakruupolri: 2745 times\n",
      "  #tolakdwifungsitni: 2711 times\n",
      "  hari: 2640 times\n",
      "  gelap: 2631 times\n",
      "  tolak: 2630 times\n",
      "  #tolakruukejaksan: 2622 times\n",
      "  #tolakrevisiuutni #tolakdwifungsiabri #supremasisipil: 2575 times\n",
      "  #gagalkanruutni #cabutruutni #tolakruutni #tolakrevisiuutni #peringatandarurat: 2565 times\n",
      "  #tolakruupolri #tolakruukejaksan: 2535 times\n",
      "  #indonesiagelap #tolakruutni #tolakrevisiuutni #tolakdwifungsiabri: 2526 times\n",
      "  rasa: 2524 times\n",
      "  polisi: 2503 times\n",
      "  baru: 2493 times\n",
      "  teman: 2449 times\n",
      "  #supremasisipil #tolakruupolri #tolakruukejaksan: 2438 times\n",
      "  militer: 2415 times\n",
      "  #peringatandarurat #indonesiagelap #tolakruutni #tolakrevisiuutni #tolakdwifungsiabri: 2382 times\n",
      "  #tolakruupolri #tolakdwifungsiabri: 2360 times\n",
      "  benar: 2343 times\n",
      "  #tolakdwifungsiabri #supremasisipil #tolakruupolri #tolakruukejaksan: 2324 times\n",
      "  #tolakruutni #tolakrevisiuutni #tolakdwifungsiabri #supremasisipil: 2314 times\n",
      "  #indonesiagelap #tolakdwifungsiabri #supremasisipil #tolakruupolri #tolakruukejaksan: 2312 times\n",
      "  indonesia gelap: 2268 times\n",
      "  prabowo: 2267 times\n",
      "  disahkan: 2262 times\n",
      "  unjuk: 2250 times\n",
      "  unjuk rasa: 2218 times\n",
      "  soal: 2156 times\n",
      "  masyarakat: 2116 times\n",
      "  #indonesiagelap #tolakruutni #tolakrevisiuutni #tolakdwifungsiabri #supremasisipil: 2104 times\n",
      "  demokrasi: 2088 times\n",
      "  pertahanan: 2088 times\n",
      "  #tolakrevisiuutni #tolakdwifungsiabri #supremasisipil #tolakruupolri: 2076 times\n",
      "  #indonesiagelap #peringatandarurat: 2050 times\n",
      "  #tolakruukuhap: 2032 times\n",
      "  #tolakrevisiuutni #tolakdwifungsiabri #supremasisipil #tolakruupolri #tolakruukejaksaan: 2014 times\n",
      "  hati: 1977 times\n",
      "  isu: 1969 times\n",
      "  #cabutuutni #tolakruupolri #indonesiagelap: 1959 times\n",
      "  #tolakrevisiuutni #tolakruupolri: 1954 times\n",
      "  revisi uu: 1909 times\n",
      "  presiden: 1900 times\n",
      "  #tolakruutni #tolakrevisiuutni #tolakdwifungsiabri #supremasisipil #tolakruupolri: 1851 times\n",
      "  kawal: 1838 times\n",
      "  #tolakrevisiuutni #tolakdwifungsiabri #tolakruupolri: 1834 times\n",
      "  menang: 1824 times\n",
      "  #tolakrevisiuutni #indonesiagelap: 1810 times\n",
      "  baik: 1765 times\n",
      "  #tolakrevisiuutni #tolakruutni: 1734 times\n",
      "  tolak ruu: 1688 times\n",
      "  #indonesiagelapjilid2: 1678 times\n",
      "  #tolakruutni #cabutruutni: 1647 times\n",
      "  masa: 1631 times\n",
      "  dukung: 1619 times\n",
      "  #tolakruukejaksan #makzulkanprabowogibran: 1585 times\n",
      "  pak: 1584 times\n",
      "  #tolakruupolri #tolakruukejaksan #makzulkanprabowogibran: 1580 times\n",
      "  #supremasisipil #tolakruupolri #tolakruukejaksan #makzulkanprabowogibran: 1577 times\n",
      "  #tolakdwifungsiabri #supremasisipil #tolakruupolri #tolakruukejaksan #makzulkanprabowogibran: 1575 times\n",
      "  undang: 1557 times\n",
      "  aparat: 1554 times\n",
      "  revisi uu tni: 1548 times\n",
      "  tolak ruu tni: 1544 times\n",
      "  semangat: 1541 times\n",
      "  hidup: 1533 times\n",
      "  tni polri: 1525 times\n",
      "  dwifungsi tni: 1518 times\n",
      "  #tolakruutni #tolakdwifungsiabri: 1515 times\n",
      "  2025: 1507 times\n",
      "  tuh: 1486 times\n",
      "  tni disahkan: 1479 times\n",
      "  turun: 1476 times\n",
      "  #tolakrevisiuutni #indonesiagelap #tolakdwifungsiabri: 1469 times\n",
      "  lihat: 1468 times\n",
      "  biar: 1462 times\n",
      "  #indonesiamakingelap: 1445 times\n",
      "  rezim: 1444 times\n",
      "  kemarin: 1435 times\n",
      "  #tolakuutni #tolakruupolri: 1433 times\n",
      "  satu: 1430 times\n",
      "  punya: 1428 times\n",
      "  #ruutni: 1427 times\n",
      "  #tolakruutni #cabutuutni: 1419 times\n",
      "  salah: 1419 times\n",
      "  #tolakruutni #tolakrevisiuutni #tolakdwifungsiabri #tolakruupolri: 1416 times\n",
      "  hukum: 1401 times\n",
      "  #cabutuutni #tolakruutni: 1397 times\n",
      "  ikut: 1377 times\n",
      "  ruu tni disahkan: 1374 times\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_tokens(list_of_texts, n=10, stop_words=None):\n",
    "    \"\"\"\n",
    "    Extracts all words and hashtags from a list of text strings, \n",
    "    counts them, and returns the top N most frequent tokens.\n",
    "\n",
    "    Args:\n",
    "        list_of_texts (list): A list of strings to process.\n",
    "        n (int): The number of top tokens to return.\n",
    "        stop_words (list, optional): A list of stopwords to ignore.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (token, count) tuples, sorted by count descending.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This regex pattern finds:\n",
    "    # 1. (#\\w+)   : A hashtag (e.g., #python)\n",
    "    # 2. |         : OR\n",
    "    # 3. (\\b\\w\\w+\\b): A regular word of 2 or more characters (e.g., python)\n",
    "    # The order is important so \"#python\" isn't matched as just \"python\".\n",
    "    token_pattern = r\"(#\\w+|\\b\\w\\w+\\b)\"\n",
    "    \n",
    "    # Filter out any non-string items\n",
    "    valid_texts = [text for text in list_of_texts if isinstance(text, str)]\n",
    "    \n",
    "    if not valid_texts:\n",
    "        return []\n",
    "\n",
    "    # Initialize the vectorizer with the new pattern and stopwords\n",
    "    vectorizer = CountVectorizer(\n",
    "        token_pattern=token_pattern, \n",
    "        lowercase=True,\n",
    "        stop_words=stop_words,\n",
    "        ngram_range=(1,5)\n",
    "    )\n",
    "    \n",
    "    # 1. Fit and transform the text\n",
    "    X = vectorizer.fit_transform(valid_texts)\n",
    "    \n",
    "    # 2. Get the feature names (words and hashtags)\n",
    "    tokens = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # 3. Sum the counts for each token\n",
    "    counts = X.sum(axis=0).A1\n",
    "    \n",
    "    # 4. Zip tokens and counts\n",
    "    tag_counts = list(zip(tokens, counts))\n",
    "    \n",
    "    # 5. Sort by count in descending order\n",
    "    sorted_tokens = sorted(tag_counts, key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # 6. Return the top N\n",
    "    return sorted_tokens[:n]\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# 3. Set how many top tokens you want\n",
    "top_n = 200\n",
    "\n",
    "# 4. Run the function\n",
    "top_tokens = get_top_n_tokens(relevant_text, n=top_n, stop_words=list(combined_stopwords))\n",
    "\n",
    "# 5. Print the results\n",
    "print(f\"Top {top_n} most used tokens (words and hashtags):\")\n",
    "for token, count in top_tokens:\n",
    "    print(f\"  {token}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79ca4414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-aggregated Top Tokens:\n",
      "  #indonesiagelap: 453502 times\n",
      "  #peringatandarurat: 416376 times\n",
      "  #tolakrevisiuutni: 349856 times\n",
      "  #tolakdwifungsiabri: 330818 times\n",
      "  #tolakruupolri: 247640 times\n",
      "  #tolakruutni: 221208 times\n",
      "  #supremasisipil: 217982 times\n",
      "  #cabutruutni: 133066 times\n",
      "  #tolakuutni: 113151 times\n",
      "  #tolakruukejaksaan: 106603 times\n",
      "  #cabutuutni: 101564 times\n",
      "  #gagalkanruutni: 62699 times\n",
      "  #gagalkanuutni: 42117 times\n",
      "  tni: 37127 times\n",
      "  ruu: 27235 times\n",
      "  ruu tni: 23497 times\n",
      "  #tolakruukejaksan: 18548 times\n",
      "  #kembalikantnikebarak: 16008 times\n",
      "  dwifungsi: 12759 times\n",
      "  #makzulkanprabowogibran: 9164 times\n",
      "  indonesia: 8280 times\n",
      "  rakyat: 6692 times\n",
      "  demo: 6204 times\n",
      "  uu: 5911 times\n",
      "  abri: 5148 times\n",
      "  orang: 5076 times\n",
      "  negara: 5040 times\n",
      "  banget: 4683 times\n",
      "  dwifungsi abri: 4643 times\n",
      "  mahasiswa: 4039 times\n",
      "  sipil: 4035 times\n",
      "  dpr: 3928 times\n",
      "  uu tni: 3818 times\n",
      "  pemerintah: 3807 times\n",
      "  aksi: 3716 times\n",
      "  polri: 3707 times\n",
      "  demonstrasi: 3386 times\n",
      "  terus: 3289 times\n",
      "  tetap: 3021 times\n",
      "  url: 2990 times\n",
      "  revisi: 2834 times\n",
      "  #tolakdwifungsitni: 2711 times\n",
      "  hari: 2640 times\n",
      "  gelap: 2631 times\n",
      "  tolak: 2630 times\n",
      "  rasa: 2524 times\n",
      "  polisi: 2503 times\n",
      "  baru: 2493 times\n",
      "  teman: 2449 times\n",
      "  militer: 2415 times\n",
      "  benar: 2343 times\n",
      "  indonesia gelap: 2268 times\n",
      "  prabowo: 2267 times\n",
      "  disahkan: 2262 times\n",
      "  unjuk: 2250 times\n",
      "  unjuk rasa: 2218 times\n",
      "  soal: 2156 times\n",
      "  masyarakat: 2116 times\n",
      "  demokrasi: 2088 times\n",
      "  pertahanan: 2088 times\n",
      "  #tolakruukuhap: 2032 times\n",
      "  hati: 1977 times\n",
      "  isu: 1969 times\n",
      "  revisi uu: 1909 times\n",
      "  presiden: 1900 times\n",
      "  kawal: 1838 times\n",
      "  menang: 1824 times\n",
      "  baik: 1765 times\n",
      "  tolak ruu: 1688 times\n",
      "  #indonesiagelapjilid2: 1678 times\n",
      "  masa: 1631 times\n",
      "  dukung: 1619 times\n",
      "  pak: 1584 times\n",
      "  undang: 1557 times\n",
      "  aparat: 1554 times\n",
      "  revisi uu tni: 1548 times\n",
      "  tolak ruu tni: 1544 times\n",
      "  semangat: 1541 times\n",
      "  hidup: 1533 times\n",
      "  tni polri: 1525 times\n",
      "  dwifungsi tni: 1518 times\n",
      "  2025: 1507 times\n",
      "  tuh: 1486 times\n",
      "  tni disahkan: 1479 times\n",
      "  turun: 1476 times\n",
      "  lihat: 1468 times\n",
      "  biar: 1462 times\n",
      "  #indonesiamakingelap: 1445 times\n",
      "  rezim: 1444 times\n",
      "  kemarin: 1435 times\n",
      "  satu: 1430 times\n",
      "  punya: 1428 times\n",
      "  #ruutni: 1427 times\n",
      "  salah: 1419 times\n",
      "  hukum: 1401 times\n",
      "  ikut: 1377 times\n",
      "  ruu tni disahkan: 1374 times\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def reaggregate_hashtag_counts(top_tokens):\n",
    "    \"\"\"\n",
    "    Re-processes a list of (token, count) tuples.\n",
    "    \n",
    "    - If a token is just words (e.g., 'ruu tni'), its count is kept as is.\n",
    "    - If a token contains hashtags (e.g., '#tolakruu #tolakpolri'), its\n",
    "      count is added to *each* individual hashtag found within it.\n",
    "    \"\"\"\n",
    "    new_counts = Counter()\n",
    "    pattern = re.compile(r\"#\\w+\")\n",
    "    \n",
    "    for token, count in top_tokens:\n",
    "        hashtags_found = pattern.findall(token)\n",
    "        \n",
    "        if not hashtags_found:\n",
    "            # Case 1: No hashtags. This is a word n-gram.\n",
    "            # Add it to the new counter directly.\n",
    "            # e.g., new_counts['tolak ruu'] += 5\n",
    "            new_counts[token] += count\n",
    "        else:\n",
    "            # Case 2: Hashtags were found.\n",
    "            # Add the count to *each* individual hashtag.\n",
    "            for tag in hashtags_found:\n",
    "                # e.g., new_counts['#tolakruu'] += 1\n",
    "                # e.g., new_counts['#tolakpolri'] += 1\n",
    "                new_counts[tag] += count\n",
    "                \n",
    "    # Return the new, correctly aggregated counts, sorted descending\n",
    "    return new_counts.most_common()\n",
    "# 2. Run the function\n",
    "reaggregated_tokens = reaggregate_hashtag_counts(top_tokens)\n",
    "\n",
    "# 3. Print the results\n",
    "print(\"Re-aggregated Top Tokens:\")\n",
    "\n",
    "for token, count in reaggregated_tokens:\n",
    "    print(f\"  {token}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e71ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordsegment model loaded.\n",
      "\n",
      "Cleaning and re-aggregating final list...\n",
      "\n",
      "--- Final Clean Token List ---\n",
      "  '#indonesiagelap': 453502 times\n",
      "  '#peringatandarurat': 416376 times\n",
      "  '#tolakrevisiuutni': 349856 times\n",
      "  '#tolakdwifungsiabri': 330818 times\n",
      "  '#tolakruupolri': 247640 times\n",
      "  '#tolakruutni': 221208 times\n",
      "  '#supremasisipil': 217982 times\n",
      "  '#cabutruutni': 133066 times\n",
      "  '#tolakuutni': 113151 times\n",
      "  '#tolakruukejaksaan': 106603 times\n",
      "  '#cabutuutni': 101564 times\n",
      "  '#gagalkanruutni': 62699 times\n",
      "  '#gagalkanuutni': 42117 times\n",
      "  'tni': 37127 times\n",
      "  'ruu': 27235 times\n",
      "  'ruu tni': 23497 times\n",
      "  '#tolakruukejaksan': 18548 times\n",
      "  '#kembalikantnikebarak': 16008 times\n",
      "  'dwi fun gsi': 12759 times\n",
      "  '#makzulkanprabowogibran': 9164 times\n",
      "  'indonesia': 8280 times\n",
      "  'rakyat': 6692 times\n",
      "  'demo': 6204 times\n",
      "  'uu': 5911 times\n",
      "  'abri': 5148 times\n",
      "  'orang': 5076 times\n",
      "  'negara': 5040 times\n",
      "  'banget': 4683 times\n",
      "  'dwi fun gsi abri': 4643 times\n",
      "  'mah as is wa': 4039 times\n",
      "  'sip il': 4035 times\n",
      "  'dpr': 3928 times\n",
      "  'uu tni': 3818 times\n",
      "  'pem erin tah': 3807 times\n",
      "  'a ksi': 3716 times\n",
      "  'polr i': 3707 times\n",
      "  'demonstra si': 3386 times\n",
      "  'terus': 3289 times\n",
      "  'tetap': 3021 times\n",
      "  'url': 2990 times\n",
      "  'revis i': 2834 times\n",
      "  '#tolakdwifungsitni': 2711 times\n",
      "  'hari': 2640 times\n",
      "  'gel ap': 2631 times\n",
      "  'to lak': 2630 times\n",
      "  'rasa': 2524 times\n",
      "  'polisi': 2503 times\n",
      "  'baru': 2493 times\n",
      "  'teman': 2449 times\n",
      "  'mi liter': 2415 times\n"
     ]
    }
   ],
   "source": [
    "import wordsegment\n",
    "from collections import Counter\n",
    "\n",
    "# --- Load Wordsegment Model (Run this once) ---\n",
    "try:\n",
    "    wordsegment.load()\n",
    "    print(\"Wordsegment model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load wordsegment data: {e}\")\n",
    "    # This is critical, so we can't continue without it.\n",
    "    raise e\n",
    "\n",
    "def clean_and_segment_token(token):\n",
    "    \"\"\"\n",
    "    Cleans a single token string.\n",
    "    \n",
    "    - If it's a hashtag, it's returned as is.\n",
    "    - If it's a jumbled word, it's segmented.\n",
    "    - If it's already a clean n-gram, it's returned as is.\n",
    "    \"\"\"\n",
    "    if not isinstance(token, str):\n",
    "        return None # Handle bad data\n",
    "    \n",
    "    # Rule 1: If it's a hashtag, leave it alone.\n",
    "    if token.startswith(\"#\"):\n",
    "        return token\n",
    "    \n",
    "    # Rule 2: If it's not a hashtag, segment it.\n",
    "    # wordsegment.segment(\"demomahasiswa\") -> [\"demo\", \"mahasiswa\"]\n",
    "    # wordsegment.segment(\"demo mahasiswa\") -> [\"demo\", \"mahasiswa\"]\n",
    "    # wordsegment.segment(\"ruutni\") -> [\"ruu\", \"tni\"]\n",
    "    segmented_words = wordsegment.segment(token)\n",
    "    \n",
    "    return \" \".join(segmented_words)\n",
    "# 2. Apply the cleaning function and re-aggregate counts\n",
    "#    We use a new Counter because \"demomahasiswa\" and \"demo mahasiswa\"\n",
    "#    will now both become \"demo mahasiswa\" and their counts should be combined.\n",
    "final_clean_counts = Counter()\n",
    "\n",
    "print(\"\\nCleaning and re-aggregating final list...\")\n",
    "for token, count in reaggregated_tokens:\n",
    "    # Use the regex from the last step to split multi-hashtag tokens\n",
    "    hashtags_found = re.findall(r\"#\\w+\", token)\n",
    "    \n",
    "    if hashtags_found:\n",
    "        # If the token is '#tolakruu #tolakpolri', add its count to both\n",
    "        for tag in hashtags_found:\n",
    "            final_clean_counts[tag] += count\n",
    "    else:\n",
    "        # Not a hashtag, so clean and segment it\n",
    "        clean_token = clean_and_segment_token(token)\n",
    "        if clean_token:\n",
    "            final_clean_counts[clean_token] += count\n",
    "\n",
    "# 3. Print the final, clean list\n",
    "print(\"\\n--- Final Clean Token List ---\")\n",
    "for token, count in final_clean_counts.most_common(50):\n",
    "    print(f\"  '{token}': {count} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
