{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d01a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ef0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = dataset[\"cleaned_labeled\"]\n",
    "relevant_dataset = cleaned_dataset.filter(lambda row: row[\"related\"] == True)\n",
    "irrelevant_dataset = cleaned_dataset.filter(lambda row: row[\"related\"] == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1e992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_text = relevant_dataset[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4a15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def get_top_n_hashtags(list_of_texts, n=10):\n",
    "    \"\"\"\n",
    "    Extracts all hashtags from a list of text strings, counts them,\n",
    "    and returns the top N most frequent hashtags.\n",
    "\n",
    "    Args:\n",
    "        list_of_texts (list): A list of strings to process.\n",
    "        n (int): The number of top hashtags to return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (hashtag, count) tuples, sorted by count descending.\n",
    "    \"\"\"\n",
    "    all_hashtags = []\n",
    "    \n",
    "    # Regex pattern: finds a '#' followed by one or more\n",
    "    # \"word\" characters (letters, numbers, or underscore).\n",
    "    pattern = r\"#\\w+\"\n",
    "    \n",
    "    for text in list_of_texts:\n",
    "        # Ensure the item is a string before processing\n",
    "        if isinstance(text, str):\n",
    "            # Find all matches and convert to lowercase for consistent counting\n",
    "            found_hashtags = re.findall(pattern, text.lower())\n",
    "            all_hashtags.extend(found_hashtags)\n",
    "            \n",
    "    # Count the frequency of all found hashtags\n",
    "    hashtag_counts = Counter(all_hashtags)\n",
    "    \n",
    "    # Get the top N most common\n",
    "    top_n = hashtag_counts.most_common(n)\n",
    "    \n",
    "    return top_n\n",
    "\n",
    "# 2. Set how many top hashtags you want\n",
    "top_n = 100\n",
    "\n",
    "# 3. Run the function\n",
    "top_hashtags = get_top_n_hashtags(relevant_text, n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef23b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621 total stopwords\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import requests\n",
    "\n",
    "# Sastrawi\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords_id = set(factory.get_stop_words())\n",
    "\n",
    "# English\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "# IndoNLU\n",
    "indonlu_url = \"https://raw.githubusercontent.com/IndoNLP/indonlu/master/utils/stopwordbahasa.txt\"\n",
    "stopwords_indonlu = set(requests.get(indonlu_url).text.split())\n",
    "\n",
    "# IDN stopwords\n",
    "idn_url = \"https://raw.githubusercontent.com/Alir3z4/stop-words/master/indonesian.txt\"\n",
    "stopwords_idn = set(requests.get(idn_url).text.split())\n",
    "\n",
    "# Colloquial extensions (dialect + slang)\n",
    "extra_slang = {\n",
    "    \"ga\",\"gak\",\"nggak\",\"ngga\",\"aja\",\"nih\",\"dong\",\"deh\",\"lah\",\"loh\",\"kok\",\n",
    "    \"kan\",\"nya\",\"ya\",\"emang\",\"tau\",\"yg\",\"pd\",\"trs\",\"pls\",\"plis\",\"thx\",\"makasih\",\n",
    "    \"makasi\",\"terimakasih\",\"bgt\",\"bngt\",\"bener\",\"btw\",\"rt\",\"dm\",\"gw\",\"gue\",\"lu\",\n",
    "    \"loe\",\"lo\",\"gua\",\"ny\",\"nya\",\"jd\",\"jadi\",\"trs\",\"sih\",\"kayak\",\"kek\",\"oke\",\n",
    "    \"ok\",\"bro\",\"sis\",\"min\",\"kalo\",\"kalau\",\"dgn\",\"dengan\",\"bikin\",\"nih\",\"dong\"\n",
    "}\n",
    "\n",
    "# Combine and lowercase normalize\n",
    "combined_stopwords = set(\n",
    "    word.lower() for word in (\n",
    "        stopwords_id |\n",
    "        stopwords_en |\n",
    "        stopwords_indonlu |\n",
    "        stopwords_idn |\n",
    "        extra_slang\n",
    "    )\n",
    ")\n",
    "\n",
    "print(len(combined_stopwords), \"total stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eae0a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordsegment model loaded.\n",
      "Loaded 111949 lines to process.\n",
      "Processing and filtering hashtags in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111949/111949 [10:29<00:00, 177.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating results...\n",
      "\n",
      "Filtered candidate count: 29121\n",
      "Filtered 'meaning' doc sample: 'dan memb uk tik an pertamina ber jalan seperti yang bukan se kedar kembali kan ten tara barak ken apa inst ansi yang mem per boleh kan tidak dengan ran ah masih engg ak men yang ka ten tara beberapa y...'\n",
      "\n",
      "Running KeyBERT on filtered data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 50 Semantically-Ranked (and Filtered) Hashtags ---\n",
      "  - 'negara2an' (Score: 0.4902)\n",
      "  - 'negara' (Score: 0.4331)\n",
      "  - 'debat' (Score: 0.4330)\n",
      "  - 'penjahat2' (Score: 0.4272)\n",
      "  - 'negara2' (Score: 0.4215)\n",
      "  - 'debate' (Score: 0.4155)\n",
      "  - 'pemerintahyg' (Score: 0.4122)\n",
      "  - 'kebangsaan' (Score: 0.4111)\n",
      "  - 'kesewenangwenangan' (Score: 0.4081)\n",
      "  - 'kerana' (Score: 0.4033)\n",
      "  - 'treasonous' (Score: 0.4028)\n",
      "  - 'masyarakat' (Score: 0.4001)\n",
      "  - 'authoritarianism' (Score: 0.3983)\n",
      "  - 'lagipemrintah' (Score: 0.3968)\n",
      "  - 'pemerintahanpeneliti' (Score: 0.3909)\n",
      "  - 'pemerintahnya' (Score: 0.3893)\n",
      "  - 'penyelewengannya' (Score: 0.3876)\n",
      "  - 'ada' (Score: 0.3859)\n",
      "  - 'pokoknyaaw' (Score: 0.3856)\n",
      "  - 'bandit' (Score: 0.3836)\n",
      "  - 'kesewenang2an' (Score: 0.3825)\n",
      "  - 'anarchism' (Score: 0.3816)\n",
      "  - 'menyangkutpautkan' (Score: 0.3780)\n",
      "  - 'menyebabkannya' (Score: 0.3779)\n",
      "  - 'ketidakterbukaan' (Score: 0.3763)\n",
      "  - 'ketidaktahuannya' (Score: 0.3743)\n",
      "  - 'penyebabnya' (Score: 0.3725)\n",
      "  - 'kenapa2' (Score: 0.3720)\n",
      "  - 'sebab' (Score: 0.3704)\n",
      "  - 'liberal' (Score: 0.3704)\n",
      "  - 'musuh2' (Score: 0.3691)\n",
      "  - 'segelap2nya' (Score: 0.3690)\n",
      "  - 'kericuhanmahasiswa' (Score: 0.3684)\n",
      "  - 'egalitarianism' (Score: 0.3682)\n",
      "  - 'country' (Score: 0.3680)\n",
      "  - 'pemerintahmu' (Score: 0.3679)\n",
      "  - 'hypocrite' (Score: 0.3677)\n",
      "  - 'mengkondusifkan' (Score: 0.3670)\n",
      "  - 'nations' (Score: 0.3669)\n",
      "  - 'sebagai' (Score: 0.3656)\n",
      "  - 'diharapkanpuan' (Score: 0.3653)\n",
      "  - 'ngawujudkeun' (Score: 0.3619)\n",
      "  - 'pemerintah2' (Score: 0.3615)\n",
      "  - 'menteri2nya' (Score: 0.3614)\n",
      "  - 'menyelewengkan' (Score: 0.3611)\n",
      "  - 'dikambinghitamkan' (Score: 0.3610)\n",
      "  - 'ngarujudakan' (Score: 0.3603)\n",
      "  - 'inggris' (Score: 0.3596)\n",
      "  - 'bukti2nya' (Score: 0.3594)\n",
      "  - 'rakyat' (Score: 0.3578)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import wordsegment\n",
    "from keybert import KeyBERT\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os # Added for ProcessPoolExecutor\n",
    "\n",
    "# --- 1. Set Global Constants ---\n",
    "MIN_AVG_WORD_LEN = 2.5\n",
    "\n",
    "# --- 2. Define Worker Function (MUST be at top level) ---\n",
    "# This function will be sent to each CPU core.\n",
    "def process_line(text_line):\n",
    "    \"\"\"\n",
    "    Processes a single line of text to find valid, non-noise hashtags\n",
    "    and their segmented \"meaning\".\n",
    "    \"\"\"\n",
    "    local_doc_parts = []\n",
    "    local_candidates = set()\n",
    "    \n",
    "    # Ensure input is a string\n",
    "    if not isinstance(text_line, str):\n",
    "        return local_doc_parts, local_candidates\n",
    "\n",
    "    hashtags = text_line.split()\n",
    "\n",
    "    for tag in hashtags:\n",
    "        # 1. Segment the hashtag\n",
    "        segmented_words = wordsegment.segment(tag)\n",
    "        \n",
    "        if not segmented_words:\n",
    "            continue\n",
    "            \n",
    "        # 2. Calculate average word length\n",
    "        avg_len = sum(len(w) for w in segmented_words) / len(segmented_words)\n",
    "        \n",
    "        # 3. Apply the filter\n",
    "        if avg_len >= MIN_AVG_WORD_LEN:\n",
    "            segmented_sentence = \" \".join(segmented_words)\n",
    "            \n",
    "            # 4. Optional: Language check\n",
    "            try:\n",
    "                lang = detect(segmented_sentence)\n",
    "                if lang not in ['id', 'en']:\n",
    "                    continue\n",
    "            except:\n",
    "                pass \n",
    "                \n",
    "            # If all filters pass, add the data\n",
    "            local_doc_parts.append(segmented_sentence)\n",
    "            local_candidates.add(tag)\n",
    "            \n",
    "    return local_doc_parts, local_candidates\n",
    "\n",
    "# --- 3. Main execution block (REQUIRED for multiprocessing) ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Load Wordsegment Model (once in main process) ---\n",
    "    try:\n",
    "        wordsegment.load()\n",
    "        print(\"Wordsegment model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load wordsegment data: {e}\")\n",
    "        print(\"Please run 'pip install wordsegment' and ensure you have internet.\")\n",
    "        raise e\n",
    "    \n",
    "    print(f\"Loaded {len(relevant_text)} lines to process.\")\n",
    "\n",
    "    # --- 4. Process Data in Parallel ---\n",
    "    doc_for_embedding_parts = []\n",
    "    hashtag_candidates = set()\n",
    "\n",
    "    print(\"Processing and filtering hashtags in parallel...\")\n",
    "    \n",
    "    # Use ProcessPoolExecutor to run CPU-bound tasks in parallel\n",
    "    # It will automatically use all available CPU cores\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        # 'executor.map' runs 'process_line' on each item in 'relevant_text'\n",
    "        # We wrap the map iterator in tqdm to get a single, clean progress bar\n",
    "        results = list(tqdm(executor.map(process_line, relevant_text), total=len(relevant_text)))\n",
    "\n",
    "    # --- 5. Aggregate Results from All Processes ---\n",
    "    print(\"Aggregating results...\")\n",
    "    for doc_parts, candidates in results:\n",
    "        doc_for_embedding_parts.extend(doc_parts)\n",
    "        hashtag_candidates.update(candidates)\n",
    "\n",
    "    # --- 6. Check if We Have Any Data Left ---\n",
    "    doc_for_embedding = \" \".join(doc_for_embedding_parts)\n",
    "    hashtag_candidates_list = list(hashtag_candidates)\n",
    "\n",
    "    if not doc_for_embedding or not hashtag_candidates_list:\n",
    "        print(\"\\nError: After filtering, no valid hashtags or corpus content was found.\")\n",
    "        print(\"Your dataset might be 100% noise, or your MIN_AVG_WORD_LEN is too high.\")\n",
    "    else:\n",
    "        print(f\"\\nFiltered candidate count: {len(hashtag_candidates_list)}\")\n",
    "        print(f\"Filtered 'meaning' doc sample: '{doc_for_embedding[:200]}...'\")\n",
    "\n",
    "        # --- 7. Load KeyBERT and Extract Keywords ---\n",
    "        model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        kw_model = KeyBERT(model=model_name)\n",
    "\n",
    "        print(\"\\nRunning KeyBERT on filtered data...\")\n",
    "        \n",
    "        # **FIX**: Use 'doc=' for a single string, not 'docs='\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            docs=doc_for_embedding,      \n",
    "            candidates=hashtag_candidates_list, \n",
    "            top_n=50\n",
    "        )\n",
    "\n",
    "        # --- 8. View Your (Now Correct) Results ---\n",
    "        print(\"\\n--- Top 50 Semantically-Ranked (and Filtered) Hashtags ---\")\n",
    "        for keyword, score in keywords:\n",
    "            print(f\"  - '{keyword}' (Score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025496a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'other', \"she'll\", 'gue', 'gw', 'mungkin', 'his', 'berapalah', 'sekarang', 'percuma', 'bener', 'me', 'nantinya', 'dapat', 'diantaranya', \"we'll\", 'beginian', 'sudahlah', 'seperti', 'by', 'hasn', 'haven', 'justru', 'sedangkan', 'meski', 'telah', 'them', 'tetapi', 'loh', 'sambil', 'who', 'bagaikan', 'dimana', 'khususnya', \"won't\", 'mustn', 'don', 'sana', 'yang', 'saat', 'bagaimana', \"weren't\", 'such', 'pun', 'bukannya', 'waduh', 'demikian', 'because', 'it', 'terhadapnya', 'dm', 'himself', 'biasa', 'couldn', 'she', 'sela', 'nanti', 'mightn', 'him', 'sebuah', 'sama', 'deh', 'atau', 'manakala', \"didn't\", 'bukanlah', 'sebegitu', 'has', 'kami', 'seraya', 'sepanjang', 'than', 'saatnya', 'dsb', 'they', 't', \"you'd\", 'cuma', 'dengan', \"wouldn't\", 'kemana', 'enggak', \"i've\", 'do', 'pantas', 'makasi', 'no', 'sedang', 'kepada', 'did', 'per', 'tidak', 'bila', 'hendak', 'hendaklah', 'sesaat', 'begitu', 'doesn', 'both', 'dekat', 'itself', 'was', 'on', 'between', 'themselves', 'sebagainya', 'semua', 'terlebih', 'lah', 'jikalau', 'masihkah', 'selain', 'setiap', 'only', 'bagi', 'just', 'mengapa', \"he's\", 'hadn', 'dari', 'meskipun', 'lo', \"shan't\", 'nyaris', 'disinilah', 'nah', 'selamanya', 'mampukah', 'an', 'dini', \"i'm\", 'yg', 'antar', 'weren', 'your', 'dua', 'demikianlah', 've', 'oleh', 'memang', 'apa', 'up', 'walau', 'is', 'pasti', 'of', 'ga', 'yakni', 'but', 'aren', 'while', 'lalu', 'hanya', \"it'll\", 'malah', 'dialah', 'tidaklah', \"they've\", 'bagaimanapun', 'di', 'sekalian', 'sekaligus', 'seringnya', 'been', 'dst', 'begitulah', 'having', 'sepertinya', \"we've\", 'yourself', 'dan', '404:', 'yaitu', 'be', 'rt', 'sesama', 'tanpa', 'siapakah', 'we', 'anu', 'seluruhnya', 'antara', \"we're\", 'sudah', \"that'll\", \"it's\", 'boleh', 'what', 'entahlah', 'nih', 'tadinya', 'haruslah', 'kecuali', 'bahkan', 'bukankah', 'does', 'will', 'loe', 'hal', 'sering', 'seorang', 'didn', 'selagi', 'tapi', 'sesudah', 'isn', 'banyak', 'kiranya', 'sih', 'nor', 'wong', \"he'd\", 'sebelum', 'bilakah', 'ours', 'selaku', 'about', 'serupa', 'sehingga', 'those', 'kamilah', 'terdiri', 'sesuatunya', 'sekalipun', 'siapa', 'sis', 'kenapa', 'kan', 'sebanyak', 'kapan', 'kini', 'o', 'sampai', \"you've\", 'demi', \"doesn't\", \"he'll\", 'nggak', 'saling', 'semuanya', 'once', 'secara', \"should've\", 'seolah', 'ini', \"mightn't\", 'amat', 'pada', 'under', 'sudahkah', 'off', \"needn't\", 'buat', 'ngga', 'aku', 'ada', 'bagaimanakah', 'sedikitnya', 'ma', 'wasn', 'toh', 'more', 'bagai', 'these', 'any', 'apabila', 'll', 'bro', 'against', 'm', 'apakah', 'again', \"haven't\", 'you', 'dia', 'pernah', 'para', \"mustn't\", 'which', 'ataukah', 'jika', 'not', 'wah', 'over', 'berapakah', 'bisa', 'wahai', 'jd', 'sayalah', 'begini', 'kalau', 'dahulu', 'kemudian', 'se', 'seketika', 'where', 'whom', 'seharusnya', 'sepantasnya', 'sesegera', 'being', 'sedemikian', 'down', 'kala', 'apatah', 'seseorang', 'guna', 'adalah', 'he', 'below', 'tentang', 'during', 'bisakah', 'manalagi', 'pd', 'kek', 'lu', 'hampir', 'sebetulnya', 'nya', 'bahwa', 'bermacam', 'semasih', 'then', 'sekitar', 'akan', 'begitupun', 'all', 'that', \"you'll\", 'padahal', 'its', 'semaunya', 'kinilah', 'masing', 'with', 'bngt', 'depan', 'aja', 'can', 'oh', 'untuk', 'i', 'setidaknya', 'mampu', 'dikarenakan', 'dgn', 'kok', 'the', 'sebegini', 'pula', 'sebelumnya', 'begitukah', 'hingga', 'after', 'too', 'lainnya', 'terimakasih', 'suatu', 'their', 'sebenarnya', \"don't\", 'am', 'juga', 'few', 'setelah', \"she's\", 'ibarat', 'adanya', 'were', 'apaan', 'gua', 'agar', 'tiap', 'for', 'karenanya', 'hendaknya', 'inilah', 'kamulah', 'and', 'oke', 'karena', 'serta', 'my', 'ingin', 'her', 'sendirinya', 'namun', \"they'd\", 'apalagi', 'kalian', 'kepadanya', 'tersebutlah', 'trs', 'dulu', 'btw', 'bolehkah', 'sesuatu', 'inginkan', 'kitalah', 'rupanya', 'beginikah', 'tolong', \"hadn't\", 'plis', 'dong', 'bukan', 'jangan', 'merupakan', 'terhadap', 'andalah', 'bersama', 'ke', 'tentunya', \"they're\", 'sementara', 'or', 'sewaktu', 'herself', 'enggaknya', 'biasanya', 'ny', 'in', 'from', 'makanya', 'jadi', 'as', 'into', 'siapapun', 'kapankah', 'our', 'thx', 'gak', 'belum', 'bahwasanya', 'above', 'diri', 'kecil', 'lamanya', 'disini', 'myself', 'selama', 'betulkah', 'berapa', \"shouldn't\", 'lain', 'kalaupun', 'walaupun', 'have', 'how', 'some', 'why', 'dirinya', 'maka', 'wouldn', 'tertentu', 'sebagaimana', 'ourselves', 'min', \"couldn't\", \"you're\", 'maupun', 'kalo', 'sedikit', 'yours', 'tadi', 'a', 'tidakkah', 'needn', 'out', 'tentulah', 'tau', 'kah', 'saja', \"aren't\", 'kapanpun', 'merekalah', 's', \"she'd\", 'bgt', 'own', 'entah', 'adapun', 'sekiranya', 'seluruh', 'sesekali', 'ok', 'padanya', 'pastilah', 'sesudahnya', 'found', 'd', \"we'd\", 'to', 'kamu', 'terlalu', 'sekali', 'tak', 'ya', 'itu', 'shan', 'each', 'tersebut', 'there', 'anda', 'makin', 'sangatlah', 'segala', \"it'd\", 'ialah', 'inginkah', 'beberapa', 'seterusnya', 'if', 'until', 'mungkinkah', 'sangat', 'segalanya', 'shouldn', 'semacam', 'further', 'dulunya', 'yourselves', 'tentu', 'kita', 'y', 'akhirnya', 'mereka', 'daripada', 'sinilah', 'supaya', 'most', 'kembali', \"isn't\", 'at', 'kayak', 'sejenak', 'when', 're', 'lagi', 'ataupun', 'won', 'sini', 'agaknya', 'paling', 'belumlah', 'saya', 'sendiri', 'sepantasnyalah', 'same', 'harusnya', 'sebaliknya', 'through', 'ain', 'semula', 'berapapun', 'agak', 'melalui', 'akulah', 'melainkan', 'lama', 'sebisanya', 'macam', 'olehnya', 'before', 'lagian', 'mau', 'mana', 'here', 'lebih', 'now', 'janganlah', 'amatlah', 'sempat', 'malahan', 'mari', 'dalam', 'sebabnya', 'ketika', \"i'd\", \"wasn't\", 'antaranya', 'sejak', 'sebab', 'so', \"hasn't\", 'pls', 'theirs', 'are', 'very', 'segera', \"they'll\", 'sekitarnya', 'semakin', 'this', 'harus', 'itulah', 'makasih', 'seberapa', 'akankah', 'selalu', 'masih', 'should', 'had', 'sebagai', 'kalaulah', 'beginilah', 'sajalah', 'ia', 'dll', 'bolehlah', 'menurut', 'emang', 'diantara', 'hanyalah', 'hers', 'jangankan', 'bikin', 'doing', 'inikah', \"i'll\", 'itukah'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m top_n = \u001b[32m100\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# 4. Run the function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m top_tokens = \u001b[43mget_top_n_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelevant_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombined_stopwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 5. Print the results\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m most used tokens (words and hashtags):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mget_top_n_tokens\u001b[39m\u001b[34m(list_of_texts, n, stop_words)\u001b[39m\n\u001b[32m     31\u001b[39m vectorizer = CountVectorizer(\n\u001b[32m     32\u001b[39m     token_pattern=token_pattern, \n\u001b[32m     33\u001b[39m     lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     34\u001b[39m     stop_words=stop_words \n\u001b[32m     35\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 1. Fit and transform the text\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m X = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 2. Get the feature names (words and hashtags)\u001b[39;00m\n\u001b[32m     41\u001b[39m tokens = vectorizer.get_feature_names_out()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/sklearn/base.py:1382\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1377\u001b[39m partial_fit_and_fitted = (\n\u001b[32m   1378\u001b[39m     fit_method.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mpartial_fit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[32m   1379\u001b[39m )\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m   1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/sklearn/base.py:436\u001b[39m, in \u001b[36mBaseEstimator._validate_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    429\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[32m    430\u001b[39m \n\u001b[32m    431\u001b[39m \u001b[33;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m \u001b[33;03m    accepted constraints.\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'other', \"she'll\", 'gue', 'gw', 'mungkin', 'his', 'berapalah', 'sekarang', 'percuma', 'bener', 'me', 'nantinya', 'dapat', 'diantaranya', \"we'll\", 'beginian', 'sudahlah', 'seperti', 'by', 'hasn', 'haven', 'justru', 'sedangkan', 'meski', 'telah', 'them', 'tetapi', 'loh', 'sambil', 'who', 'bagaikan', 'dimana', 'khususnya', \"won't\", 'mustn', 'don', 'sana', 'yang', 'saat', 'bagaimana', \"weren't\", 'such', 'pun', 'bukannya', 'waduh', 'demikian', 'because', 'it', 'terhadapnya', 'dm', 'himself', 'biasa', 'couldn', 'she', 'sela', 'nanti', 'mightn', 'him', 'sebuah', 'sama', 'deh', 'atau', 'manakala', \"didn't\", 'bukanlah', 'sebegitu', 'has', 'kami', 'seraya', 'sepanjang', 'than', 'saatnya', 'dsb', 'they', 't', \"you'd\", 'cuma', 'dengan', \"wouldn't\", 'kemana', 'enggak', \"i've\", 'do', 'pantas', 'makasi', 'no', 'sedang', 'kepada', 'did', 'per', 'tidak', 'bila', 'hendak', 'hendaklah', 'sesaat', 'begitu', 'doesn', 'both', 'dekat', 'itself', 'was', 'on', 'between', 'themselves', 'sebagainya', 'semua', 'terlebih', 'lah', 'jikalau', 'masihkah', 'selain', 'setiap', 'only', 'bagi', 'just', 'mengapa', \"he's\", 'hadn', 'dari', 'meskipun', 'lo', \"shan't\", 'nyaris', 'disinilah', 'nah', 'selamanya', 'mampukah', 'an', 'dini', \"i'm\", 'yg', 'antar', 'weren', 'your', 'dua', 'demikianlah', 've', 'oleh', 'memang', 'apa', 'up', 'walau', 'is', 'pasti', 'of', 'ga', 'yakni', 'but', 'aren', 'while', 'lalu', 'hanya', \"it'll\", 'malah', 'dialah', 'tidaklah', \"they've\", 'bagaimanapun', 'di', 'sekalian', 'sekaligus', 'seringnya', 'been', 'dst', 'begitulah', 'having', 'sepertinya', \"we've\", 'yourself', 'dan', '404:', 'yaitu', 'be', 'rt', 'sesama', 'tanpa', 'siapakah', 'we', 'anu', 'seluruhnya', 'antara', \"we're\", 'sudah', \"that'll\", \"it's\", 'boleh', 'what', 'entahlah', 'nih', 'tadinya', 'haruslah', 'kecuali', 'bahkan', 'bukankah', 'does', 'will', 'loe', 'hal', 'sering', 'seorang', 'didn', 'selagi', 'tapi', 'sesudah', 'isn', 'banyak', 'kiranya', 'sih', 'nor', 'wong', \"he'd\", 'sebelum', 'bilakah', 'ours', 'selaku', 'about', 'serupa', 'sehingga', 'those', 'kamilah', 'terdiri', 'sesuatunya', 'sekalipun', 'siapa', 'sis', 'kenapa', 'kan', 'sebanyak', 'kapan', 'kini', 'o', 'sampai', \"you've\", 'demi', \"doesn't\", \"he'll\", 'nggak', 'saling', 'semuanya', 'once', 'secara', \"should've\", 'seolah', 'ini', \"mightn't\", 'amat', 'pada', 'under', 'sudahkah', 'off', \"needn't\", 'buat', 'ngga', 'aku', 'ada', 'bagaimanakah', 'sedikitnya', 'ma', 'wasn', 'toh', 'more', 'bagai', 'these', 'any', 'apabila', 'll', 'bro', 'against', 'm', 'apakah', 'again', \"haven't\", 'you', 'dia', 'pernah', 'para', \"mustn't\", 'which', 'ataukah', 'jika', 'not', 'wah', 'over', 'berapakah', 'bisa', 'wahai', 'jd', 'sayalah', 'begini', 'kalau', 'dahulu', 'kemudian', 'se', 'seketika', 'where', 'whom', 'seharusnya', 'sepantasnya', 'sesegera', 'being', 'sedemikian', 'down', 'kala', 'apatah', 'seseorang', 'guna', 'adalah', 'he', 'below', 'tentang', 'during', 'bisakah', 'manalagi', 'pd', 'kek', 'lu', 'hampir', 'sebetulnya', 'nya', 'bahwa', 'bermacam', 'semasih', 'then', 'sekitar', 'akan', 'begitupun', 'all', 'that', \"you'll\", 'padahal', 'its', 'semaunya', 'kinilah', 'masing', 'with', 'bngt', 'depan', 'aja', 'can', 'oh', 'untuk', 'i', 'setidaknya', 'mampu', 'dikarenakan', 'dgn', 'kok', 'the', 'sebegini', 'pula', 'sebelumnya', 'begitukah', 'hingga', 'after', 'too', 'lainnya', 'terimakasih', 'suatu', 'their', 'sebenarnya', \"don't\", 'am', 'juga', 'few', 'setelah', \"she's\", 'ibarat', 'adanya', 'were', 'apaan', 'gua', 'agar', 'tiap', 'for', 'karenanya', 'hendaknya', 'inilah', 'kamulah', 'and', 'oke', 'karena', 'serta', 'my', 'ingin', 'her', 'sendirinya', 'namun', \"they'd\", 'apalagi', 'kalian', 'kepadanya', 'tersebutlah', 'trs', 'dulu', 'btw', 'bolehkah', 'sesuatu', 'inginkan', 'kitalah', 'rupanya', 'beginikah', 'tolong', \"hadn't\", 'plis', 'dong', 'bukan', 'jangan', 'merupakan', 'terhadap', 'andalah', 'bersama', 'ke', 'tentunya', \"they're\", 'sementara', 'or', 'sewaktu', 'herself', 'enggaknya', 'biasanya', 'ny', 'in', 'from', 'makanya', 'jadi', 'as', 'into', 'siapapun', 'kapankah', 'our', 'thx', 'gak', 'belum', 'bahwasanya', 'above', 'diri', 'kecil', 'lamanya', 'disini', 'myself', 'selama', 'betulkah', 'berapa', \"shouldn't\", 'lain', 'kalaupun', 'walaupun', 'have', 'how', 'some', 'why', 'dirinya', 'maka', 'wouldn', 'tertentu', 'sebagaimana', 'ourselves', 'min', \"couldn't\", \"you're\", 'maupun', 'kalo', 'sedikit', 'yours', 'tadi', 'a', 'tidakkah', 'needn', 'out', 'tentulah', 'tau', 'kah', 'saja', \"aren't\", 'kapanpun', 'merekalah', 's', \"she'd\", 'bgt', 'own', 'entah', 'adapun', 'sekiranya', 'seluruh', 'sesekali', 'ok', 'padanya', 'pastilah', 'sesudahnya', 'found', 'd', \"we'd\", 'to', 'kamu', 'terlalu', 'sekali', 'tak', 'ya', 'itu', 'shan', 'each', 'tersebut', 'there', 'anda', 'makin', 'sangatlah', 'segala', \"it'd\", 'ialah', 'inginkah', 'beberapa', 'seterusnya', 'if', 'until', 'mungkinkah', 'sangat', 'segalanya', 'shouldn', 'semacam', 'further', 'dulunya', 'yourselves', 'tentu', 'kita', 'y', 'akhirnya', 'mereka', 'daripada', 'sinilah', 'supaya', 'most', 'kembali', \"isn't\", 'at', 'kayak', 'sejenak', 'when', 're', 'lagi', 'ataupun', 'won', 'sini', 'agaknya', 'paling', 'belumlah', 'saya', 'sendiri', 'sepantasnyalah', 'same', 'harusnya', 'sebaliknya', 'through', 'ain', 'semula', 'berapapun', 'agak', 'melalui', 'akulah', 'melainkan', 'lama', 'sebisanya', 'macam', 'olehnya', 'before', 'lagian', 'mau', 'mana', 'here', 'lebih', 'now', 'janganlah', 'amatlah', 'sempat', 'malahan', 'mari', 'dalam', 'sebabnya', 'ketika', \"i'd\", \"wasn't\", 'antaranya', 'sejak', 'sebab', 'so', \"hasn't\", 'pls', 'theirs', 'are', 'very', 'segera', \"they'll\", 'sekitarnya', 'semakin', 'this', 'harus', 'itulah', 'makasih', 'seberapa', 'akankah', 'selalu', 'masih', 'should', 'had', 'sebagai', 'kalaulah', 'beginilah', 'sajalah', 'ia', 'dll', 'bolehlah', 'menurut', 'emang', 'diantara', 'hanyalah', 'hers', 'jangankan', 'bikin', 'doing', 'inikah', \"i'll\", 'itukah'} instead."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_tokens(list_of_texts, n=10, stop_words=None):\n",
    "    \"\"\"\n",
    "    Extracts all words and hashtags from a list of text strings, \n",
    "    counts them, and returns the top N most frequent tokens.\n",
    "\n",
    "    Args:\n",
    "        list_of_texts (list): A list of strings to process.\n",
    "        n (int): The number of top tokens to return.\n",
    "        stop_words (list, optional): A list of stopwords to ignore.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (token, count) tuples, sorted by count descending.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This regex pattern finds:\n",
    "    # 1. (#\\w+)   : A hashtag (e.g., #python)\n",
    "    # 2. |         : OR\n",
    "    # 3. (\\b\\w\\w+\\b): A regular word of 2 or more characters (e.g., python)\n",
    "    # The order is important so \"#python\" isn't matched as just \"python\".\n",
    "    token_pattern = r\"(#\\w+|\\b\\w\\w+\\b)\"\n",
    "    \n",
    "    # Filter out any non-string items\n",
    "    valid_texts = [text for text in list_of_texts if isinstance(text, str)]\n",
    "    \n",
    "    if not valid_texts:\n",
    "        return []\n",
    "\n",
    "    # Initialize the vectorizer with the new pattern and stopwords\n",
    "    vectorizer = CountVectorizer(\n",
    "        token_pattern=token_pattern, \n",
    "        lowercase=True,\n",
    "        stop_words=stop_words \n",
    "    )\n",
    "    \n",
    "    # 1. Fit and transform the text\n",
    "    X = vectorizer.fit_transform(valid_texts)\n",
    "    \n",
    "    # 2. Get the feature names (words and hashtags)\n",
    "    tokens = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # 3. Sum the counts for each token\n",
    "    counts = X.sum(axis=0).A1\n",
    "    \n",
    "    # 4. Zip tokens and counts\n",
    "    tag_counts = list(zip(tokens, counts))\n",
    "    \n",
    "    # 5. Sort by count in descending order\n",
    "    sorted_tokens = sorted(tag_counts, key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # 6. Return the top N\n",
    "    return sorted_tokens[:n]\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# 3. Set how many top tokens you want\n",
    "top_n = 100\n",
    "\n",
    "# 4. Run the function\n",
    "top_tokens = get_top_n_tokens(relevant_text, n=top_n, stop_words=list(combined_stopwords))\n",
    "\n",
    "# 5. Print the results\n",
    "print(f\"Top {top_n} most used tokens (words and hashtags):\")\n",
    "for token, count in top_tokens:\n",
    "    print(f\"  {token}: {count} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
