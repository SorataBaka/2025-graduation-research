{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f445e61b",
   "metadata": {},
   "source": [
    "## Upload all of the initially parsed data to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9878ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'out/raw_dump_stage_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m raw_json = json.loads(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mout/raw_dump_stage_1.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.read())\n\u001b[32m     29\u001b[39m clean_json = normalize_mongo_json(raw_json)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mparsed_dump_stage_1.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'out/raw_dump_stage_1.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def normalize_mongo_json(data):\n",
    "    cleaned = []\n",
    "    for item in data:\n",
    "        obj = {}\n",
    "\n",
    "        for k, v in item.items():\n",
    "            if k in [\"_id\", \"__v\", \"created_at\"]:\n",
    "                continue  # drop fields\n",
    "\n",
    "            # unwrap MongoDB extended JSON types\n",
    "            if isinstance(v, dict) and \"$date\" in v:\n",
    "                # ensure it's ISO 8601 without Z\n",
    "                dt = v[\"$date\"]\n",
    "                # remove Z if present\n",
    "                dt = dt.rstrip(\"Z\")\n",
    "                obj[k] = dt\n",
    "            else:\n",
    "                obj[k] = v\n",
    "\n",
    "        cleaned.append(obj)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "raw_json = json.loads(open(\"raw_dump_stage_1.json\").read())\n",
    "clean_json = normalize_mongo_json(raw_json)\n",
    "\n",
    "with open(\"parsed_dump_stage_1.json\", \"w\") as f:\n",
    "    json.dump(clean_json, f, indent=2)\n",
    "\n",
    "raw_json = json.loads(open(\"raw_dump_stage_2.json\").read())\n",
    "clean_json = normalize_mongo_json(raw_json)\n",
    "\n",
    "with open(\"parsed_dump_stage_2.json\", \"w\") as f:\n",
    "    json.dump(clean_json, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6f25e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201583 247820\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "stage_1_dataset = Dataset.from_json(\"parsed_dump_stage_1.json\")\n",
    "stage_2_dataset = Dataset.from_json(\"parsed_dump_stage_2.json\")\n",
    "print(f\"{len(stage_1_dataset)} {len(stage_2_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a60ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6568cb158ebb4873bbbe10a20a14d617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/201583 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3e6bc43b2641ffb35ed791ec31436a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/247820 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "dataset_1 = Dataset.from_json(\"parsed_dump_stage_1.json\")\n",
    "dataset_2 = Dataset.from_json(\"parsed_dump_stage_2.json\")\n",
    "\n",
    "dataset_1 = dataset_1.map(\n",
    "    lambda x: {\n",
    "        \"relevant\": False,   # default boolean\n",
    "        \"sentiment\": 0       # default integer (-1, 0, 1 allowed)\n",
    "    }\n",
    ")\n",
    "dataset_2 = dataset_2.map(\n",
    "    lambda x: {\n",
    "        \"relevant\": False,  # default boolean\n",
    "        \"sentiment\": 0,  # default integer (-1, 0, 1 allowed)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46778bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acef2aa1b6a44709565bd634b2a6a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7a6c0261e74920916c1065bcdd1fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dee1620fae6498a8baaa101a85c41c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd7fbb275984e108340ac41129e9607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fc539ab67a4727be8e943efb30b540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7737c36bef43cfbb984a91f562bf57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1680bb850af44c8cbdbe826d512f40e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f6777f7871486787af81a983ccc0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ac1c4d2d0b4388babcb36021b3a339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tianharjuno/twitter-parse/commit/dbf93f69b6513a82701a40b76a6e91b8a84dc3b6', commit_message='Initial Commit', commit_description='', oid='dbf93f69b6513a82701a40b76a6e91b8a84dc3b6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tianharjuno/twitter-parse', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tianharjuno/twitter-parse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = DatasetDict()\n",
    "dataset_dict[\"source_stage_1\"] = dataset_1\n",
    "dataset_dict[\"source_stage_2\"] = dataset_2\n",
    "dataset_dict.push_to_hub(\"tianharjuno/twitter-parse\", commit_message=\"Initial Commit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129ee3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")\n",
    "raw_ds = ds[\"source_stage_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f768af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kembalikan Tentara ke barak\\n#TolakRUUTNI',\n",
       " 'Masih ga nyangka suamiku tentara #tolakruutni',\n",
       " 'Beberapa zine yg akan saya bawa saat melapak nanti\\n\\n#TolakRUUTNI \\n#SupremasiSipil',\n",
       " 'Sebagai mahasiswa hukum, kami memiliki tanggung jawab akademik untuk mengawal proses pembentukan undang-undang agar memiliki proses partisipasi yg bermakna!\\n\\nberjuang turun ke jalan, bahkan berjuang di ruang sidang, semua sah dilakukan. yg penting #tolakruutni',\n",
       " 'sekali-kali ga nyebelin ye kite #FallingInLoveEra #TolakRUUTNI',\n",
       " 'Presiden kita ini udah KOSONG dari pas zaman debat pilpres. Dan skrg negara kita di pimpin oleh 2 manusia KOSONG bin DONGO \\n\\n#TolakRUUTNI #SupremasiSipil',\n",
       " 'Kalo bisa menteri dan pejabat negara lainnya ikut disandera juga.. wkwkwkkwk\\n\\n#TolakRUUTNI QRIS',\n",
       " 'Great \\nECCO CHI Ses\\nBingx3\\n#shailenzo #zelena #yulioli #grandefratello #zeudiners #ENHYPEN #ホットスポット #Trump #TolakRUUTNI',\n",
       " 'Kalau mau bertani jangan jadi tentara.\\n#TolakRUUTNI',\n",
       " '\"Whenever I lay down to close my eyes… that’s when the nightmares begin. And lately, they’ve been getting worse.\"\\n-Dave Torres\\n\\n#TolakUUTNI #TolakRUUTNI #TolakRevisiUUTNI #PeringatanDarurat #IndonesiaGelap #TolakRUUPolri #TolakRKUHAP',\n",
       " 'Lah ngatur \\nSekarang gua tanya  ngapain? Ngapain lu makan di kampus? Makan di barak aja ransum masih banyak kayanya \\nBerkali-kali diingetin deterrent effectnya buruk masih aja ngeyel nanem padi dan skrng malah main di kampus  \\n#TolakRUUTNI',\n",
       " 'TANGI CAH INDONESIA DIACAK-ACAK PEMERINTAH\\n#TolakRUUTNI #SupremasiSipil',\n",
       " '*hajar kali maksudnya \\n\\n#TolakRUUTNI\\n#TolakRUUPolri',\n",
       " 'Ada yg pernah cukur di sini? Info review\\n\\n#TolakRUUTNI #SupremasiSipil',\n",
       " 'Mahkamah Konstitusi (MK) melarang lembaga pemerintah, institusi, dan korporasi mengadukan laporan dugaan pencemaran nama baik.\\n\\n#TolakRUUTNI\\n#TolakRUUPolri\\n#KawalRKUHAP',\n",
       " 'Siapa berani jadi warga negara yang sadar dan terjaga?\\n\\nSimak ruang dialog politik dan kebudayaan antar warga negara untuk merawat kebugaran demokrasi di https://youtu.be/p-lQkBTu1pw\\n\\n#KeepTalking #EepSaefullohFatah #TolakRUUTNI #TolakDwifungsiABRI #TolakRevisiUUTNI #PeringatanDarurat',\n",
       " 'BJIR INI ADALAH SATU HAL MENDASAR MENGAPA KITA HARUS #TOLAKRUUTNI GUYSSSS',\n",
       " 'and y’all still support ruu tni? #TolakRUUTNI',\n",
       " 'Bagaimana bisa TNI mengajar, mereka apa pernah menjadi seorang mahasiswa, jika ada matkul ekonomi apa mereka hanya mengajar bagaimana KKN di negri ini dilakukan. #TolakRUUTNI baru disuarakan meski tak didengar,tapi implementasi di sah kan mulai dilakukan.',\n",
       " 'Great \\nECCO CHI Ses\\nGate gate gate 4\\n#shailenzo #zelena #yulioli #grandefratello #zeudiners #ENHYPEN #ホットスポット #Trump #TolakRUUTNI',\n",
       " 'Great \\nECCO CHI Ses\\nGate gate gate 1\\n#shailenzo #zelena #yulioli #grandefratello #zeudiners #ENHYPEN #ホットスポット #Trump #TolakRUUTNI',\n",
       " 'Great \\nECCO CHI Ses\\nGate gate gate 2\\n#shailenzo #zelena #yulioli #grandefratello #zeudiners #ENHYPEN #ホットスポット #Trump #TolakRUUTNI',\n",
       " 'Great \\nECCO CHI Ses\\nGate gate gate 3\\n#shailenzo #zelena #yulioli #grandefratello #zeudiners #ENHYPEN #ホットスポット #Trump #TolakRUUTNI',\n",
       " 'Buset sampe bikin spanduk yg anggarannya dilebihin biar bisa ditilep. Ga penting banget\\n#SupremasiSipil\\n#BatalkanRUUTNI\\n#TolakRUUTNI\\n#TolakRUUPolri \\n#TolakDwifungsiABRI\\n#IndonesiaGelap https://x.com/jackjackparrr//jackjackparrr/status/1911770673913204789…',\n",
       " 'setuju ganti presiden \"Dwi fungsi yang sudah berjalan #viral #update #bicarafakta #tolakRUUTNI #fyp #info #pengkhianat #mahasiswaindonesia\"',\n",
       " 'pas ngomong dipenjara pada ketawa. udah gila.\\n\\n#TolakRUUTNI\\n#CabutRUUTNI \\n#BatalkanRUUTNI\\n#SupremasiSipil\\n#TolakRUUPolri\\n#TolakRUUKUHAP',\n",
       " 'soalnya uu tni udah disahkan. kontol lu semua yang mengesahkan.\\n\\n#TolakRUUTNI\\n#CabutRUUTNI \\n#BatalkanRUUTNI\\n#SupremasiSipil\\n#TolakRUUPolri\\n#TolakRUUKUHAP',\n",
       " 'makanya jangan dwifungsi tni dan mau menjadikan kepemimpinan militer  \\n\\n#TolakRUUTNI\\n#CabutRUUTNI \\n#BatalkanRUUTNI\\n#SupremasiSipil\\n#TolakRUUPolri\\n#TolakRUUKUHAP',\n",
       " 'buna walaupun lagi lari harus tetap cantik.\\n\\n#TolakRUUTNI\\n#CabutRUUTNI \\n#BatalkanRUUTNI\\n#SupremasiSipil\\n#TolakRUUPolri\\n#TolakRUUKUHAP',\n",
       " 'kak ijin save dan print yahh #TolakRUUTNI #TolakDwifungsiABRI',\n",
       " 'Jangan lupa kawula muda, mari kita kawal ini\\n#TolakRUUTNI\\n#CabutUUTNI',\n",
       " 'Apalagi dirut pfn nya orang titipan. Ya sudahlah\\n#tolakRUUTNI',\n",
       " 'tanpa ada tokoh kaya anies bilang \"#TolakRUUTNI dan #CabutUUTNI\" udah dianggap demo antek asing.',\n",
       " 'Ga jelas ngentot #TolakRUUTNI',\n",
       " 'Panjang umur perlawanan    \\n#CabutUUTNI #BatalkanRUUTNI #TolakRUUTNI\\n#TolakRevisiUUTNI\\n#TolakDwifungsiABRI\\n#TolakRUUPolri\\n#SupremasiSipil',\n",
       " 'Mengakui kegagalan dan menghentikan langkah sejenak lebih baik daripada memikirkan ego dan pride yang kemudian membuat orang lain menderita dan merugikan negara.\\n\\n#MBG\\n#tolakRUUTNI \\n#SupremasiSipil',\n",
       " 'setuju ganti presiden \"Dwi fungsi yang sudah berjalan #viral #update #bicarafakta #tolakRUUTNI #fyp #info #pengkhianat #mahasiswaindonesia\"',\n",
       " 'KPK kini tidak bisa lagi menangkap direksi dan komisaris BUMN yang melakukan korupsi\\n\\n#TolakRUUTNI\\n#TolakRUUPolri\\n#KawalRKUHAP',\n",
       " 'Amazing Dolphin - At Diani Beach - Mombasa -Kenya - الدلافين الجميلة- في منطقة دياني بيتش - مومباسا - كينيا - #LaCasaDeLosFamososCol #LaCasaDelLosFamososCol #Beckysangels #素のまんま #Beckysangels #notebook0616 #Espressolab #TolakRUUTNI',\n",
       " 'Dislike dan wapres adalah suatu hal yg gabisa dipisahkan. Mark tolong rubuhkan rezim ini \\n\\n#TolakRUUTNI #TolakRUUPolri #cabutruutni',\n",
       " 'teruskan perjuangan bangsa yang saat ini menderita \"Dwi fungsi yang sudah berjalan #viral #update #bicarafakta #tolakRUUTNI #fyp #info #pengkhianat #mahasiswaindonesia\"',\n",
       " 'Wo mana wo katanya demo damai, ini kurang damai apa dah kok malah nurunin parcok \\n\\n#TolakRUUPolri #SupremasiSipil #Civilphobia #BatalkanRUUTNI #KawalRKUHAP #TolakRUUTNI',\n",
       " 'Great \\nECCO CHI Ses\\nBingx2\\n#shailenzo #zelena #yulioli #grandefratello #zeudiners #ENHYPEN #ホットスポット #Trump #TolakRUUTNI',\n",
       " 'Great \\nECCO CHI Ses\\nGate gate gate 1\\n#shailenzo #zelena #yulioli #grandefratello #zeudiners #ENHYPEN #ホットスポット #Trump #TolakRUUTNI',\n",
       " 'nyampe warkop diledek karena cuma nyusul pas laper #KurangKonsisten #I_Love_Indomie_Kornet #ILoveMyFriends #SupremasiSipil #TolakRUUTNI',\n",
       " 'Gaada gunanya, kayak mereka didengerin aja\\n\\nTapi jangan berhenti.\\n#GagalkanRUUTNI\\n#CabutRUUTNI\\n#TolakRUUTNI\\n#TolakRevisiUUTNI\\n#PeringatanDarurat #IndonesiaGelap\\n#TolakDwifungsiABRI #SupremasiSipil\\n#TolakRUUPolri\\n#TolakRUUKejaksaan\\n#KamiBersamaTempo',\n",
       " 'kalo tni yg ini gapapa lah #TolakRUUTNI',\n",
       " 'mau nyisipin tagar #TolakRUUTNI di tweet rbb tp lupa udah disahin',\n",
       " 'YAHHH TERUSSS? Maunya gimanaaaaaaaa sih\\nmau hobi nyanyi lagi #TolakRUUTNI gitu?',\n",
       " 'SIAPA YANG NGIDE MASUKIN ARTI SEMBOYAN TNI JIR #tolakRUUTNI']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds[\"content\"][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159d3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_WS       = re.compile(r'\\s+')\n",
    "_KUTI_CUT = re.compile(r'(?i)kutipan.*$', re.DOTALL)\n",
    "\n",
    "# --- (MODIFIED) ---\n",
    "# Catches \"word\" + \"dari\" + \"domain.com\" -> replaces with \"word\"\n",
    "# Changed \\w+ to \\S+ to include punctuation like '!'\n",
    "_DARI_URL_ATTACHED = re.compile(r'(\\S+)dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I) \n",
    "\n",
    "# Catches \" dari \" + \"domain.com\" -> replaces with empty string\n",
    "_DARI_URL_SPACED = re.compile(r'\\s+dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# --- (NEW) ---\n",
    "# Catches any word ending in \"dari\" (e.g., \"anarko!dari\", \"negaradari\")\n",
    "_DARI_STUCK = re.compile(r'(\\S+)dari\\b', re.I)\n",
    "\n",
    "def cleantext(row: str):\n",
    "    text = row[\"content\"] #type: ignore\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "    text = text.replace(\"tanya grok\", \" \")\n",
    "    text = text.replace(\"grokproductivitypasang\", \" \")\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "    \n",
    "    # Handle standard URLs first\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = text.replace('ini tidak tersedia', ' ')\n",
    "    \n",
    "    text = _MENTION.sub('@USER', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "    text = _KUTI_CUT.sub('', text)\n",
    "\n",
    "    # text = _DARI_URL_ATTACHED.sub(r'\\1', text) \n",
    "    # text = _DARI_URL_SPACED.sub('', text)\n",
    "    # text = _DARI_STUCK.sub(r'\\1', text)\n",
    "    \n",
    "    text = _WS.sub(' ', text).strip()\n",
    "    row[\"content\"] = text #type: ignore\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bb8cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe877155912435287295f500ac69372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/247820 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "raw_ds = raw_ds.map(cleantext)\n",
    "# remove duplicates\n",
    "raw_df = raw_ds.to_pandas()\n",
    "raw_df = raw_df.drop_duplicates(subset=\"content\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "raw_ds_dedup = Dataset.from_pandas(raw_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d210c8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "    num_rows: 195952\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a052cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = raw_ds[\"content\"]\n",
    "\n",
    "# Correct syntax: \"\\n\".join(content_list)\n",
    "# Added encoding=\"utf-8\" to prevent errors with special characters\n",
    "with open(\"reader.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e0bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"cleaned\"] = raw_ds_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71ce7874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba40eef1388742f8a857ecfa70e0154c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f364c538e84517ad67a8a49debfa36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/202 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4702585db54b4ee7b023cd4f72542fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f1ead130814d399e01a1664f4f7f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05be861f300044a0949469431f56602d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0679c5a37b114cab90b756baabe512f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/196 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4421f86e88e4cf7bcf7f5e77fbd4e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8b780e2ffc4c82b78a17759db60a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d48548f6d94efea3af2ee3f0acaf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f26fda21aa54b5884e70500f131c52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dac6f11814b4b06a08652a11a2e3c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tianharjuno/twitter-parse/commit/71d4a3692c4929d47ba0b545f276e8a46adb879e', commit_message='cleaned up text', commit_description='', oid='71d4a3692c4929d47ba0b545f276e8a46adb879e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tianharjuno/twitter-parse', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tianharjuno/twitter-parse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds.push_to_hub(\"tianharjuno/twitter-parse\", commit_message=\"cleaned up text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "591a52ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "    num_rows: 195952\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"cleaned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ab14718",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"cleaned\"]\n",
    "train_ds_split = train_ds.train_test_split(train_size=5000, seed=42)\n",
    "test_ds = train_ds_split[\"train\"]\n",
    "leftover_ds = train_ds_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2535d9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leftover_ds_split = leftover_ds.train_test_split(train_size=20000, seed=42)\n",
    "final_train_ds = leftover_ds_split[\"train\"]\n",
    "final_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97057425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    source_stage_1: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 201583\n",
       "    })\n",
       "    source_stage_2: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 247820\n",
       "    })\n",
       "    cleaned: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 195952\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "086f6213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a779f6a1d2824b099adbcfc23c89547a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad62539f1f05487791b0cff999484da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/202 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c6cf1213264a3d8697e2d57cb49472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd265e8a04e49a6b3cb3f557578e70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdc35fe5e1c4fabb4880554c696ad1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3436f176294cc08350ded9a9c8ef74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/196 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0ceb55f6d747ea8368fe1c47116327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38474d0a4ff84ed48cd6936bf8cedf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99f0e36c32c4ef9814beec3ce13a8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed9c8945fa648cd9a53245a19bdc83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a2a844aeb1483494a359c2b24a262b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tianharjuno/twitter-parse/commit/2fee1a3524a90baca823df9239ba213cd9e1f1ef', commit_message='Created a test dataset', commit_description='', oid='2fee1a3524a90baca823df9239ba213cd9e1f1ef', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tianharjuno/twitter-parse', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tianharjuno/twitter-parse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"test\"] = test_ds\n",
    "ds[\"train\"] = final_train_ds\n",
    "\n",
    "ds.push_to_hub(\"tianharjuno/twitter-parse\", commit_message=\"Created a test dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
