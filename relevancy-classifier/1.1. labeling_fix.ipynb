{
 "cells": [
  {
   "cell_type": "code",
   "id": "f915361b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T08:42:29.670012Z",
     "start_time": "2025-11-19T08:42:26.475435Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"/data/cache/\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T08:43:11.696907Z",
     "start_time": "2025-11-19T08:43:09.652408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import ClassLabel\n",
    "class_labels = ClassLabel(2, [\"Irrelevant\", \"Relevant\"])\n",
    "train_ds = dataset[\"cleaned\"].train_test_split(train_size=80000)[\"train\"].select_columns([\"content\", \"relevant\"])\n",
    "train_ds = train_ds.rename_column(\"relevant\", \"label\")\n",
    "train_ds = train_ds.cast_column(\"label\", class_labels)\n",
    "\n",
    "train_ds"
   ],
   "id": "6b49c2e5c12f26b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/80000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2efff94e04dd4cd2acef0c75a087363f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'label'],\n",
       "    num_rows: 80000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T08:43:18.229251Z",
     "start_time": "2025-11-19T08:43:18.178562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "# We are NOT importing replace_word_elongation anymore\n",
    "from indoNLP.preprocessing import emoji_to_words\n",
    "\n",
    "def clean_tweet_for_nusabert(row):\n",
    "    text = row['content']\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # 3. Remove mentions and RT\n",
    "    # This regex is safe and does not affect 'ruu tni'\n",
    "    text = re.sub(r'rt @\\S+|@\\S+', '', text)\n",
    "\n",
    "    # 4. Remove hashtags (keep the word)\n",
    "    text = re.sub(r'#(\\S+)', r'\\1', text)\n",
    "\n",
    "    # 5. Convert emojis to words (Preserves sentiment)\n",
    "    text = emoji_to_words(text)\n",
    "\n",
    "    # 6. Normalize word elongation (CUSTOM, SAFER REGEX)\n",
    "    # This replaces 3 or more repeated chars (e.g., 'bangeeet' -> 'banget')\n",
    "    # It will NOT affect 'uu' or 'ruu', fixing your bug.\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # 7. Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    row[\"content\"] = text\n",
    "    return row"
   ],
   "id": "601f8af29dfe0e0d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T08:43:29.418873Z",
     "start_time": "2025-11-19T08:43:19.998227Z"
    }
   },
   "cell_type": "code",
   "source": "sentence_train_ds = train_ds.map(clean_tweet_for_nusabert, num_proc=30)",
   "id": "b3c953307b9e5c31",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=30):   0%|          | 0/80000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a2f2b33d3ee4405a83fd96d404f9d6f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T08:46:14.441091Z",
     "start_time": "2025-11-19T08:43:38.255130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_transformer = SentenceTransformer(\"LazarusNLP/all-nusabert-large-v4\",\n",
    "                                           cache_folder=\"/data/cache/\",\n",
    "                                           device=\"cuda\",\n",
    "                                           )\n",
    "embeddings = sentence_transformer.encode(sentence_train_ds[\"content\"], show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True, device=\"cuda\", batch_size=256)"
   ],
   "id": "35de7688c725880a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5247864e717445aea35eca40b7defffd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:17:37.500269Z",
     "start_time": "2025-11-19T08:46:32.853461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dream_pipeline import DreamCluster\n",
    "clusterer = DreamCluster(\"stability\")\n",
    "clusterer.fit(embeddings)"
   ],
   "id": "58705535a899d6b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding intrinsic dimension with TwoNN\n",
      "Found intrinsic dimension: 19\n",
      "Running first stage reduction\n",
      "Running second stage reduction\n",
      "Tuning HDBSCAN with mode stability\n",
      "  -> Using 'stability' mode (Bayesian Optimization on relative_validity)\n",
      "Transforming full dataset with trained reducers...\n",
      "Fitting final clusterer on full reduced dataset...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.int64(179), np.int64(67))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:19:54.389496Z",
     "start_time": "2025-11-19T09:19:44.836719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_ds = dataset[\"test\"]\n",
    "test_embeddings = sentence_transformer.encode(test_ds[\"content\"], show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True, device=\"cuda\")"
   ],
   "id": "4976c1f25e9f47b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c417c50188f0437fbec70008dda2f385"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:20:49.522238Z",
     "start_time": "2025-11-19T09:19:55.488950Z"
    }
   },
   "cell_type": "code",
   "source": "train_labels, train_probabilities, reduced_embeddings = clusterer.predict(test_embeddings)",
   "id": "a98806a191e914a3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:39:53.594344Z",
     "start_time": "2025-11-19T09:39:46.726542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tianharjuno/ruu-tni-relevancy-classification-p1\", cache_dir=\"/data/cache/\", num_labels=len(class_labels.names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tianharjuno/ruu-tni-relevancy-classification-p1\", cache_dir=\"/data/cache/\")\n",
    "model.to(device)\n",
    "trainer_arguments = TrainingArguments(\n",
    "    per_device_eval_batch_size=512\n",
    ")\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer, args=trainer_arguments)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"content\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "encoded_test_ds = test_ds.map(tokenize, batched=True, batch_size=256)\n",
    "predictions = trainer.predict(encoded_test_ds)\n",
    "predicted_class_ids = predictions.predictions.argmax(axis=1)"
   ],
   "id": "ec6710cedd66d3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "ac1558228cea5ac2110d38b189dd87b2"
     }
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:04:18.915057Z",
     "start_time": "2025-11-19T11:04:18.895008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, Any, Tuple, List\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# NOTE: This solution assumes compute_metrics is available and properly defined\n",
    "# in your environment.\n",
    "def compute_metrics(class_names):\n",
    "    num_classes = len(class_names)\n",
    "    def callback(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "        if isinstance(labels, torch.Tensor):\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average=\"macro\", zero_division=0\n",
    "        )\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        p_cls, r_cls, f1_cls, support_cls = precision_recall_fscore_support(\n",
    "            labels,\n",
    "            preds,\n",
    "            average=None,\n",
    "            zero_division=0,\n",
    "            labels=list(range(num_classes)),\n",
    "        )\n",
    "        metrics = {\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": macro_f1,\n",
    "            \"macro_precision\": macro_p,\n",
    "            \"macro_recall\": macro_r,\n",
    "        }\n",
    "        for idx, name in enumerate(class_names):\n",
    "            metrics[f\"{name}_precision\"] = p_cls[idx]  # type: ignore\n",
    "            metrics[f\"{name}_recall\"] = r_cls[idx]  # type: ignore\n",
    "            metrics[f\"{name}_f1\"] = f1_cls[idx]  # type: ignore\n",
    "            metrics[f\"{name}_support\"] = int(support_cls[idx])  # type: ignore\n",
    "        return metrics\n",
    "    return callback\n",
    "def calculate_cluster_metrics(\n",
    "    eval_predictions: Tuple[Any, Any],\n",
    "    cluster_assignments: np.ndarray,\n",
    "    class_names: List[str],\n",
    "    cluster_prefix: str = \"cluster\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Separates the evaluation predictions by cluster and calculates performance\n",
    "    metrics for the model on each cluster subset.\n",
    "\n",
    "    Args:\n",
    "        eval_predictions: A tuple (logits, labels) from the trainer's predict/evaluate step.\n",
    "        cluster_assignments: A 1D numpy array of cluster indices.\n",
    "        class_names: List of class names (e.g., [\"0\", \"1\", \"2\"]).\n",
    "        cluster_prefix: Prefix for the metric keys.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing combined metrics for all clusters.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_predictions\n",
    "    combined_cluster_metrics: Dict[str, Any] = {}\n",
    "\n",
    "    ## ðŸ’¥ CRITICAL FIX: Ensure logits and labels are NumPy arrays for boolean indexing.\n",
    "    # Logits conversion\n",
    "    if isinstance(logits, torch.Tensor):\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "    elif isinstance(logits, list):\n",
    "        logits = np.array(logits)\n",
    "\n",
    "    # Labels conversion\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(labels, list):\n",
    "        labels = np.array(labels)\n",
    "\n",
    "    # Check if cluster_assignments is a numpy array (it should be)\n",
    "    if not isinstance(cluster_assignments, np.ndarray):\n",
    "        cluster_assignments = np.array(cluster_assignments)\n",
    "\n",
    "    unique_clusters = np.unique(cluster_assignments)\n",
    "\n",
    "    # Get the inner callback from the user's compute_metrics function\n",
    "    metrics_callback = compute_metrics(class_names)\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        # Create a boolean mask for the current cluster\n",
    "        mask = cluster_assignments == cluster_id\n",
    "\n",
    "        # Filter the logits and true labels for the current cluster\n",
    "        cluster_logits = logits[mask]\n",
    "        cluster_labels = labels[mask]\n",
    "        cluster_support = len(cluster_labels)\n",
    "\n",
    "        if cluster_support == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate metrics for the current cluster\n",
    "        cluster_eval_preds = (cluster_logits, cluster_labels)\n",
    "        cluster_metrics = metrics_callback(cluster_eval_preds)\n",
    "\n",
    "        # Add cluster size/support\n",
    "        cluster_metrics[\"support\"] = cluster_support\n",
    "\n",
    "        # Rename and combine metrics with a clear prefix\n",
    "        metric_prefix = f\"{cluster_prefix}_{cluster_id}\"\n",
    "        for key, value in cluster_metrics.items():\n",
    "            combined_cluster_metrics[f\"{metric_prefix}_{key}\"] = value\n",
    "\n",
    "    return combined_cluster_metrics"
   ],
   "id": "8727aaccb58ad1c2",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:04:20.005109Z",
     "start_time": "2025-11-19T11:04:19.994584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_logits = predictions.predictions # Assuming the first element is logits\n",
    "test_labels = test_ds[\"relevant\"]  # Assuming the second element is true labels\n",
    "eval_preds = (test_logits, test_labels)"
   ],
   "id": "6622d89faed09c5b",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:04:22.150018Z",
     "start_time": "2025-11-19T11:04:21.957991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Single Reduction\n",
    "import numpy as np\n",
    "single_reduction_metrics = calculate_cluster_metrics(\n",
    "    eval_predictions=eval_preds,\n",
    "    cluster_assignments=train_labels,\n",
    "    class_names=class_labels.names,\n",
    ")\n",
    "for key, value in single_reduction_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ],
   "id": "20b664d5b0fb12bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cluster_-1_accuracy: 0.8424\n",
      "  cluster_-1_macro_f1: 0.8341\n",
      "  cluster_-1_macro_precision: 0.8308\n",
      "  cluster_-1_macro_recall: 0.8385\n",
      "  cluster_-1_Irrelevant_precision: 0.7727\n",
      "  cluster_-1_Irrelevant_recall: 0.8226\n",
      "  cluster_-1_Irrelevant_f1: 0.7969\n",
      "  cluster_-1_Irrelevant_support: 62.0000\n",
      "  cluster_-1_Relevant_precision: 0.8889\n",
      "  cluster_-1_Relevant_recall: 0.8544\n",
      "  cluster_-1_Relevant_f1: 0.8713\n",
      "  cluster_-1_Relevant_support: 103.0000\n",
      "  cluster_-1_support: 165.0000\n",
      "  cluster_0_accuracy: 1.0000\n",
      "  cluster_0_macro_f1: 1.0000\n",
      "  cluster_0_macro_precision: 1.0000\n",
      "  cluster_0_macro_recall: 1.0000\n",
      "  cluster_0_Irrelevant_precision: 0.0000\n",
      "  cluster_0_Irrelevant_recall: 0.0000\n",
      "  cluster_0_Irrelevant_f1: 0.0000\n",
      "  cluster_0_Irrelevant_support: 0.0000\n",
      "  cluster_0_Relevant_precision: 1.0000\n",
      "  cluster_0_Relevant_recall: 1.0000\n",
      "  cluster_0_Relevant_f1: 1.0000\n",
      "  cluster_0_Relevant_support: 13.0000\n",
      "  cluster_0_support: 13.0000\n",
      "  cluster_1_accuracy: 0.8548\n",
      "  cluster_1_macro_f1: 0.8021\n",
      "  cluster_1_macro_precision: 0.8375\n",
      "  cluster_1_macro_recall: 0.7810\n",
      "  cluster_1_Irrelevant_precision: 0.8077\n",
      "  cluster_1_Irrelevant_recall: 0.6176\n",
      "  cluster_1_Irrelevant_f1: 0.7000\n",
      "  cluster_1_Irrelevant_support: 34.0000\n",
      "  cluster_1_Relevant_precision: 0.8673\n",
      "  cluster_1_Relevant_recall: 0.9444\n",
      "  cluster_1_Relevant_f1: 0.9043\n",
      "  cluster_1_Relevant_support: 90.0000\n",
      "  cluster_1_support: 124.0000\n",
      "  cluster_2_accuracy: 1.0000\n",
      "  cluster_2_macro_f1: 1.0000\n",
      "  cluster_2_macro_precision: 1.0000\n",
      "  cluster_2_macro_recall: 1.0000\n",
      "  cluster_2_Irrelevant_precision: 0.0000\n",
      "  cluster_2_Irrelevant_recall: 0.0000\n",
      "  cluster_2_Irrelevant_f1: 0.0000\n",
      "  cluster_2_Irrelevant_support: 0.0000\n",
      "  cluster_2_Relevant_precision: 1.0000\n",
      "  cluster_2_Relevant_recall: 1.0000\n",
      "  cluster_2_Relevant_f1: 1.0000\n",
      "  cluster_2_Relevant_support: 20.0000\n",
      "  cluster_2_support: 20.0000\n",
      "  cluster_3_accuracy: 0.6786\n",
      "  cluster_3_macro_f1: 0.6748\n",
      "  cluster_3_macro_precision: 0.6771\n",
      "  cluster_3_macro_recall: 0.6744\n",
      "  cluster_3_Irrelevant_precision: 0.6667\n",
      "  cluster_3_Irrelevant_recall: 0.6154\n",
      "  cluster_3_Irrelevant_f1: 0.6400\n",
      "  cluster_3_Irrelevant_support: 13.0000\n",
      "  cluster_3_Relevant_precision: 0.6875\n",
      "  cluster_3_Relevant_recall: 0.7333\n",
      "  cluster_3_Relevant_f1: 0.7097\n",
      "  cluster_3_Relevant_support: 15.0000\n",
      "  cluster_3_support: 28.0000\n",
      "  cluster_4_accuracy: 1.0000\n",
      "  cluster_4_macro_f1: 1.0000\n",
      "  cluster_4_macro_precision: 1.0000\n",
      "  cluster_4_macro_recall: 1.0000\n",
      "  cluster_4_Irrelevant_precision: 1.0000\n",
      "  cluster_4_Irrelevant_recall: 1.0000\n",
      "  cluster_4_Irrelevant_f1: 1.0000\n",
      "  cluster_4_Irrelevant_support: 1.0000\n",
      "  cluster_4_Relevant_precision: 1.0000\n",
      "  cluster_4_Relevant_recall: 1.0000\n",
      "  cluster_4_Relevant_f1: 1.0000\n",
      "  cluster_4_Relevant_support: 106.0000\n",
      "  cluster_4_support: 107.0000\n",
      "  cluster_5_accuracy: 0.9323\n",
      "  cluster_5_macro_f1: 0.8357\n",
      "  cluster_5_macro_precision: 0.8455\n",
      "  cluster_5_macro_recall: 0.8267\n",
      "  cluster_5_Irrelevant_precision: 0.7333\n",
      "  cluster_5_Irrelevant_recall: 0.6875\n",
      "  cluster_5_Irrelevant_f1: 0.7097\n",
      "  cluster_5_Irrelevant_support: 16.0000\n",
      "  cluster_5_Relevant_precision: 0.9576\n",
      "  cluster_5_Relevant_recall: 0.9658\n",
      "  cluster_5_Relevant_f1: 0.9617\n",
      "  cluster_5_Relevant_support: 117.0000\n",
      "  cluster_5_support: 133.0000\n",
      "  cluster_6_accuracy: 0.9843\n",
      "  cluster_6_macro_f1: 0.4960\n",
      "  cluster_6_macro_precision: 0.4921\n",
      "  cluster_6_macro_recall: 0.5000\n",
      "  cluster_6_Irrelevant_precision: 0.9843\n",
      "  cluster_6_Irrelevant_recall: 1.0000\n",
      "  cluster_6_Irrelevant_f1: 0.9921\n",
      "  cluster_6_Irrelevant_support: 188.0000\n",
      "  cluster_6_Relevant_precision: 0.0000\n",
      "  cluster_6_Relevant_recall: 0.0000\n",
      "  cluster_6_Relevant_f1: 0.0000\n",
      "  cluster_6_Relevant_support: 3.0000\n",
      "  cluster_6_support: 191.0000\n",
      "  cluster_7_accuracy: 1.0000\n",
      "  cluster_7_macro_f1: 1.0000\n",
      "  cluster_7_macro_precision: 1.0000\n",
      "  cluster_7_macro_recall: 1.0000\n",
      "  cluster_7_Irrelevant_precision: 0.0000\n",
      "  cluster_7_Irrelevant_recall: 0.0000\n",
      "  cluster_7_Irrelevant_f1: 0.0000\n",
      "  cluster_7_Irrelevant_support: 0.0000\n",
      "  cluster_7_Relevant_precision: 1.0000\n",
      "  cluster_7_Relevant_recall: 1.0000\n",
      "  cluster_7_Relevant_f1: 1.0000\n",
      "  cluster_7_Relevant_support: 13.0000\n",
      "  cluster_7_support: 13.0000\n",
      "  cluster_8_accuracy: 0.9853\n",
      "  cluster_8_macro_f1: 0.7463\n",
      "  cluster_8_macro_precision: 0.8688\n",
      "  cluster_8_macro_recall: 0.6862\n",
      "  cluster_8_Irrelevant_precision: 0.7500\n",
      "  cluster_8_Irrelevant_recall: 0.3750\n",
      "  cluster_8_Irrelevant_f1: 0.5000\n",
      "  cluster_8_Irrelevant_support: 16.0000\n",
      "  cluster_8_Relevant_precision: 0.9876\n",
      "  cluster_8_Relevant_recall: 0.9975\n",
      "  cluster_8_Relevant_f1: 0.9925\n",
      "  cluster_8_Relevant_support: 798.0000\n",
      "  cluster_8_support: 814.0000\n",
      "  cluster_9_accuracy: 1.0000\n",
      "  cluster_9_macro_f1: 1.0000\n",
      "  cluster_9_macro_precision: 1.0000\n",
      "  cluster_9_macro_recall: 1.0000\n",
      "  cluster_9_Irrelevant_precision: 1.0000\n",
      "  cluster_9_Irrelevant_recall: 1.0000\n",
      "  cluster_9_Irrelevant_f1: 1.0000\n",
      "  cluster_9_Irrelevant_support: 10.0000\n",
      "  cluster_9_Relevant_precision: 0.0000\n",
      "  cluster_9_Relevant_recall: 0.0000\n",
      "  cluster_9_Relevant_f1: 0.0000\n",
      "  cluster_9_Relevant_support: 0.0000\n",
      "  cluster_9_support: 10.0000\n",
      "  cluster_10_accuracy: 0.8923\n",
      "  cluster_10_macro_f1: 0.5822\n",
      "  cluster_10_macro_precision: 0.9453\n",
      "  cluster_10_macro_recall: 0.5625\n",
      "  cluster_10_Irrelevant_precision: 0.8906\n",
      "  cluster_10_Irrelevant_recall: 1.0000\n",
      "  cluster_10_Irrelevant_f1: 0.9421\n",
      "  cluster_10_Irrelevant_support: 57.0000\n",
      "  cluster_10_Relevant_precision: 1.0000\n",
      "  cluster_10_Relevant_recall: 0.1250\n",
      "  cluster_10_Relevant_f1: 0.2222\n",
      "  cluster_10_Relevant_support: 8.0000\n",
      "  cluster_10_support: 65.0000\n",
      "  cluster_11_accuracy: 0.8889\n",
      "  cluster_11_macro_f1: 0.4706\n",
      "  cluster_11_macro_precision: 0.5000\n",
      "  cluster_11_macro_recall: 0.4444\n",
      "  cluster_11_Irrelevant_precision: 0.0000\n",
      "  cluster_11_Irrelevant_recall: 0.0000\n",
      "  cluster_11_Irrelevant_f1: 0.0000\n",
      "  cluster_11_Irrelevant_support: 0.0000\n",
      "  cluster_11_Relevant_precision: 1.0000\n",
      "  cluster_11_Relevant_recall: 0.8889\n",
      "  cluster_11_Relevant_f1: 0.9412\n",
      "  cluster_11_Relevant_support: 18.0000\n",
      "  cluster_11_support: 18.0000\n",
      "  cluster_12_accuracy: 1.0000\n",
      "  cluster_12_macro_f1: 1.0000\n",
      "  cluster_12_macro_precision: 1.0000\n",
      "  cluster_12_macro_recall: 1.0000\n",
      "  cluster_12_Irrelevant_precision: 1.0000\n",
      "  cluster_12_Irrelevant_recall: 1.0000\n",
      "  cluster_12_Irrelevant_f1: 1.0000\n",
      "  cluster_12_Irrelevant_support: 38.0000\n",
      "  cluster_12_Relevant_precision: 1.0000\n",
      "  cluster_12_Relevant_recall: 1.0000\n",
      "  cluster_12_Relevant_f1: 1.0000\n",
      "  cluster_12_Relevant_support: 3.0000\n",
      "  cluster_12_support: 41.0000\n",
      "  cluster_13_accuracy: 0.7812\n",
      "  cluster_13_macro_f1: 0.4386\n",
      "  cluster_13_macro_precision: 0.4032\n",
      "  cluster_13_macro_recall: 0.4808\n",
      "  cluster_13_Irrelevant_precision: 0.8065\n",
      "  cluster_13_Irrelevant_recall: 0.9615\n",
      "  cluster_13_Irrelevant_f1: 0.8772\n",
      "  cluster_13_Irrelevant_support: 26.0000\n",
      "  cluster_13_Relevant_precision: 0.0000\n",
      "  cluster_13_Relevant_recall: 0.0000\n",
      "  cluster_13_Relevant_f1: 0.0000\n",
      "  cluster_13_Relevant_support: 6.0000\n",
      "  cluster_13_support: 32.0000\n",
      "  cluster_14_accuracy: 0.7000\n",
      "  cluster_14_macro_f1: 0.4118\n",
      "  cluster_14_macro_precision: 0.3500\n",
      "  cluster_14_macro_recall: 0.5000\n",
      "  cluster_14_Irrelevant_precision: 0.7000\n",
      "  cluster_14_Irrelevant_recall: 1.0000\n",
      "  cluster_14_Irrelevant_f1: 0.8235\n",
      "  cluster_14_Irrelevant_support: 7.0000\n",
      "  cluster_14_Relevant_precision: 0.0000\n",
      "  cluster_14_Relevant_recall: 0.0000\n",
      "  cluster_14_Relevant_f1: 0.0000\n",
      "  cluster_14_Relevant_support: 3.0000\n",
      "  cluster_14_support: 10.0000\n",
      "  cluster_15_accuracy: 0.9091\n",
      "  cluster_15_macro_f1: 0.9060\n",
      "  cluster_15_macro_precision: 0.9286\n",
      "  cluster_15_macro_recall: 0.9000\n",
      "  cluster_15_Irrelevant_precision: 0.8571\n",
      "  cluster_15_Irrelevant_recall: 1.0000\n",
      "  cluster_15_Irrelevant_f1: 0.9231\n",
      "  cluster_15_Irrelevant_support: 6.0000\n",
      "  cluster_15_Relevant_precision: 1.0000\n",
      "  cluster_15_Relevant_recall: 0.8000\n",
      "  cluster_15_Relevant_f1: 0.8889\n",
      "  cluster_15_Relevant_support: 5.0000\n",
      "  cluster_15_support: 11.0000\n",
      "  cluster_16_accuracy: 0.7200\n",
      "  cluster_16_macro_f1: 0.6667\n",
      "  cluster_16_macro_precision: 0.7018\n",
      "  cluster_16_macro_recall: 0.6597\n",
      "  cluster_16_Irrelevant_precision: 0.7368\n",
      "  cluster_16_Irrelevant_recall: 0.8750\n",
      "  cluster_16_Irrelevant_f1: 0.8000\n",
      "  cluster_16_Irrelevant_support: 16.0000\n",
      "  cluster_16_Relevant_precision: 0.6667\n",
      "  cluster_16_Relevant_recall: 0.4444\n",
      "  cluster_16_Relevant_f1: 0.5333\n",
      "  cluster_16_Relevant_support: 9.0000\n",
      "  cluster_16_support: 25.0000\n",
      "  cluster_17_accuracy: 0.8864\n",
      "  cluster_17_macro_f1: 0.6906\n",
      "  cluster_17_macro_precision: 0.7125\n",
      "  cluster_17_macro_recall: 0.6744\n",
      "  cluster_17_Irrelevant_precision: 0.9250\n",
      "  cluster_17_Irrelevant_recall: 0.9487\n",
      "  cluster_17_Irrelevant_f1: 0.9367\n",
      "  cluster_17_Irrelevant_support: 39.0000\n",
      "  cluster_17_Relevant_precision: 0.5000\n",
      "  cluster_17_Relevant_recall: 0.4000\n",
      "  cluster_17_Relevant_f1: 0.4444\n",
      "  cluster_17_Relevant_support: 5.0000\n",
      "  cluster_17_support: 44.0000\n",
      "  cluster_18_accuracy: 0.8889\n",
      "  cluster_18_macro_f1: 0.8615\n",
      "  cluster_18_macro_precision: 0.9286\n",
      "  cluster_18_macro_recall: 0.8333\n",
      "  cluster_18_Irrelevant_precision: 1.0000\n",
      "  cluster_18_Irrelevant_recall: 0.6667\n",
      "  cluster_18_Irrelevant_f1: 0.8000\n",
      "  cluster_18_Irrelevant_support: 9.0000\n",
      "  cluster_18_Relevant_precision: 0.8571\n",
      "  cluster_18_Relevant_recall: 1.0000\n",
      "  cluster_18_Relevant_f1: 0.9231\n",
      "  cluster_18_Relevant_support: 18.0000\n",
      "  cluster_18_support: 27.0000\n",
      "  cluster_19_accuracy: 0.9429\n",
      "  cluster_19_macro_f1: 0.4853\n",
      "  cluster_19_macro_precision: 0.4714\n",
      "  cluster_19_macro_recall: 0.5000\n",
      "  cluster_19_Irrelevant_precision: 0.0000\n",
      "  cluster_19_Irrelevant_recall: 0.0000\n",
      "  cluster_19_Irrelevant_f1: 0.0000\n",
      "  cluster_19_Irrelevant_support: 2.0000\n",
      "  cluster_19_Relevant_precision: 0.9429\n",
      "  cluster_19_Relevant_recall: 1.0000\n",
      "  cluster_19_Relevant_f1: 0.9706\n",
      "  cluster_19_Relevant_support: 33.0000\n",
      "  cluster_19_support: 35.0000\n",
      "  cluster_20_accuracy: 0.9000\n",
      "  cluster_20_macro_f1: 0.4737\n",
      "  cluster_20_macro_precision: 0.4737\n",
      "  cluster_20_macro_recall: 0.4737\n",
      "  cluster_20_Irrelevant_precision: 0.0000\n",
      "  cluster_20_Irrelevant_recall: 0.0000\n",
      "  cluster_20_Irrelevant_f1: 0.0000\n",
      "  cluster_20_Irrelevant_support: 1.0000\n",
      "  cluster_20_Relevant_precision: 0.9474\n",
      "  cluster_20_Relevant_recall: 0.9474\n",
      "  cluster_20_Relevant_f1: 0.9474\n",
      "  cluster_20_Relevant_support: 19.0000\n",
      "  cluster_20_support: 20.0000\n",
      "  cluster_21_accuracy: 0.9211\n",
      "  cluster_21_macro_f1: 0.4795\n",
      "  cluster_21_macro_precision: 0.4605\n",
      "  cluster_21_macro_recall: 0.5000\n",
      "  cluster_21_Irrelevant_precision: 0.0000\n",
      "  cluster_21_Irrelevant_recall: 0.0000\n",
      "  cluster_21_Irrelevant_f1: 0.0000\n",
      "  cluster_21_Irrelevant_support: 3.0000\n",
      "  cluster_21_Relevant_precision: 0.9211\n",
      "  cluster_21_Relevant_recall: 1.0000\n",
      "  cluster_21_Relevant_f1: 0.9589\n",
      "  cluster_21_Relevant_support: 35.0000\n",
      "  cluster_21_support: 38.0000\n",
      "  cluster_22_accuracy: 0.8860\n",
      "  cluster_22_macro_f1: 0.6789\n",
      "  cluster_22_macro_precision: 0.8362\n",
      "  cluster_22_macro_recall: 0.6373\n",
      "  cluster_22_Irrelevant_precision: 0.8919\n",
      "  cluster_22_Irrelevant_recall: 0.9864\n",
      "  cluster_22_Irrelevant_f1: 0.9368\n",
      "  cluster_22_Irrelevant_support: 661.0000\n",
      "  cluster_22_Relevant_precision: 0.7805\n",
      "  cluster_22_Relevant_recall: 0.2883\n",
      "  cluster_22_Relevant_f1: 0.4211\n",
      "  cluster_22_Relevant_support: 111.0000\n",
      "  cluster_22_support: 772.0000\n",
      "  cluster_23_accuracy: 0.9863\n",
      "  cluster_23_macro_f1: 0.4966\n",
      "  cluster_23_macro_precision: 0.4932\n",
      "  cluster_23_macro_recall: 0.5000\n",
      "  cluster_23_Irrelevant_precision: 0.9863\n",
      "  cluster_23_Irrelevant_recall: 1.0000\n",
      "  cluster_23_Irrelevant_f1: 0.9931\n",
      "  cluster_23_Irrelevant_support: 72.0000\n",
      "  cluster_23_Relevant_precision: 0.0000\n",
      "  cluster_23_Relevant_recall: 0.0000\n",
      "  cluster_23_Relevant_f1: 0.0000\n",
      "  cluster_23_Relevant_support: 1.0000\n",
      "  cluster_23_support: 73.0000\n",
      "  cluster_24_accuracy: 0.7407\n",
      "  cluster_24_macro_f1: 0.6956\n",
      "  cluster_24_macro_precision: 0.7229\n",
      "  cluster_24_macro_recall: 0.6868\n",
      "  cluster_24_Irrelevant_precision: 0.7600\n",
      "  cluster_24_Irrelevant_recall: 0.8736\n",
      "  cluster_24_Irrelevant_f1: 0.8128\n",
      "  cluster_24_Irrelevant_support: 261.0000\n",
      "  cluster_24_Relevant_precision: 0.6857\n",
      "  cluster_24_Relevant_recall: 0.5000\n",
      "  cluster_24_Relevant_f1: 0.5783\n",
      "  cluster_24_Relevant_support: 144.0000\n",
      "  cluster_24_support: 405.0000\n",
      "  cluster_25_accuracy: 0.9853\n",
      "  cluster_25_macro_f1: 0.4963\n",
      "  cluster_25_macro_precision: 0.4926\n",
      "  cluster_25_macro_recall: 0.5000\n",
      "  cluster_25_Irrelevant_precision: 0.0000\n",
      "  cluster_25_Irrelevant_recall: 0.0000\n",
      "  cluster_25_Irrelevant_f1: 0.0000\n",
      "  cluster_25_Irrelevant_support: 1.0000\n",
      "  cluster_25_Relevant_precision: 0.9853\n",
      "  cluster_25_Relevant_recall: 1.0000\n",
      "  cluster_25_Relevant_f1: 0.9926\n",
      "  cluster_25_Relevant_support: 67.0000\n",
      "  cluster_25_support: 68.0000\n",
      "  cluster_26_accuracy: 0.9221\n",
      "  cluster_26_macro_f1: 0.6045\n",
      "  cluster_26_macro_precision: 0.9605\n",
      "  cluster_26_macro_recall: 0.5714\n",
      "  cluster_26_Irrelevant_precision: 0.9211\n",
      "  cluster_26_Irrelevant_recall: 1.0000\n",
      "  cluster_26_Irrelevant_f1: 0.9589\n",
      "  cluster_26_Irrelevant_support: 70.0000\n",
      "  cluster_26_Relevant_precision: 1.0000\n",
      "  cluster_26_Relevant_recall: 0.1429\n",
      "  cluster_26_Relevant_f1: 0.2500\n",
      "  cluster_26_Relevant_support: 7.0000\n",
      "  cluster_26_support: 77.0000\n",
      "  cluster_27_accuracy: 0.8780\n",
      "  cluster_27_macro_f1: 0.8339\n",
      "  cluster_27_macro_precision: 0.8909\n",
      "  cluster_27_macro_recall: 0.8049\n",
      "  cluster_27_Irrelevant_precision: 0.8696\n",
      "  cluster_27_Irrelevant_recall: 0.9756\n",
      "  cluster_27_Irrelevant_f1: 0.9195\n",
      "  cluster_27_Irrelevant_support: 205.0000\n",
      "  cluster_27_Relevant_precision: 0.9123\n",
      "  cluster_27_Relevant_recall: 0.6341\n",
      "  cluster_27_Relevant_f1: 0.7482\n",
      "  cluster_27_Relevant_support: 82.0000\n",
      "  cluster_27_support: 287.0000\n",
      "  cluster_28_accuracy: 0.9312\n",
      "  cluster_28_macro_f1: 0.6542\n",
      "  cluster_28_macro_precision: 0.7497\n",
      "  cluster_28_macro_recall: 0.6172\n",
      "  cluster_28_Irrelevant_precision: 0.5556\n",
      "  cluster_28_Irrelevant_recall: 0.2500\n",
      "  cluster_28_Irrelevant_f1: 0.3448\n",
      "  cluster_28_Irrelevant_support: 20.0000\n",
      "  cluster_28_Relevant_precision: 0.9438\n",
      "  cluster_28_Relevant_recall: 0.9844\n",
      "  cluster_28_Relevant_f1: 0.9637\n",
      "  cluster_28_Relevant_support: 256.0000\n",
      "  cluster_28_support: 276.0000\n",
      "  cluster_29_accuracy: 1.0000\n",
      "  cluster_29_macro_f1: 1.0000\n",
      "  cluster_29_macro_precision: 1.0000\n",
      "  cluster_29_macro_recall: 1.0000\n",
      "  cluster_29_Irrelevant_precision: 1.0000\n",
      "  cluster_29_Irrelevant_recall: 1.0000\n",
      "  cluster_29_Irrelevant_f1: 1.0000\n",
      "  cluster_29_Irrelevant_support: 1.0000\n",
      "  cluster_29_Relevant_precision: 1.0000\n",
      "  cluster_29_Relevant_recall: 1.0000\n",
      "  cluster_29_Relevant_f1: 1.0000\n",
      "  cluster_29_Relevant_support: 6.0000\n",
      "  cluster_29_support: 7.0000\n",
      "  cluster_30_accuracy: 0.9187\n",
      "  cluster_30_macro_f1: 0.6526\n",
      "  cluster_30_macro_precision: 0.7460\n",
      "  cluster_30_macro_recall: 0.6173\n",
      "  cluster_30_Irrelevant_precision: 0.5588\n",
      "  cluster_30_Irrelevant_recall: 0.2533\n",
      "  cluster_30_Irrelevant_f1: 0.3486\n",
      "  cluster_30_Irrelevant_support: 75.0000\n",
      "  cluster_30_Relevant_precision: 0.9333\n",
      "  cluster_30_Relevant_recall: 0.9812\n",
      "  cluster_30_Relevant_f1: 0.9566\n",
      "  cluster_30_Relevant_support: 798.0000\n",
      "  cluster_30_support: 873.0000\n",
      "  cluster_31_accuracy: 0.9466\n",
      "  cluster_31_macro_f1: 0.6679\n",
      "  cluster_31_macro_precision: 0.6841\n",
      "  cluster_31_macro_recall: 0.6547\n",
      "  cluster_31_Irrelevant_precision: 0.4000\n",
      "  cluster_31_Irrelevant_recall: 0.3333\n",
      "  cluster_31_Irrelevant_f1: 0.3636\n",
      "  cluster_31_Irrelevant_support: 6.0000\n",
      "  cluster_31_Relevant_precision: 0.9683\n",
      "  cluster_31_Relevant_recall: 0.9760\n",
      "  cluster_31_Relevant_f1: 0.9721\n",
      "  cluster_31_Relevant_support: 125.0000\n",
      "  cluster_31_support: 131.0000\n",
      "  cluster_32_accuracy: 0.8085\n",
      "  cluster_32_macro_f1: 0.4471\n",
      "  cluster_32_macro_precision: 0.4222\n",
      "  cluster_32_macro_recall: 0.4750\n",
      "  cluster_32_Irrelevant_precision: 0.0000\n",
      "  cluster_32_Irrelevant_recall: 0.0000\n",
      "  cluster_32_Irrelevant_f1: 0.0000\n",
      "  cluster_32_Irrelevant_support: 7.0000\n",
      "  cluster_32_Relevant_precision: 0.8444\n",
      "  cluster_32_Relevant_recall: 0.9500\n",
      "  cluster_32_Relevant_f1: 0.8941\n",
      "  cluster_32_Relevant_support: 40.0000\n",
      "  cluster_32_support: 47.0000\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:54:36.979011Z",
     "start_time": "2025-11-19T10:54:36.953861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Any, Tuple, List\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- Helper Function for Mode Calculation ---\n",
    "\n",
    "def _get_mode(preds: np.ndarray) -> Tuple[Any, int]:\n",
    "    \"\"\"Calculates the mode (most frequent label) and its count.\"\"\"\n",
    "    if len(preds) == 0:\n",
    "        return None, 0\n",
    "    counts = np.bincount(preds)\n",
    "    mode_label = np.argmax(counts)\n",
    "    mode_count = counts[mode_label]\n",
    "    return int(mode_label), int(mode_count)\n",
    "\n",
    "# --- Main Diagnostic Function ---\n",
    "\n",
    "def analyze_cluster_predictions_and_suggest_corrections(\n",
    "    dataset: Dataset,\n",
    "    cluster_labels: np.ndarray,\n",
    "    model_predictions: np.ndarray,\n",
    ") -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Analyzes predictions and suggests clusters most likely to contain labeling errors\n",
    "    due to systematic misclassification (semantic bias).\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The Hugging Face Dataset object (must contain a 'relevant' column).\n",
    "        cluster_labels (np.ndarray): 1D array of cluster IDs.\n",
    "        model_predictions (np.ndarray): 1D array of predicted class IDs.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List]: A dictionary containing the top clusters suggested for review.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Input Validation and Conversion\n",
    "    if len(dataset) != len(cluster_labels) or len(dataset) != len(model_predictions):\n",
    "        print(\"Error: Input array lengths must match the dataset length.\")\n",
    "        return {\"suggestions\": []}\n",
    "\n",
    "    cluster_labels = np.asarray(cluster_labels)\n",
    "    model_predictions = np.asarray(model_predictions)\n",
    "    true_labels = np.asarray(dataset[\"relevant\"]).astype(int)\n",
    "\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    review_candidates = []\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CLUSTER PREDICTION BREAKDOWN BY TRUE LABEL\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        current_cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        cluster_true_labels = true_labels[current_cluster_indices]\n",
    "        cluster_preds = model_predictions[current_cluster_indices]\n",
    "        cluster_support = len(cluster_true_labels)\n",
    "\n",
    "        if cluster_support == 0:\n",
    "            continue\n",
    "\n",
    "        # 2. Split by True Label and Determine Majority/Minority\n",
    "        rel_mask = (cluster_true_labels == 1)\n",
    "        irrel_mask = (cluster_true_labels == 0)\n",
    "\n",
    "        rel_count = np.sum(rel_mask)\n",
    "        irrel_count = np.sum(irrel_mask)\n",
    "\n",
    "        if rel_count >= irrel_count:\n",
    "            majority_true_label = 1\n",
    "            minority_true_label = 0\n",
    "            majority_preds = cluster_preds[rel_mask]\n",
    "            minority_preds = cluster_preds[irrel_mask]\n",
    "        else:\n",
    "            majority_true_label = 0\n",
    "            minority_true_label = 1\n",
    "            majority_preds = cluster_preds[irrel_mask]\n",
    "            minority_preds = cluster_preds[rel_mask]\n",
    "\n",
    "        # Get modes for logging\n",
    "        maj_mode_label, maj_mode_count = _get_mode(majority_preds)\n",
    "        min_mode_label, min_mode_count = _get_mode(minority_preds)\n",
    "\n",
    "        # 3. Log Diagnostic Output\n",
    "        print(f\"\\nCLUSTER {cluster_id}, TOTAL SAMPLES: {cluster_support}\")\n",
    "        print(f\"\\tTRUE MAJORITY (Label {majority_true_label}) COUNT: {len(majority_preds)}\")\n",
    "        if len(majority_preds) > 0:\n",
    "            print(f\"\\t\\tPREDICTED MODE: {maj_mode_label} (Ratio: {maj_mode_count / len(majority_preds):.2f})\")\n",
    "\n",
    "        print(f\"\\tTRUE MINORITY (Label {minority_true_label}) COUNT: {len(minority_preds)}\")\n",
    "        if len(minority_preds) > 0:\n",
    "            print(f\"\\t\\tPREDICTED MODE: {min_mode_label} (Ratio: {min_mode_count / len(minority_preds):.2f})\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        # 4. --- Calculate Misclassification Severity Score (MSS) ---\n",
    "\n",
    "        MIN_MINOR_COUNT = 5 # Require at least 5 minority samples for meaningful calculation\n",
    "        if len(minority_preds) < MIN_MINOR_COUNT:\n",
    "            continue\n",
    "\n",
    "        # The core bias check: how many minority samples were misclassified as the majority label?\n",
    "        misclassified_as_majority = np.sum(minority_preds == majority_true_label)\n",
    "        misclassification_ratio = misclassified_as_majority / len(minority_preds)\n",
    "\n",
    "        # Misclassification Severity Score (MSS): Weights the ratio by the square root of the count\n",
    "        # This penalizes high misclassification on larger minority groups.\n",
    "        mss_score = misclassification_ratio * np.sqrt(len(minority_preds))\n",
    "\n",
    "        if mss_score > 0:\n",
    "            review_candidates.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'mss_score': mss_score,\n",
    "                'misclassification_ratio': misclassification_ratio,\n",
    "                'minority_count': len(minority_preds),\n",
    "                'minority_true_label': minority_true_label,\n",
    "                'majority_true_label': majority_true_label,\n",
    "            })\n",
    "\n",
    "    # 5. Suggest Top Candidates\n",
    "    review_candidates.sort(key=lambda x: x['mss_score'], reverse=True)\n",
    "    top_suggestions = review_candidates[:5]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TOP 5 CLUSTERS SUGGESTED FOR LABELING REVIEW (Highest MSS Score)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if not top_suggestions:\n",
    "        print(\"No clusters met the minimum criteria for high-priority review.\")\n",
    "\n",
    "    for i, item in enumerate(top_suggestions):\n",
    "        print(f\"RANK {i+1}: CLUSTER {item['cluster_id']}\")\n",
    "        print(f\"  - Minority Label: {item['minority_true_label']} (Count: {item['minority_count']})\")\n",
    "        print(f\"  - Majority Label: {item['majority_true_label']}\")\n",
    "        print(f\"  - Misclassification Ratio (Minority predicted as Majority): {item['misclassification_ratio']:.2f}\")\n",
    "        print(f\"  - Severity Score (MSS): {item['mss_score']:.2f}\")\n",
    "        print(\"  => Action: Extract the minority samples (True Label 1 or 0) for this cluster and manually check their labels.\")\n",
    "\n",
    "    return {\"suggestions\": top_suggestions}"
   ],
   "id": "3bfd9b775a1df8ae",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:01:02.440309Z",
     "start_time": "2025-11-19T11:01:02.418401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "suggestions = analyze_cluster_predictions_and_suggest_corrections(\n",
    "    test_ds,\n",
    "    train_labels,\n",
    "    predicted_class_ids\n",
    ")\n"
   ],
   "id": "ee8c2d25ad5ab8b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLUSTER PREDICTION BREAKDOWN BY TRUE LABEL\n",
      "======================================================================\n",
      "\n",
      "CLUSTER -1, TOTAL SAMPLES: 165\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 103\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.85)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 62\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.82)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 0, TOTAL SAMPLES: 13\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 13\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 0\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 1, TOTAL SAMPLES: 124\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 90\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.94)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 34\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.62)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 2, TOTAL SAMPLES: 20\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 20\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 0\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 3, TOTAL SAMPLES: 28\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 15\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.73)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 13\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.62)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 4, TOTAL SAMPLES: 107\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 106\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 1\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 5, TOTAL SAMPLES: 133\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 117\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.97)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 16\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.69)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 6, TOTAL SAMPLES: 191\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 188\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 3\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 7, TOTAL SAMPLES: 13\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 13\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 0\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 8, TOTAL SAMPLES: 814\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 798\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 16\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.62)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 9, TOTAL SAMPLES: 10\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 10\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 0\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 10, TOTAL SAMPLES: 65\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 57\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 8\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.88)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 11, TOTAL SAMPLES: 18\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 18\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.89)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 0\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 12, TOTAL SAMPLES: 41\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 38\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 3\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 13, TOTAL SAMPLES: 32\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 26\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.96)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 6\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 14, TOTAL SAMPLES: 10\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 7\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 3\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 15, TOTAL SAMPLES: 11\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 6\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 5\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.80)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 16, TOTAL SAMPLES: 25\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 16\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.88)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 9\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.56)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 17, TOTAL SAMPLES: 44\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 39\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.95)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 5\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.60)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 18, TOTAL SAMPLES: 27\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 18\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 9\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.67)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 19, TOTAL SAMPLES: 35\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 33\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 2\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 20, TOTAL SAMPLES: 20\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 19\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.95)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 1\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 21, TOTAL SAMPLES: 38\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 35\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 3\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 22, TOTAL SAMPLES: 772\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 661\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.99)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 111\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.71)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 23, TOTAL SAMPLES: 73\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 72\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 1\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 24, TOTAL SAMPLES: 405\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 261\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.87)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 144\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.50)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 25, TOTAL SAMPLES: 68\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 67\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 1\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 26, TOTAL SAMPLES: 77\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 70\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 7\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.86)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 27, TOTAL SAMPLES: 287\n",
      "\tTRUE MAJORITY (Label 0) COUNT: 205\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 0.98)\n",
      "\tTRUE MINORITY (Label 1) COUNT: 82\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.63)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 28, TOTAL SAMPLES: 276\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 256\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.98)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 20\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.75)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 29, TOTAL SAMPLES: 7\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 6\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 1\n",
      "\t\tPREDICTED MODE: 0 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 30, TOTAL SAMPLES: 873\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 798\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.98)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 75\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.75)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 31, TOTAL SAMPLES: 131\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 125\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.98)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 6\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.67)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CLUSTER 32, TOTAL SAMPLES: 47\n",
      "\tTRUE MAJORITY (Label 1) COUNT: 40\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 0.95)\n",
      "\tTRUE MINORITY (Label 0) COUNT: 7\n",
      "\t\tPREDICTED MODE: 1 (Ratio: 1.00)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "TOP 5 CLUSTERS SUGGESTED FOR LABELING REVIEW (Highest MSS Score)\n",
      "======================================================================\n",
      "RANK 1: CLUSTER 22\n",
      "  - Minority Label: 1 (Count: 111)\n",
      "  - Majority Label: 0\n",
      "  - Misclassification Ratio (Minority predicted as Majority): 0.71\n",
      "  - Severity Score (MSS): 7.50\n",
      "  => Action: Extract the minority samples (True Label 1 or 0) for this cluster and manually check their labels.\n",
      "RANK 2: CLUSTER 30\n",
      "  - Minority Label: 0 (Count: 75)\n",
      "  - Majority Label: 1\n",
      "  - Misclassification Ratio (Minority predicted as Majority): 0.75\n",
      "  - Severity Score (MSS): 6.47\n",
      "  => Action: Extract the minority samples (True Label 1 or 0) for this cluster and manually check their labels.\n",
      "RANK 3: CLUSTER 24\n",
      "  - Minority Label: 1 (Count: 144)\n",
      "  - Majority Label: 0\n",
      "  - Misclassification Ratio (Minority predicted as Majority): 0.50\n",
      "  - Severity Score (MSS): 6.00\n",
      "  => Action: Extract the minority samples (True Label 1 or 0) for this cluster and manually check their labels.\n",
      "RANK 4: CLUSTER 28\n",
      "  - Minority Label: 0 (Count: 20)\n",
      "  - Majority Label: 1\n",
      "  - Misclassification Ratio (Minority predicted as Majority): 0.75\n",
      "  - Severity Score (MSS): 3.35\n",
      "  => Action: Extract the minority samples (True Label 1 or 0) for this cluster and manually check their labels.\n",
      "RANK 5: CLUSTER 27\n",
      "  - Minority Label: 1 (Count: 82)\n",
      "  - Majority Label: 0\n",
      "  - Misclassification Ratio (Minority predicted as Majority): 0.37\n",
      "  - Severity Score (MSS): 3.31\n",
      "  => Action: Extract the minority samples (True Label 1 or 0) for this cluster and manually check their labels.\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:02:11.279642Z",
     "start_time": "2025-11-19T11:02:11.272508Z"
    }
   },
   "cell_type": "code",
   "source": "suggestions[\"suggestions\"]",
   "id": "69f6bf6b1d7d1d5c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cluster_id': np.int32(22),\n",
       "  'mss_score': np.float64(7.498348166444742),\n",
       "  'misclassification_ratio': np.float64(0.7117117117117117),\n",
       "  'minority_count': 111,\n",
       "  'minority_true_label': 1,\n",
       "  'majority_true_label': 0},\n",
       " {'cluster_id': np.int32(30),\n",
       "  'mss_score': np.float64(6.46632301492381),\n",
       "  'misclassification_ratio': np.float64(0.7466666666666667),\n",
       "  'minority_count': 75,\n",
       "  'minority_true_label': 0,\n",
       "  'majority_true_label': 1},\n",
       " {'cluster_id': np.int32(24),\n",
       "  'mss_score': np.float64(6.0),\n",
       "  'misclassification_ratio': np.float64(0.5),\n",
       "  'minority_count': 144,\n",
       "  'minority_true_label': 1,\n",
       "  'majority_true_label': 0},\n",
       " {'cluster_id': np.int32(28),\n",
       "  'mss_score': np.float64(3.3541019662496847),\n",
       "  'misclassification_ratio': np.float64(0.75),\n",
       "  'minority_count': 20,\n",
       "  'minority_true_label': 0,\n",
       "  'majority_true_label': 1},\n",
       " {'cluster_id': np.int32(27),\n",
       "  'mss_score': np.float64(3.3129457822453965),\n",
       "  'misclassification_ratio': np.float64(0.36585365853658536),\n",
       "  'minority_count': 82,\n",
       "  'minority_true_label': 1,\n",
       "  'majority_true_label': 0}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:56:07.459859Z",
     "start_time": "2025-11-19T09:56:07.449831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "# from datasets import Dataset # Uncomment if your test_ds is a Hugging Face Dataset\n",
    "\n",
    "def prepare_data_for_suite(dataset, cluster_labels, cluster_probabilities, output_filename=\"data_for_suite.json\"):\n",
    "    \"\"\"\n",
    "    Combines the dataset, cluster labels, and probabilities into a single\n",
    "    JSON file and prints its content to the console.\n",
    "\n",
    "    Args:\n",
    "        dataset: Your test_ds (either a Pandas DataFrame or Hugging Face Dataset).\n",
    "        cluster_labels: The NumPy array of cluster assignments.\n",
    "        cluster_probabilities: The NumPy array of cluster probabilities.\n",
    "        output_filename: The name of the JSON file to create.\n",
    "    \"\"\"\n",
    "    print(\"Starting data preparation...\")\n",
    "\n",
    "    # --- Convert to Pandas DataFrame ---\n",
    "    try:\n",
    "        df = dataset.to_pandas()\n",
    "        print(f\"Successfully converted dataset to Pandas. Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting dataset to Pandas: {e}\")\n",
    "        print(\"Please ensure 'dataset' is a Pandas DataFrame or Hugging Face Dataset.\")\n",
    "        return\n",
    "\n",
    "    # --- Add cluster and probability data ---\n",
    "    if len(df) != len(cluster_labels) or len(df) != len(cluster_probabilities):\n",
    "        print(\"Error: Dataset length does not match cluster labels or probabilities length.\")\n",
    "        print(f\"Dataset: {len(df)}, Clusters: {len(cluster_labels)}, Probs: {len(cluster_probabilities)}\")\n",
    "        return\n",
    "\n",
    "    df['cluster'] = cluster_labels\n",
    "    df['probability'] = cluster_probabilities\n",
    "\n",
    "    # --- Add a unique ID and status columns for the app ---\n",
    "    df['id'] = df.index\n",
    "    df['original_label'] = df['relevant'] # Preserve the Llama-generated label\n",
    "    df['status'] = 'uncorrected' # 'uncorrected' or 'corrected'\n",
    "\n",
    "    # --- Reorder columns for clarity in the app ---\n",
    "    try:\n",
    "        cols = ['id', 'cluster', 'probability', 'content', 'relevant', 'original_label', 'status']\n",
    "        other_cols = [col for col in df.columns if col not in cols]\n",
    "        final_cols = cols + other_cols\n",
    "        df = df[final_cols]\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: The dataset must contain 'content' and 'label' columns. Found: {df.columns}\")\n",
    "        return\n",
    "\n",
    "    # --- Save to JSON on the kernel's filesystem ---\n",
    "    json_data = []\n",
    "    try:\n",
    "        # We must use .to_dict('records') to ensure JSON compatibility\n",
    "        json_data = df.to_dict('records')\n",
    "\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, indent=2)\n",
    "\n",
    "        print(f\"\\nSuccess! Data prepared and saved to kernel as '{output_filename}'.\")\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON file: {e}\")\n",
    "        return"
   ],
   "id": "a5f15203bd6b11b9",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:12:39.686509Z",
     "start_time": "2025-11-19T10:12:39.472595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_ds = test_ds.cast_column(\"relevant\", class_labels)\n",
    "prepare_data_for_suite(test_ds, train_labels, train_probabilities, output_filename=\"/data/data_for_suite.json\", )"
   ],
   "id": "cd574b67473dcb6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0734611d083a4c549248b91a24f4b69e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preparation...\n",
      "Successfully converted dataset to Pandas. Shape: (5000, 10)\n",
      "\n",
      "Success! Data prepared and saved to kernel as '/data/data_for_suite.json'.\n",
      "Total records: 5000\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "!pwd",
   "id": "110cf72e8f43a10e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:02:01.649873Z",
     "start_time": "2025-11-19T15:01:58.861720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "corrected_dataset = Dataset.from_json(\"data_corrected.json\")\n",
    "original_dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")\n",
    "test_ds = original_dataset[\"test\"]\n",
    "\n",
    "# Assuming corrected_dataset and test_ds are loaded:\n",
    "\n",
    "# --- 2. Create the Efficient Lookup Dictionary ---\n",
    "# Filter for only the rows that were actually corrected/changed\n",
    "# NOTE: corrected_dataset['relevant'] is likely an integer (0 or 1) here.\n",
    "corrected_only = corrected_dataset.filter(lambda x: x['relevant'] != x.get(\"original_label\", -1))\n",
    "selected_columns = corrected_only.select_columns([\"tweet_id\", \"relevant\"])\n",
    "\n",
    "# Convert the small correction dataset into a fast dictionary lookup: {tweet_id: new_label_as_INT}\n",
    "print(\"Creating fast lookup dictionary for corrected data...\")\n",
    "corrected_data_dict = {\n",
    "    str(tweet_id): label for tweet_id, label in zip(selected_columns[\"tweet_id\"], selected_columns[\"relevant\"])\n",
    "}\n",
    "print(f\"Lookup dictionary created with {len(corrected_data_dict)} corrections.\")\n",
    "\n",
    "\n",
    "# --- 3. Efficient Mapping Function (FIXED) ---\n",
    "def fix_ds_optimized(row):\n",
    "    \"\"\"\n",
    "    Updates the 'relevant' label only if the tweet_id exists in the\n",
    "    global corrected_data_dict, ensuring the value is cast to a boolean\n",
    "    to match the original dataset feature type.\n",
    "    \"\"\"\n",
    "    tweet_id = row[\"tweet_id\"]\n",
    "\n",
    "    # Check for the ID in the dictionary (O(1) operation)\n",
    "    if tweet_id in corrected_data_dict:\n",
    "        new_label_int = corrected_data_dict[tweet_id]\n",
    "\n",
    "        # ðŸ’¥ FIX: Explicitly cast the integer label (0 or 1) to a boolean (False or True)\n",
    "        # to match the expected feature type of the original dataset column.\n",
    "        row[\"relevant\"] = bool(new_label_int)\n",
    "\n",
    "    # If the ID is not found, the original row['relevant'] is kept\n",
    "    return row\n",
    "\n",
    "# --- 4. Apply Correction ---\n",
    "print(\"Applying label corrections to the test dataset...\")\n",
    "test_ds_corrected = test_ds.map(fix_ds_optimized)\n",
    "\n",
    "print(\"Correction complete.\")"
   ],
   "id": "7a956a17489be73e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fast lookup dictionary for corrected data...\n",
      "Lookup dictionary created with 323 corrections.\n",
      "Applying label corrections to the test dataset...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71c26a7e2bce496c91936059092139ca"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction complete.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:03:02.445963Z",
     "start_time": "2025-11-19T15:02:49.849330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "original_dataset[\"test\"] = test_ds_corrected\n",
    "original_dataset.push_to_hub(\"tianharjuno/twitter-parse\", commit_message=\"Fixed some labeling with clustering\")"
   ],
   "id": "bd3e0f2a1361df2b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47104c5cf0c74b26ac74d984f75d57dc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/202 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f6fe966274245c28278398d24a32bdd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf55b59364674176b476c419631034fb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfd2cf3c55484fe9af45ae32f0959d83"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a97e8cb1fe044c78a9c60035bac18728"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/196 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec15eb887323485980a1e1c372b097f4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be58b05563e74052b839ac66a180d3d5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b3741f904824419a5f767571d5483f0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cadb4e4d7ff440338238ff6876b39bed"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7a1d2b85c844beab6f8c417d76ff733"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2b0adb124e54e1b89bb3264a28b33f4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "061a49b2b093499abad1775f443bd330"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tianharjuno/twitter-parse/commit/a2652e2a9217b279e00a6826e86b65d0d2c7e9e0', commit_message='Fixed some labeling with clustering', commit_description='', oid='a2652e2a9217b279e00a6826e86b65d0d2c7e9e0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tianharjuno/twitter-parse', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tianharjuno/twitter-parse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
