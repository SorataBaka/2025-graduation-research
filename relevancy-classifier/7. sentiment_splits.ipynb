{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:21:17.179966Z",
     "start_time": "2025-11-19T15:21:17.128445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_WS       = re.compile(r'\\s+')\n",
    "_KUTI_CUT = re.compile(r'(?i)kutipan.*$', re.DOTALL)\n",
    "\n",
    "# --- (MODIFIED) ---\n",
    "# Catches \"word\" + \"dari\" + \"domain.com\" -> replaces with \"word\"\n",
    "# Changed \\w+ to \\S+ to include punctuation like '!'\n",
    "_DARI_URL_ATTACHED = re.compile(r'(\\S+)dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# Catches \" dari \" + \"domain.com\" -> replaces with empty string\n",
    "_DARI_URL_SPACED = re.compile(r'\\s+dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# --- (NEW) ---\n",
    "# Catches any word ending in \"dari\" (e.g., \"anarko!dari\", \"negaradari\")\n",
    "_DARI_STUCK = re.compile(r'(\\S+)dari\\b', re.I)\n",
    "\n",
    "def cleantext(row: str):\n",
    "    text = row[\"content\"] #type: ignore\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "    text = text.replace(\"tanya grok\", \" \")\n",
    "    text = text.replace(\"grokproductivitypasang\", \" \")\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "\n",
    "    # Handle standard URLs first\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = text.replace('ini tidak tersedia', ' ')\n",
    "\n",
    "    text = _MENTION.sub('@USER', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "    text = _KUTI_CUT.sub('', text)\n",
    "\n",
    "    # text = _DARI_URL_ATTACHED.sub(r'\\1', text)\n",
    "    # text = _DARI_URL_SPACED.sub('', text)\n",
    "    # text = _DARI_STUCK.sub(r'\\1', text)\n",
    "\n",
    "    text = _WS.sub(' ', text).strip()\n",
    "    row[\"content\"] = text #type: ignore\n",
    "    return row"
   ],
   "id": "ca189ad05aed87ba",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-19T15:27:37.779298Z",
     "start_time": "2025-11-19T15:27:34.753153Z"
    }
   },
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")\n",
    "source_ds = dataset[\"source_labeled\"]\n",
    "source_ds = source_ds.map(cleantext, num_proc=30)\n",
    "source_df = source_ds.to_pandas()\n",
    "source_df = source_df.drop_duplicates(subset=\"content\", keep=\"first\").reset_index(drop=True)\n",
    "source_ds = Dataset.from_pandas(source_df)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:28:04.483914Z",
     "start_time": "2025-11-19T15:28:02.754970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "relevant_only = source_ds.filter(lambda x: x[\"relevant\"] == True)\n",
    "relevant_sampled_split = relevant_only.train_test_split(train_size=30000, test_size=10000, seed=42)\n",
    "\n",
    "dataset[\"train_sentiment\"] = relevant_sampled_split[\"train\"]\n",
    "dataset[\"test_sentiment\"] = relevant_sampled_split[\"test\"]\n",
    "dataset"
   ],
   "id": "8ec526ecb072e5ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/195952 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9d0e1b26635451e874f9f016564c7c0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    source_stage_1: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 201583\n",
       "    })\n",
       "    source_stage_2: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 247820\n",
       "    })\n",
       "    cleaned: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 195952\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 19999\n",
       "    })\n",
       "    source_labeled: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 247820\n",
       "    })\n",
       "    train_sentiment: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "    test_sentiment: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:29:56.835039Z",
     "start_time": "2025-11-19T15:29:56.575625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# Assuming 'dataset' is your Hugging Face DatasetDict containing the splits:\n",
    "# dataset[\"train_sentiment\"]\n",
    "# dataset[\"test_sentiment\"]\n",
    "\n",
    "def check_for_leakage(train_ds: Dataset, test_ds: Dataset, id_column: str = \"tweet_id\"):\n",
    "    \"\"\"\n",
    "    Checks for and quantifies data leakage by identifying overlapping IDs\n",
    "    between the training and testing datasets.\n",
    "    \"\"\"\n",
    "    print(f\"Checking {train_ds.num_rows} training samples against {test_ds.num_rows} testing samples...\")\n",
    "\n",
    "    # 1. Extract and convert IDs to sets for fast lookup (O(1))\n",
    "    try:\n",
    "        # Use .to_list() to ensure we get a standard Python list of IDs\n",
    "        train_ids = set(train_ds[id_column])\n",
    "        test_ids = set(test_ds[id_column])\n",
    "    except KeyError:\n",
    "        print(f\"Error: The datasets do not contain a column named '{id_column}'. Please verify the ID column name.\")\n",
    "        return\n",
    "\n",
    "    # 2. Find the intersection of the two sets\n",
    "    overlapping_ids = train_ids.intersection(test_ids)\n",
    "\n",
    "    overlap_count = len(overlapping_ids)\n",
    "\n",
    "    print(\"\\n--- Leakage Detection Results ---\")\n",
    "\n",
    "    if overlap_count > 0:\n",
    "        print(f\"ðŸš¨ðŸš¨ **CRITICAL LEAKAGE DETECTED!** ðŸš¨ðŸš¨\")\n",
    "        print(f\"Found **{overlap_count}** samples present in BOTH the training and testing sets.\")\n",
    "        print(f\"This represents {overlap_count / train_ds.num_rows * 100:.4f}% of the training data.\")\n",
    "        print(f\"This overlap must be removed to ensure valid model evaluation.\")\n",
    "\n",
    "        # Optional: Print the first few overlapping IDs\n",
    "        print(\"\\nFirst 5 Overlapping IDs:\")\n",
    "        for i, tid in enumerate(list(overlapping_ids)[:5]):\n",
    "            print(f\"- {tid}\")\n",
    "\n",
    "    else:\n",
    "        print(\"âœ… **No sample overlap detected.** The splits are unique by ID.\")\n",
    "\n",
    "    return overlapping_ids\n",
    "overlapping_ids = check_for_leakage(\n",
    "    train_ds=dataset[\"train_sentiment\"],\n",
    "    test_ds=dataset[\"test_sentiment\"],\n",
    "    id_column=\"content\" # Change if your unique ID column is named differently\n",
    ")"
   ],
   "id": "fe0c127e018767c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 30000 training samples against 10000 testing samples...\n",
      "\n",
      "--- Leakage Detection Results ---\n",
      "âœ… **No sample overlap detected.** The splits are unique by ID.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:30:48.421603Z",
     "start_time": "2025-11-19T15:30:32.021980Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.push_to_hub(\"tianharjuno/twitter-parse\", commit_message=\"Created splits for sentiment classifier training\")",
   "id": "fd42e37bb9214712",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f58769e2c7d54aef9ec9f49bef106a75"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/202 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3a03d9aecb54bab98a645c1dbf301fc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6767cc5974a5485fb1a128a7b59c532d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b859bd3c5984e748fb9ce71400ae8d7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "766a55f64ec5497eb5acbd7fc0cf0f40"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/196 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49a2df2fdb0e43a1a52f4fa5ef140a7b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39d2e7ffa1f54cf7b7609256024a9683"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "876a5da2d7d74433abbc05f61a8d3e9f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4874a9760e24db5ad17731c3558551f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dcc476f445444e187b2d17b095c753b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dde03cc60dd9468383afa057e65cfc33"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "baf64eee141049a89bd9574c5e9b22e7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "956f068c765d4dc89300bd990a6c4756"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/30 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5d5f58f6661404aa0d70f77fa8b9d47"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88bf982366844012a748d5d1e4253479"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c08c60e803440f9868c4469a6de8f66"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tianharjuno/twitter-parse/commit/94adc90dba7861ea837ddb3dac78a2afb3795eb8', commit_message='Created splits for sentiment classifier training', commit_description='', oid='94adc90dba7861ea837ddb3dac78a2afb3795eb8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tianharjuno/twitter-parse', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tianharjuno/twitter-parse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
