{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:21:35.589172Z",
     "start_time": "2025-11-22T01:21:35.559436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_WS       = re.compile(r'\\s+')\n",
    "_KUTI_CUT = re.compile(r'(?i)kutipan.*$', re.DOTALL)\n",
    "\n",
    "# --- (MODIFIED) ---\n",
    "# Catches \"word\" + \"dari\" + \"domain.com\" -> replaces with \"word\"\n",
    "# Changed \\w+ to \\S+ to include punctuation like '!'\n",
    "_DARI_URL_ATTACHED = re.compile(r'(\\S+)dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# Catches \" dari \" + \"domain.com\" -> replaces with empty string\n",
    "_DARI_URL_SPACED = re.compile(r'\\s+dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# --- (NEW) ---\n",
    "# Catches any word ending in \"dari\" (e.g., \"anarko!dari\", \"negaradari\")\n",
    "_DARI_STUCK = re.compile(r'(\\S+)dari\\b', re.I)\n",
    "\n",
    "def cleantext(row: str):\n",
    "    text = row[\"content\"] #type: ignore\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "    text = text.replace(\"tanya grok\", \" \")\n",
    "    text = text.replace(\"grokproductivitypasang\", \" \")\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "\n",
    "    # Handle standard URLs first\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = text.replace('ini tidak tersedia', ' ')\n",
    "\n",
    "    text = _MENTION.sub('@USER', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "    text = _KUTI_CUT.sub('', text)\n",
    "\n",
    "    # text = _DARI_URL_ATTACHED.sub(r'\\1', text)\n",
    "    # text = _DARI_URL_SPACED.sub('', text)\n",
    "    # text = _DARI_STUCK.sub(r'\\1', text)\n",
    "\n",
    "    text = _WS.sub(' ', text).strip()\n",
    "    row[\"content\"] = text #type: ignore\n",
    "    return row"
   ],
   "id": "ca189ad05aed87ba",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-22T01:21:52.291250Z",
     "start_time": "2025-11-22T01:21:35.820437Z"
    }
   },
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")\n",
    "source_ds = dataset[\"source_labeled\"]\n",
    "source_ds = source_ds.map(cleantext, num_proc=30)\n",
    "source_df = source_ds.to_pandas()\n",
    "source_df = source_df.drop_duplicates(subset=\"content\", keep=\"first\").reset_index(drop=True)\n",
    "source_ds = Dataset.from_pandas(source_df)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=30):   0%|          | 0/247820 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a4e1005a7ac4fdcaf0f2e482a98a861"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:21:54.103431Z",
     "start_time": "2025-11-22T01:21:53.452097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "relevant_only = source_ds.filter(lambda x: x[\"relevant\"] == True)\n",
    "relevant_sampled_split = relevant_only.train_test_split(train_size=20000, test_size=10000, seed=42)\n",
    "\n",
    "dataset[\"train_sentiment\"] = relevant_sampled_split[\"train\"]\n",
    "dataset[\"test_sentiment\"] = relevant_sampled_split[\"test\"]\n",
    "dataset"
   ],
   "id": "8ec526ecb072e5ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/195952 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e3aef5b5c8d40c599a17746d3baae80"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    source_stage_1: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 201583\n",
       "    })\n",
       "    source_stage_2: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 247820\n",
       "    })\n",
       "    cleaned: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 195952\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 19999\n",
       "    })\n",
       "    source_labeled: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 247820\n",
       "    })\n",
       "    train_sentiment: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    test_sentiment: Dataset({\n",
       "        features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:22:01.257802Z",
     "start_time": "2025-11-22T01:22:01.143594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# Assuming 'dataset' is your Hugging Face DatasetDict containing the splits:\n",
    "# dataset[\"train_sentiment\"]\n",
    "# dataset[\"test_sentiment\"]\n",
    "\n",
    "def check_for_leakage(train_ds: Dataset, test_ds: Dataset, id_column: str = \"tweet_id\"):\n",
    "    \"\"\"\n",
    "    Checks for and quantifies data leakage by identifying overlapping IDs\n",
    "    between the training and testing datasets.\n",
    "    \"\"\"\n",
    "    print(f\"Checking {train_ds.num_rows} training samples against {test_ds.num_rows} testing samples...\")\n",
    "\n",
    "    # 1. Extract and convert IDs to sets for fast lookup (O(1))\n",
    "    try:\n",
    "        # Use .to_list() to ensure we get a standard Python list of IDs\n",
    "        train_ids = set(train_ds[id_column])\n",
    "        test_ids = set(test_ds[id_column])\n",
    "    except KeyError:\n",
    "        print(f\"Error: The datasets do not contain a column named '{id_column}'. Please verify the ID column name.\")\n",
    "        return\n",
    "\n",
    "    # 2. Find the intersection of the two sets\n",
    "    overlapping_ids = train_ids.intersection(test_ids)\n",
    "\n",
    "    overlap_count = len(overlapping_ids)\n",
    "\n",
    "    print(\"\\n--- Leakage Detection Results ---\")\n",
    "\n",
    "    if overlap_count > 0:\n",
    "        print(f\"ðŸš¨ðŸš¨ **CRITICAL LEAKAGE DETECTED!** ðŸš¨ðŸš¨\")\n",
    "        print(f\"Found **{overlap_count}** samples present in BOTH the training and testing sets.\")\n",
    "        print(f\"This represents {overlap_count / train_ds.num_rows * 100:.4f}% of the training data.\")\n",
    "        print(f\"This overlap must be removed to ensure valid model evaluation.\")\n",
    "\n",
    "        # Optional: Print the first few overlapping IDs\n",
    "        print(\"\\nFirst 5 Overlapping IDs:\")\n",
    "        for i, tid in enumerate(list(overlapping_ids)[:5]):\n",
    "            print(f\"- {tid}\")\n",
    "\n",
    "    else:\n",
    "        print(\"âœ… **No sample overlap detected.** The splits are unique by ID.\")\n",
    "\n",
    "    return overlapping_ids\n",
    "overlapping_ids = check_for_leakage(\n",
    "    train_ds=dataset[\"train_sentiment\"],\n",
    "    test_ds=dataset[\"test_sentiment\"],\n",
    "    id_column=\"content\" # Change if your unique ID column is named differently\n",
    ")"
   ],
   "id": "fe0c127e018767c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 20000 training samples against 10000 testing samples...\n",
      "\n",
      "--- Leakage Detection Results ---\n",
      "âœ… **No sample overlap detected.** The splits are unique by ID.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:22:20.282460Z",
     "start_time": "2025-11-22T01:22:04.506402Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.push_to_hub(\"tianharjuno/twitter-parse\", commit_message=\"Created splits for sentiment classifier training\")",
   "id": "fd42e37bb9214712",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a37458f570a49279fa1300366c3a2d9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/202 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "effdded8f84a4d90b9f95e66bd413287"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1740fe31d7b34f8198005822549d4dbd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a18b292093dc4c26a3f986406abd90ca"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b870c52b5035418998006f5012811eaf"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/196 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc663ecfb3904d67a1cef7aaf780a591"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb6cc006307a41be8b106550e20a48e5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bc9568f1280425bb593a135ded3c342"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ddc5cefc5bd4f9b8d9747508d8c96c9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "265fc6132a4d49dc8283f926834811a4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "adf8b5b79612435e8c62d40a41a4c12a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea2a6a65aea54e7bbff95d58c5c380c9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f5027d47bf7489db2dc48f28e8b2268"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2a0360d03fd4aa290e06791f48fb75b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed024005378d4931add4643bfb15ae22"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7d82a64ecea472393e803ac11e4d370"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tianharjuno/twitter-parse/commit/c03e69afd1e602489e2afe92eeccf028c856723a', commit_message='Created splits for sentiment classifier training', commit_description='', oid='c03e69afd1e602489e2afe92eeccf028c856723a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tianharjuno/twitter-parse', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tianharjuno/twitter-parse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
