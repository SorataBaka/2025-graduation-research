{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "train the pipeline, with a larger amount\n",
    "\n",
    "mix in the training data with the labeled training data\n",
    "\n",
    "then we can retrieve more samples of a certain type\n",
    "\n",
    "in this case, we only care about relevant samples"
   ],
   "id": "f167067f8ab8d1e8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-21T11:52:08.506260Z",
     "start_time": "2025-11-21T11:52:00.169313Z"
    }
   },
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "class_labels = ClassLabel(3, [\"Negative\", \"Neutral\", \"Positive\"])\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T10:45:03.448344Z",
     "start_time": "2025-11-22T10:45:03.426508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_WS       = re.compile(r'\\s+')\n",
    "_KUTI_CUT = re.compile(r'(?i)kutipan.*$', re.DOTALL)\n",
    "\n",
    "# --- (MODIFIED) ---\n",
    "# Catches \"word\" + \"dari\" + \"domain.com\" -> replaces with \"word\"\n",
    "# Changed \\w+ to \\S+ to include punctuation like '!'\n",
    "_DARI_URL_ATTACHED = re.compile(r'(\\S+)dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# Catches \" dari \" + \"domain.com\" -> replaces with empty string\n",
    "_DARI_URL_SPACED = re.compile(r'\\s+dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# --- (NEW) ---\n",
    "# Catches any word ending in \"dari\" (e.g., \"anarko!dari\", \"negaradari\")\n",
    "_DARI_STUCK = re.compile(r'(\\S+)dari\\b', re.I)\n",
    "\n",
    "def cleantext(row: str):\n",
    "    text = row[\"content\"] #type: ignore\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "    text = text.replace(\"tanya grok\", \" \")\n",
    "    text = text.replace(\"grokproductivitypasang\", \" \")\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "\n",
    "    # Handle standard URLs first\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = text.replace('ini tidak tersedia', ' ')\n",
    "\n",
    "    text = _MENTION.sub('@USER', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "    text = _KUTI_CUT.sub('', text)\n",
    "\n",
    "    # text = _DARI_URL_ATTACHED.sub(r'\\1', text)\n",
    "    # text = _DARI_URL_SPACED.sub('', text)\n",
    "    # text = _DARI_STUCK.sub(r'\\1', text)\n",
    "\n",
    "    text = _WS.sub(' ', text).strip()\n",
    "    row[\"content\"] = text #type: ignore\n",
    "    return row"
   ],
   "id": "45357000d90ce985",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T10:44:54.164878Z",
     "start_time": "2025-11-22T10:44:54.143338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "source_ds = dataset[\"source_labeled\"]\n",
    "source_ds = source_ds.map(cleantext, num_proc=16)\n",
    "\n",
    "relevant_ds = source_ds.filter(lambda ex: ex[\"relevant\"] == True)\n",
    "relevant_df = relevant_ds.to_pandas()\n",
    "relevant_df = relevant_df.drop_duplicates(subset=\"content\", keep=\"first\").reset_index(drop=True)\n",
    "relevant_ds = Dataset.from_pandas(relevant_df)\n",
    "\n",
    "train_ds = dataset[\"train_sentiment\"]\n",
    "test_ds = dataset[\"test_sentiment\"]\n",
    "\n",
    "train_ids = set(train_ds[\"content\"])\n",
    "test_ids = set(test_ds[\"content\"])\n",
    "\n",
    "filtered = relevant_ds.filter(\n",
    "    lambda ex: ex[\"content\"] not in train_ids and ex[\"content\"] not in test_ids\n",
    ")\n",
    "\n",
    "dataset_list = []\n",
    "\n",
    "dataset_list.append(train_ds)\n",
    "unlabeled_ds = filtered.train_test_split(train_size=50000, seed=42)[\"train\"]\n",
    "def reset_sentiment(row):\n",
    "    row[\"sentiment\"] = -1\n",
    "    return row\n",
    "unlabeled_ds = unlabeled_ds.map(reset_sentiment)\n",
    "dataset_list.append(unlabeled_ds)\n",
    "\n",
    "concat_ds = concatenate_datasets(dataset_list)"
   ],
   "id": "533ef990226f1992",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleantext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Dataset, concatenate_datasets\n\u001B[32m      2\u001B[39m source_ds = dataset[\u001B[33m\"\u001B[39m\u001B[33msource_labeled\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m source_ds = source_ds.map(\u001B[43mcleantext\u001B[49m, num_proc=\u001B[32m16\u001B[39m)\n\u001B[32m      5\u001B[39m relevant_ds = source_ds.filter(\u001B[38;5;28;01mlambda\u001B[39;00m ex: ex[\u001B[33m\"\u001B[39m\u001B[33mrelevant\u001B[39m\u001B[33m\"\u001B[39m] == \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      6\u001B[39m relevant_df = relevant_ds.to_pandas()\n",
      "\u001B[31mNameError\u001B[39m: name 'cleantext' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T10:44:58.962430Z",
     "start_time": "2025-11-22T10:44:58.948575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "# We are NOT importing replace_word_elongation anymore\n",
    "from indoNLP.preprocessing import emoji_to_words\n",
    "\n",
    "def clean_tweet_for_nusabert(row):\n",
    "    text = row['content']\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # 3. Remove mentions and RT\n",
    "    # This regex is safe and does not affect 'ruu tni'\n",
    "    text = re.sub(r'rt @\\S+|@\\S+', '', text)\n",
    "\n",
    "    # 4. Remove hashtags (keep the word)\n",
    "    text = re.sub(r'#(\\S+)', r'\\1', text)\n",
    "\n",
    "    # 5. Convert emojis to words (Preserves sentiment)\n",
    "    text = emoji_to_words(text)\n",
    "\n",
    "    # 6. Normalize word elongation (CUSTOM, SAFER REGEX)\n",
    "    # This replaces 3 or more repeated chars (e.g., 'bangeeet' -> 'banget')\n",
    "    # It will NOT affect 'uu' or 'ruu', fixing your bug.\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # 7. Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    row[\"content\"] = text\n",
    "    return row"
   ],
   "id": "761a1de40da00074",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:28:54.129083Z",
     "start_time": "2025-11-21T15:28:54.090648Z"
    }
   },
   "cell_type": "code",
   "source": "concat_ds = concat_ds.map(clean_tweet_for_nusabert, num_proc=16)",
   "id": "11116b0d48fb54e1",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concat_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m concat_ds = \u001B[43mconcat_ds\u001B[49m.map(clean_tweet_for_nusabert, num_proc=\u001B[32m16\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'concat_ds' is not defined"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T11:56:12.031519Z",
     "start_time": "2025-11-21T11:54:03.134480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_transformer = SentenceTransformer(\"LazarusNLP/all-nusabert-large-v4\",\n",
    "                                           cache_folder=\"/data/cache/\",\n",
    "                                           device=\"cuda\",\n",
    "                                           )\n",
    "embeddings = sentence_transformer.encode(concat_ds[\"content\"], show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True, device=\"cuda\", batch_size=128)"
   ],
   "id": "4a82b92642fc11ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/625 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbb08101d7764b67812c9c9128b6931b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T12:09:44.247850Z",
     "start_time": "2025-11-21T11:57:17.527181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dream_pipeline import DreamCluster\n",
    "clusterer = DreamCluster(\"stability\")\n",
    "clusterer.fit(embeddings)"
   ],
   "id": "7e9d5e91f51f4d89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding intrinsic dimension with TwoNN\n",
      "Found intrinsic dimension: 20\n",
      "Running first stage reduction\n",
      "Running second stage reduction\n",
      "Tuning HDBSCAN with mode stability\n",
      "  -> Using 'stability' mode (Bayesian Optimization on relative_validity)\n",
      "Transforming full dataset with trained reducers...\n",
      "Fitting final clusterer on full reduced dataset...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.int64(146), np.int64(32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T12:10:14.270916Z",
     "start_time": "2025-11-21T12:10:09.026981Z"
    }
   },
   "cell_type": "code",
   "source": "cluster_labels, cluster_probabilities, reduced_embeddings = clusterer.predict(embeddings=embeddings)",
   "id": "45e64220fe36933d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T12:10:34.371788Z",
     "start_time": "2025-11-21T12:10:34.153467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "unique_labels, unique_label_counts = np.unique(cluster_labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, unique_label_counts):\n",
    "    indices = np.where(cluster_labels == label)[0]\n",
    "    cluster_ds = concat_ds.select(indices)\n",
    "    cluster_classes, cluster_class_counts = np.unique(cluster_ds[\"sentiment\"], return_counts=True)\n",
    "    print(f\"CLUSTER {label}: {dict(zip(cluster_classes, cluster_class_counts))}\")"
   ],
   "id": "11f91f9eae01aa52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER -1: {np.int64(-1): np.int64(7318), np.int64(0): np.int64(3598), np.int64(1): np.int64(538), np.int64(2): np.int64(210)}\n",
      "CLUSTER 0: {np.int64(-1): np.int64(260), np.int64(1): np.int64(9), np.int64(2): np.int64(173)}\n",
      "CLUSTER 1: {np.int64(-1): np.int64(385), np.int64(0): np.int64(1), np.int64(1): np.int64(3), np.int64(2): np.int64(222)}\n",
      "CLUSTER 2: {np.int64(-1): np.int64(451), np.int64(0): np.int64(244), np.int64(1): np.int64(5), np.int64(2): np.int64(2)}\n",
      "CLUSTER 3: {np.int64(-1): np.int64(195), np.int64(1): np.int64(1), np.int64(2): np.int64(109)}\n",
      "CLUSTER 4: {np.int64(-1): np.int64(142), np.int64(0): np.int64(83), np.int64(1): np.int64(11)}\n",
      "CLUSTER 5: {np.int64(-1): np.int64(1677), np.int64(0): np.int64(874), np.int64(1): np.int64(53), np.int64(2): np.int64(72)}\n",
      "CLUSTER 6: {np.int64(-1): np.int64(370), np.int64(0): np.int64(154), np.int64(1): np.int64(91), np.int64(2): np.int64(1)}\n",
      "CLUSTER 7: {np.int64(-1): np.int64(89), np.int64(0): np.int64(25), np.int64(1): np.int64(23), np.int64(2): np.int64(14)}\n",
      "CLUSTER 8: {np.int64(-1): np.int64(561), np.int64(0): np.int64(183), np.int64(1): np.int64(83), np.int64(2): np.int64(14)}\n",
      "CLUSTER 9: {np.int64(-1): np.int64(159), np.int64(0): np.int64(87), np.int64(1): np.int64(5)}\n",
      "CLUSTER 10: {np.int64(-1): np.int64(214), np.int64(0): np.int64(82), np.int64(1): np.int64(17)}\n",
      "CLUSTER 11: {np.int64(-1): np.int64(134), np.int64(1): np.int64(7), np.int64(2): np.int64(70)}\n",
      "CLUSTER 12: {np.int64(-1): np.int64(3835), np.int64(0): np.int64(2109), np.int64(1): np.int64(93), np.int64(2): np.int64(53)}\n",
      "CLUSTER 13: {np.int64(-1): np.int64(1378), np.int64(0): np.int64(777), np.int64(1): np.int64(27), np.int64(2): np.int64(4)}\n",
      "CLUSTER 14: {np.int64(-1): np.int64(112), np.int64(0): np.int64(54), np.int64(1): np.int64(19), np.int64(2): np.int64(5)}\n",
      "CLUSTER 15: {np.int64(-1): np.int64(299), np.int64(0): np.int64(10), np.int64(1): np.int64(4), np.int64(2): np.int64(159)}\n",
      "CLUSTER 16: {np.int64(-1): np.int64(498), np.int64(0): np.int64(11), np.int64(1): np.int64(22), np.int64(2): np.int64(310)}\n",
      "CLUSTER 17: {np.int64(-1): np.int64(135), np.int64(0): np.int64(86), np.int64(1): np.int64(2)}\n",
      "CLUSTER 18: {np.int64(-1): np.int64(334), np.int64(0): np.int64(218), np.int64(1): np.int64(2)}\n",
      "CLUSTER 19: {np.int64(-1): np.int64(5565), np.int64(0): np.int64(3286), np.int64(1): np.int64(15)}\n",
      "CLUSTER 20: {np.int64(-1): np.int64(85), np.int64(0): np.int64(1), np.int64(1): np.int64(1), np.int64(2): np.int64(61)}\n",
      "CLUSTER 21: {np.int64(-1): np.int64(422), np.int64(0): np.int64(276)}\n",
      "CLUSTER 22: {np.int64(-1): np.int64(259), np.int64(0): np.int64(5), np.int64(1): np.int64(3), np.int64(2): np.int64(136)}\n",
      "CLUSTER 23: {np.int64(-1): np.int64(201), np.int64(0): np.int64(15), np.int64(1): np.int64(20), np.int64(2): np.int64(103)}\n",
      "CLUSTER 24: {np.int64(-1): np.int64(18800), np.int64(0): np.int64(6632), np.int64(1): np.int64(3667), np.int64(2): np.int64(1009)}\n",
      "CLUSTER 25: {np.int64(-1): np.int64(330), np.int64(0): np.int64(241)}\n",
      "CLUSTER 26: {np.int64(-1): np.int64(214), np.int64(0): np.int64(151), np.int64(2): np.int64(1)}\n",
      "CLUSTER 27: {np.int64(-1): np.int64(3455), np.int64(0): np.int64(1857), np.int64(1): np.int64(126), np.int64(2): np.int64(100)}\n",
      "CLUSTER 28: {np.int64(-1): np.int64(411), np.int64(0): np.int64(190), np.int64(1): np.int64(66), np.int64(2): np.int64(13)}\n",
      "CLUSTER 29: {np.int64(-1): np.int64(97), np.int64(0): np.int64(30), np.int64(1): np.int64(31), np.int64(2): np.int64(1)}\n",
      "CLUSTER 30: {np.int64(-1): np.int64(1415), np.int64(0): np.int64(610), np.int64(1): np.int64(89), np.int64(2): np.int64(132)}\n",
      "CLUSTER 31: {np.int64(-1): np.int64(200), np.int64(0): np.int64(66), np.int64(1): np.int64(23), np.int64(2): np.int64(14)}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T12:10:41.794077Z",
     "start_time": "2025-11-21T12:10:41.778045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def build_labeling_json(concat_ds, cluster_labels, cluster_probabilities, output_path):\n",
    "    \"\"\"\n",
    "    Build labeling JSON for HTML labeling suite.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    concat_ds : HuggingFace Dataset\n",
    "        Must contain columns: tweet_id, time, author, content, sentiment\n",
    "    cluster_labels : Sequence[int]\n",
    "        Cluster assignment per sample\n",
    "    cluster_probabilities : Sequence[float]\n",
    "        Cluster probability per sample\n",
    "    output_path : str\n",
    "        Where to save the output JSON file\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to Python lists for safety\n",
    "    cluster_labels = list(cluster_labels)\n",
    "    cluster_probabilities = list(cluster_probabilities)\n",
    "\n",
    "    # Basic checks\n",
    "    assert len(concat_ds) == len(cluster_labels)\n",
    "    assert len(concat_ds) == len(cluster_probabilities)\n",
    "\n",
    "    # Organize samples by cluster\n",
    "    clusters = defaultdict(list)\n",
    "    for idx, row in enumerate(concat_ds):\n",
    "        cid = int(cluster_labels[idx])\n",
    "        prob = float(cluster_probabilities[idx])\n",
    "\n",
    "        original_label = int(row[\"sentiment\"])\n",
    "        is_unlabeled = original_label == -1\n",
    "\n",
    "        sample = {\n",
    "            \"tweet_id\": row[\"tweet_id\"],\n",
    "            \"content\": row[\"content\"],\n",
    "            \"author\": row.get(\"author\", None),\n",
    "            \"time\": row.get(\"time\", None),\n",
    "\n",
    "            \"cluster_probability\": prob,\n",
    "            \"original_label\": original_label,\n",
    "            \"current_label\": original_label,\n",
    "            \"is_unlabeled\": is_unlabeled,\n",
    "            \"needs_check\": False  # Will fill below\n",
    "        }\n",
    "\n",
    "        clusters[cid].append(sample)\n",
    "\n",
    "    # Build cluster stats + mark minority labels\n",
    "    cluster_blocks = []\n",
    "    for cid, samples in clusters.items():\n",
    "        # Collect stats\n",
    "        label_counts = Counter(s[\"original_label\"] for s in samples if s[\"original_label\"] != -1)\n",
    "        size = len(samples)\n",
    "\n",
    "        # Determine \"dominant\" class for minority detection\n",
    "        if len(label_counts) > 0:\n",
    "            dominant_label, dominant_count = label_counts.most_common(1)[0]\n",
    "        else:\n",
    "            dominant_label, dominant_count = None, 0  # All unlabeled cluster\n",
    "\n",
    "        # Mark minority samples\n",
    "        for s in samples:\n",
    "            orig = s[\"original_label\"]\n",
    "            if orig != -1 and orig != dominant_label:\n",
    "                s[\"needs_check\"] = True\n",
    "\n",
    "        cluster_blocks.append({\n",
    "            \"cluster_id\": cid,\n",
    "            \"stats\": {\n",
    "                \"size\": size,\n",
    "                \"labels\": {str(k): v for k, v in label_counts.items()}\n",
    "            },\n",
    "            \"samples\": samples\n",
    "        })\n",
    "\n",
    "    # Compose final JSON\n",
    "    output_json = {\n",
    "        \"metadata\": {\n",
    "            \"version\": 1,\n",
    "            \"generated_at\": datetime.utcnow().isoformat(),\n",
    "            \"num_clusters\": len(cluster_blocks),\n",
    "            \"num_samples\": len(concat_ds),\n",
    "\n",
    "            \"label_schema\": {\n",
    "                \"-1\": \"Unlabeled\",\n",
    "                \"0\": \"Negative\",\n",
    "                \"1\": \"Neutral\",\n",
    "                \"2\": \"Positive\"\n",
    "            }\n",
    "        },\n",
    "        \"clusters\": cluster_blocks\n",
    "    }\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[OK] Labeling JSON exported to: {output_path}\")\n"
   ],
   "id": "f3472b40e9ca7778",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T12:10:48.228845Z",
     "start_time": "2025-11-21T12:10:45.587141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "build_labeling_json(\n",
    "    concat_ds=concat_ds,\n",
    "    cluster_labels=cluster_labels,\n",
    "    cluster_probabilities=cluster_probabilities,\n",
    "    output_path=\"out/sentiment_train_data_unlabeled.json\"\n",
    ")"
   ],
   "id": "3c273516349e88aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Labeling JSON exported to: labeling_data.json\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T14:41:42.277240Z",
     "start_time": "2025-11-21T14:41:42.022252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"out/sentiment_train_data_labeled.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "all_samples = []\n",
    "\n",
    "for cluster in data[\"clusters\"]:\n",
    "    samples = cluster.get(\"samples\", [])\n",
    "    all_samples.extend(samples)\n",
    "\n",
    "print(f\"Total samples collected: {len(all_samples)}\")"
   ],
   "id": "9e93e016cab13e05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples collected: 80000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T14:49:20.426881Z",
     "start_time": "2025-11-21T14:49:11.683065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "original_dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")\n",
    "sentiment_train_ds = original_dataset[\"train_sentiment\"]\n",
    "source_labeled = original_dataset[\"source_labeled\"]\n",
    "parsed_ds = Dataset.from_list(all_samples)\n",
    "parsed_ds = parsed_ds.filter(lambda row: row[\"original_label\"] != row[\"current_label\"])"
   ],
   "id": "1aa7b14991ce088a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/80000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5a5c069f7cf46cc8f8113bd0d774a96"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T14:52:23.821922Z",
     "start_time": "2025-11-21T14:52:21.826848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# 1. Create a mapping of ID -> New Label from your corrected data\n",
    "# We assume 'parsed_ds' is already filtered for changes as per your code\n",
    "update_map = {row['tweet_id']: row['current_label'] for row in parsed_ds}\n",
    "correction_ids = set(update_map.keys())\n",
    "\n",
    "# 2. distinct sets for logic\n",
    "existing_train_ids = set(sentiment_train_ds['tweet_id'])\n",
    "ids_to_update = existing_train_ids.intersection(correction_ids)\n",
    "ids_to_add = correction_ids.difference(existing_train_ids)\n",
    "\n",
    "# 3. Update the existing rows in sentiment_train_ds\n",
    "def update_existing_sentiment(row):\n",
    "    if row['tweet_id'] in ids_to_update:\n",
    "        row['sentiment'] = update_map[row['tweet_id']]\n",
    "    return row\n",
    "\n",
    "updated_train_ds = sentiment_train_ds.map(update_existing_sentiment)\n",
    "\n",
    "# 4. specific handling for NEW rows (referencing source_labeled)\n",
    "if len(ids_to_add) > 0:\n",
    "    # Filter source_labeled to get the raw data for the new IDs\n",
    "    source_labeled = original_dataset[\"source_labeled\"]\n",
    "    new_rows_ds = source_labeled.filter(lambda row: row['tweet_id'] in ids_to_add)\n",
    "\n",
    "    # Apply the new labels and ensure relevance is set to True\n",
    "    def prepare_new_rows(row):\n",
    "        row['sentiment'] = update_map[row['tweet_id']]\n",
    "        row['relevant'] = True\n",
    "        return row\n",
    "\n",
    "    new_rows_ds = new_rows_ds.map(prepare_new_rows)\n",
    "\n",
    "    # 5. Ensure column consistency before merging\n",
    "    # We select only columns present in the training set to avoid schema conflicts\n",
    "    train_columns = updated_train_ds.column_names\n",
    "    new_rows_ds = new_rows_ds.select_columns(train_columns)\n",
    "\n",
    "    # 6. Concatenate\n",
    "    final_train_ds = concatenate_datasets([updated_train_ds, new_rows_ds])\n",
    "else:\n",
    "    final_train_ds = updated_train_ds\n",
    "\n",
    "print(f\"Original Size: {len(sentiment_train_ds)}\")\n",
    "print(f\"New Size: {len(final_train_ds)}\")"
   ],
   "id": "e69046083c748561",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79b3aa20df354d0092427b02161c8a7a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/247820 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd936421cf074047b226146d732f00b4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1612 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f024a0ce5af34fd0a0855c7c9af23008"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Size: 30000\n",
      "New Size: 31612\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T14:53:50.761759Z",
     "start_time": "2025-11-21T14:53:50.754498Z"
    }
   },
   "cell_type": "code",
   "source": "final_train_ds",
   "id": "33e9d06d6918b95",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tweet_id', 'time', 'author', 'content', 'comment_count', 'repost_count', 'like_count', 'view_count', 'relevant', 'sentiment'],\n",
       "    num_rows: 31612\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T14:54:36.729762Z",
     "start_time": "2025-11-21T14:54:20.771217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "sentiment_column = final_train_ds[\"sentiment\"]\n",
    "original_dataset[\"train_sentiment\"] = final_train_ds\n",
    "original_dataset.push_to_hub(\"tianharjuno/twitter-parse\", commit_message=\"added more relevant data through clustering\")"
   ],
   "id": "7973fd183270827c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "692948a2c86a431ca50c7e838fc0311a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/202 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eaea169ffa644696825f0c8c37463773"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37010f55100b4d86a111f1d628255af4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "767ade79217a489ba763ecc3527ab237"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8ffcd52875f4271beede85f752bb116"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/196 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a32a3830c884cbebf3c4992354c1c79"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24c0b301969547f983ea96a0a18ee141"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93e18f79e56c4f4a8984143754b4bc80"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6432c3b24ee846618fd36145edf120f0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1f9a10ce11d45358e7a2a3a0947d45a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bec1fe1fefce4e4d85f4f5a2d1701daa"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "338b8330d8b24b4abeb8d271a084fb9b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "795230f973004ff69eefb42b5f38fc96"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/32 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95983ebc12bf4e52958b00434d26b0f9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84d321df496a4cbc968aed2db4472e9c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dad5204ac5ed49cc98d87d0cea8e7de5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tianharjuno/twitter-parse/commit/aea7d115dcf04d8ed008f60db3fd834ea99dc199', commit_message='added more relevant data through clustering', commit_description='', oid='aea7d115dcf04d8ed008f60db3fd834ea99dc199', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tianharjuno/twitter-parse', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tianharjuno/twitter-parse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T10:48:17.071583Z",
     "start_time": "2025-11-22T10:45:33.991377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, Dataset\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")\n",
    "relevant_ds = dataset[\"source_labeled\"].filter(lambda row: row[\"relevant\"] == True)\n",
    "test_ds = dataset[\"test_sentiment\"]\n",
    "\n",
    "relevant_ds = relevant_ds.map(cleantext, num_proc=10)\n",
    "test_ds = test_ds.map(cleantext, num_proc=10)\n",
    "\n",
    "relevant_ds = relevant_ds.map(clean_tweet_for_nusabert, num_proc=10)\n",
    "test_ds = test_ds.map(clean_tweet_for_nusabert, num_proc=10)\n",
    "\n",
    "relevant_df = relevant_ds.to_pandas().drop_duplicates(subset=\"content\", keep=\"first\").reset_index(drop=True)\n",
    "relevant_ds = Dataset.from_pandas(relevant_df)\n",
    "relevant_ds = relevant_ds.train_test_split(train_size=80000, shuffle=True, seed=42)[\"train\"]\n",
    "\n",
    "sentence_transformer = SentenceTransformer(\"LazarusNLP/all-nusabert-large-v4\",\n",
    "                                           cache_folder=\"/data/cache/\",\n",
    "                                           device=\"cuda\",\n",
    "                                           )\n",
    "train_embeddings = sentence_transformer.encode(relevant_ds[\"content\"], show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True, batch_size=32)\n",
    "test_embeddings = sentence_transformer.encode(test_ds[\"content\"], show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True, batch_size=32)\n",
    "\n"
   ],
   "id": "20bc56f42d0f7fed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/147701 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2dad2749be3491b8b47c71ee750a3a5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43bd5f5cc7d644c1ab909d9546d3d134"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/147701 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5242d666a52347b2a0cc3a941e63194c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f620eed2b484d14a83cb611be950571"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4058bf99a24644bb8eff4dbb91804220"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b73c609d6bd6458ba641a0d5cd341a9a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T11:01:00.319811Z",
     "start_time": "2025-11-22T10:49:08.535458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dream_pipeline import DreamCluster\n",
    "clusterer = DreamCluster(\"stability\")\n",
    "clusterer.fit(train_embeddings)"
   ],
   "id": "94f66a9b35470b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding intrinsic dimension with TwoNN\n",
      "Found intrinsic dimension: 20\n",
      "Running first stage reduction\n",
      "Running second stage reduction\n",
      "Tuning HDBSCAN with mode stability\n",
      "  -> Using 'stability' mode (Bayesian Optimization on relative_validity)\n",
      "Transforming full dataset with trained reducers...\n",
      "Fitting final clusterer on full reduced dataset...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.int64(11), np.int64(231))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T11:01:36.830114Z",
     "start_time": "2025-11-22T11:01:00.618748Z"
    }
   },
   "cell_type": "code",
   "source": "cluster_labels, cluster_probabilities, reduced_embeddings = clusterer.predict(test_embeddings)",
   "id": "fe407729364b3086",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T11:01:53.467697Z",
     "start_time": "2025-11-22T11:01:52.894854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# 1. Add Cluster and Probability (Your existing logic)\n",
    "if \"cluster\" not in test_ds.column_names:\n",
    "    test_ds = test_ds.add_column(\"cluster\", cluster_labels)\n",
    "if \"probability\" not in test_ds.column_names:\n",
    "    test_ds = test_ds.add_column(\"probability\", cluster_probabilities)\n",
    "\n",
    "# 2. Setup Labeling Columns\n",
    "# We keep 'original_label' as a backup, and 'sentiment' as the field to be edited.\n",
    "if \"original_label\" not in test_ds.column_names:\n",
    "    test_ds = test_ds.add_column(\"original_label\", test_ds[\"sentiment\"])\n",
    "\n",
    "# Optimization: add_column is much faster than map for constant values\n",
    "if \"status\" not in test_ds.column_names:\n",
    "    test_ds = test_ds.add_column(\"status\", [\"uncorrected\"] * len(test_ds))\n",
    "\n",
    "# 3. Add ID (Required for UI logic)\n",
    "if \"id\" not in test_ds.column_names:\n",
    "    test_ds = test_ds.add_column(\"id\", range(len(test_ds)))\n",
    "\n",
    "# 4. Add 'relevant' column (Required for the schema)\n",
    "# If you don't have relevancy data, we default to 1 (Relevant) so they show up in the UI.\n",
    "if \"relevant\" not in test_ds.column_names:\n",
    "    test_ds = test_ds.add_column(\"relevant\", [1] * len(test_ds))\n",
    "\n",
    "# 5. Export to JSON with Schema Mapping\n",
    "# We use a list comprehension to strictly enforce the key names required by the HTML.\n",
    "def export_to_labeling_format(dataset, output_file=\"sentiment_data.json\"):\n",
    "    export_data = []\n",
    "\n",
    "    for row in dataset:\n",
    "        # Map dataset columns to UI schema keys\n",
    "        # row.get(x, y) tries to find column x, returns y if not found\n",
    "        item = {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"cluster\": row[\"cluster\"],\n",
    "            \"probability\": row[\"probability\"],\n",
    "            # Map 'text' to 'content' if necessary\n",
    "            \"content\": row.get(\"content\", row.get(\"text\", \"\")),\n",
    "            \"relevant\": row[\"relevant\"],\n",
    "            \"original_label\": row[\"original_label\"],\n",
    "            \"status\": row[\"status\"],\n",
    "            \"sentiment\": row[\"sentiment\"], # This is the active column for 0, 1, 2\n",
    "\n",
    "            # Metadata (Fill with defaults if your dataset doesn't have them)\n",
    "            \"tweet_id\": str(row.get(\"tweet_id\", row.get(\"id\", \"\"))),\n",
    "            \"time\": row.get(\"time\", row.get(\"created_at\", \"\")),\n",
    "            \"author\": row.get(\"author\", row.get(\"username\", \"Unknown\")),\n",
    "            \"comment_count\": row.get(\"comment_count\", 0),\n",
    "            \"repost_count\": row.get(\"repost_count\", 0),\n",
    "            \"like_count\": row.get(\"like_count\", 0),\n",
    "            \"view_count\": row.get(\"view_count\", 0),\n",
    "        }\n",
    "        export_data.append(item)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "\n",
    "    print(f\"Exported {len(export_data)} items to {output_file}\")\n",
    "\n",
    "# Run the export\n",
    "export_to_labeling_format(test_ds)"
   ],
   "id": "b2392c543eb10059",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 10000 items to sentiment_data.json\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T14:44:46.833208Z",
     "start_time": "2025-11-22T14:44:42.328183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")\n",
    "test_ds = dataset[\"test_sentiment\"]\n",
    "test_ds.column_names"
   ],
   "id": "da70943adaf9ab2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweet_id',\n",
       " 'time',\n",
       " 'author',\n",
       " 'content',\n",
       " 'comment_count',\n",
       " 'repost_count',\n",
       " 'like_count',\n",
       " 'view_count',\n",
       " 'relevant',\n",
       " 'sentiment']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T14:48:52.352464Z",
     "start_time": "2025-11-22T14:48:52.003246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# 1. Load your JSON list (assuming it's in a variable called `corrected_labels`)\n",
    "# If it's in a file:\n",
    "with open('sentiment_corrected.json', 'r') as f:\n",
    "    corrected_labels = json.load(f)\n",
    "\n",
    "# 2. Create a high-speed lookup dictionary\n",
    "# We use 'tweet_id' as the key.\n",
    "# CRITICAL: We convert keys to strings to ensure matching works reliably.\n",
    "correction_map = {\n",
    "    str(item['tweet_id']): item['sentiment']\n",
    "    for item in corrected_labels\n",
    "    # Optional: Uncomment below if you only want to apply items marked 'corrected'\n",
    "    # if item['status'] == 'corrected'\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(correction_map)} corrections.\")\n",
    "\n",
    "# 3. Define the update function\n",
    "def apply_corrections(example):\n",
    "    # Ensure the dataset ID is also a string for comparison\n",
    "    t_id = str(example['tweet_id'])\n",
    "\n",
    "    if t_id in correction_map:\n",
    "        example['sentiment'] = correction_map[t_id]\n",
    "\n",
    "    return example\n",
    "\n",
    "# 4. Apply to the dataset\n",
    "# distinct=False ensures we don't accidentally drop duplicates if that's not intended\n",
    "updated_test_ds = test_ds.map(apply_corrections)\n",
    "\n",
    "# Verification: Check if it worked\n",
    "# Compare a specific ID you know changed, or just print the first few\n",
    "print(\"Update complete.\")\n",
    "print(updated_test_ds[0])"
   ],
   "id": "58ebd1c2c43dd600",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 corrections.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e83bf75dbb4644ff901873e7ccc4b7a2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update complete.\n",
      "{'tweet_id': '1904997744181436599', 'time': '2025-03-26T20:43:50.000', 'author': '@ramakszl', 'content': 'dari sekian banyak demo yang begitu maraknya di setiap daerah tapi apa kebijakan mereka, yap betul bikin pengalihan isu soal selangakan pejabat rakyat demo tu di simak pukimak, rakyat gak setolol wapres #TolakRUUTNI #TolakRUUPolri #IndonesiaGelap', 'comment_count': 0, 'repost_count': 0, 'like_count': 0, 'view_count': 53, 'relevant': True, 'sentiment': 0}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T14:49:42.044003Z",
     "start_time": "2025-11-22T14:49:25.909254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset[\"test_sentiment\"] = updated_test_ds\n",
    "dataset.push_to_hub(\"tianharjuno/twitter-parse\", commit_message=\"Fixed test_ds with clustering\")"
   ],
   "id": "db0d2f07da3c2ea5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15125971bd8c41e78983ecf68bdd6b93"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/202 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53fbb4695a9e4dd0a13803ca8fbd0900"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1af5f1dace046b6a9dd264460b5e5bc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46ae90003c4144ed9109352eb1131b87"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6933adb69437464490eba9d2105908e6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/196 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a28e8c7fefd944eda7282e7748ad61e5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58964b5043254d2ab6af25c3e130dc8c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6918479beae40f095245890ffdddeb3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d9bce2d41d248ccb04c43173cdfe070"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b86cf3b5977343a6beb97d17bccafce8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9a7ceb02f2b4999a4974ffaa14096a2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/248 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "290e0763276244379393f29b88396f34"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "718768133210447482c4654e4692d6c8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04bb09cdaf9d476187db76b56f806116"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c2a2a96cfd94fa3b5d7d1784f72c4f3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce4dd4fcea594f6bbd9ed29fda93ab71"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tianharjuno/twitter-parse/commit/76a5543d25ca67ee8aa54bc45d8ec22b3f78d192', commit_message='Fixed test_ds with clustering', commit_description='', oid='76a5543d25ca67ee8aa54bc45d8ec22b3f78d192', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tianharjuno/twitter-parse', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tianharjuno/twitter-parse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1dc2941471812adb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
