{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "train the pipeline, with a larger amount\n",
    "\n",
    "mix in the training data with the labeled training data\n",
    "\n",
    "then we can retrieve more samples of a certain type\n",
    "\n",
    "in this case, we only care about relevant samples"
   ],
   "id": "f167067f8ab8d1e8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-21T01:57:52.260354Z",
     "start_time": "2025-11-21T01:57:47.988180Z"
    }
   },
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "class_labels = ClassLabel(3, [\"Negative\", \"Neutral\", \"Positive\"])\n",
    "dataset = load_dataset(\"tianharjuno/twitter-parse\", cache_dir=\"cache/\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:57:52.503477Z",
     "start_time": "2025-11-21T01:57:52.477845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re, unicodedata, jaconv, emoji\n",
    "\n",
    "_URL      = re.compile(r'https?://\\S+')\n",
    "_MENTION  = re.compile(r'@\\w+')\n",
    "_WS       = re.compile(r'\\s+')\n",
    "_KUTI_CUT = re.compile(r'(?i)kutipan.*$', re.DOTALL)\n",
    "\n",
    "# --- (MODIFIED) ---\n",
    "# Catches \"word\" + \"dari\" + \"domain.com\" -> replaces with \"word\"\n",
    "# Changed \\w+ to \\S+ to include punctuation like '!'\n",
    "_DARI_URL_ATTACHED = re.compile(r'(\\S+)dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# Catches \" dari \" + \"domain.com\" -> replaces with empty string\n",
    "_DARI_URL_SPACED = re.compile(r'\\s+dari\\s+([a-z0-9.-]+\\.[a-z]{2,})\\b', re.I)\n",
    "\n",
    "# --- (NEW) ---\n",
    "# Catches any word ending in \"dari\" (e.g., \"anarko!dari\", \"negaradari\")\n",
    "_DARI_STUCK = re.compile(r'(\\S+)dari\\b', re.I)\n",
    "\n",
    "def cleantext(row: str):\n",
    "    text = row[\"content\"] #type: ignore\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)\n",
    "    text = text.replace(\"tanya grok\", \" \")\n",
    "    text = text.replace(\"grokproductivitypasang\", \" \")\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "\n",
    "    # Handle standard URLs first\n",
    "    text = _URL.sub(' <url> ', text)\n",
    "    text = text.replace('ini tidak tersedia', ' ')\n",
    "\n",
    "    text = _MENTION.sub('@USER', text)\n",
    "    text = re.sub(r'^rt\\s+', '', text, flags=re.I)\n",
    "    text = re.sub(r'(\\b\\d{4})(?=[a-zA-Z])', r'\\1 ', text)\n",
    "    text = _KUTI_CUT.sub('', text)\n",
    "\n",
    "    # text = _DARI_URL_ATTACHED.sub(r'\\1', text)\n",
    "    # text = _DARI_URL_SPACED.sub('', text)\n",
    "    # text = _DARI_STUCK.sub(r'\\1', text)\n",
    "\n",
    "    text = _WS.sub(' ', text).strip()\n",
    "    row[\"content\"] = text #type: ignore\n",
    "    return row"
   ],
   "id": "45357000d90ce985",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:57:55.954732Z",
     "start_time": "2025-11-21T01:57:52.624472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "source_ds = dataset[\"source_labeled\"]\n",
    "source_ds = source_ds.map(cleantext, num_proc=16)\n",
    "\n",
    "relevant_ds = source_ds.filter(lambda ex: ex[\"relevant\"] == True)\n",
    "relevant_df = relevant_ds.to_pandas()\n",
    "relevant_df = relevant_df.drop_duplicates(subset=\"content\", keep=\"first\").reset_index(drop=True)\n",
    "relevant_ds = Dataset.from_pandas(relevant_df)\n",
    "\n",
    "train_ds = dataset[\"train_sentiment\"]\n",
    "test_ds = dataset[\"test_sentiment\"]\n",
    "\n",
    "train_ids = set(train_ds[\"content\"])\n",
    "test_ids = set(test_ds[\"content\"])\n",
    "\n",
    "filtered = relevant_ds.filter(\n",
    "    lambda ex: ex[\"content\"] not in train_ids and ex[\"content\"] not in test_ids\n",
    ")\n",
    "\n",
    "dataset_list = []\n",
    "\n",
    "dataset_list.append(train_ds)\n",
    "unlabeled_ds = filtered.train_test_split(train_size=50000, seed=42)[\"train\"]\n",
    "def reset_sentiment(row):\n",
    "    row[\"sentiment\"] = -1\n",
    "    return row\n",
    "unlabeled_ds = unlabeled_ds.map(reset_sentiment)\n",
    "dataset_list.append(unlabeled_ds)\n",
    "\n",
    "concat_ds = concatenate_datasets(dataset_list)"
   ],
   "id": "533ef990226f1992",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/113876 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b155320c5a14cf3bda16be8766504a8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "213f530737914290867fc9e528fe4d60"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:57:57.960920Z",
     "start_time": "2025-11-21T01:57:57.943125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "# We are NOT importing replace_word_elongation anymore\n",
    "from indoNLP.preprocessing import emoji_to_words\n",
    "\n",
    "def clean_tweet_for_nusabert(row):\n",
    "    text = row['content']\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # 3. Remove mentions and RT\n",
    "    # This regex is safe and does not affect 'ruu tni'\n",
    "    text = re.sub(r'rt @\\S+|@\\S+', '', text)\n",
    "\n",
    "    # 4. Remove hashtags (keep the word)\n",
    "    text = re.sub(r'#(\\S+)', r'\\1', text)\n",
    "\n",
    "    # 5. Convert emojis to words (Preserves sentiment)\n",
    "    text = emoji_to_words(text)\n",
    "\n",
    "    # 6. Normalize word elongation (CUSTOM, SAFER REGEX)\n",
    "    # This replaces 3 or more repeated chars (e.g., 'bangeeet' -> 'banget')\n",
    "    # It will NOT affect 'uu' or 'ruu', fixing your bug.\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # 7. Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    row[\"content\"] = text\n",
    "    return row"
   ],
   "id": "761a1de40da00074",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:57:58.146275Z",
     "start_time": "2025-11-21T01:57:58.089078Z"
    }
   },
   "cell_type": "code",
   "source": "concat_ds = concat_ds.map(clean_tweet_for_nusabert, num_proc=16)",
   "id": "11116b0d48fb54e1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T02:00:07.754198Z",
     "start_time": "2025-11-21T01:57:58.273748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_transformer = SentenceTransformer(\"LazarusNLP/all-nusabert-large-v4\",\n",
    "                                           cache_folder=\"/data/cache/\",\n",
    "                                           device=\"cuda\",\n",
    "                                           )\n",
    "embeddings = sentence_transformer.encode(concat_ds[\"content\"], show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True, device=\"cuda\", batch_size=128)"
   ],
   "id": "4a82b92642fc11ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/625 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3cd87ecde774963abbbaa84bd91b3fb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T02:14:11.864248Z",
     "start_time": "2025-11-21T02:01:36.703444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dream_pipeline import DreamCluster\n",
    "clusterer = DreamCluster(\"stability\")\n",
    "clusterer.fit(embeddings)"
   ],
   "id": "7e9d5e91f51f4d89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding intrinsic dimension with TwoNN\n",
      "Found intrinsic dimension: 20\n",
      "Running first stage reduction\n",
      "Running second stage reduction\n",
      "Tuning HDBSCAN with mode stability\n",
      "  -> Using 'stability' mode (Bayesian Optimization on relative_validity)\n",
      "Transforming full dataset with trained reducers...\n",
      "Fitting final clusterer on full reduced dataset...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.int64(146), np.int64(32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T02:14:45.032703Z",
     "start_time": "2025-11-21T02:14:39.834899Z"
    }
   },
   "cell_type": "code",
   "source": "cluster_labels, cluster_probabilities, reduced_embeddings = clusterer.predict(embeddings=embeddings)",
   "id": "45e64220fe36933d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T02:18:45.742102Z",
     "start_time": "2025-11-21T02:18:45.526585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "unique_labels, unique_label_counts = np.unique(cluster_labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, unique_label_counts):\n",
    "    indices = np.where(cluster_labels == label)[0]\n",
    "    cluster_ds = concat_ds.select(indices)\n",
    "    cluster_classes, cluster_class_counts = np.unique(cluster_ds[\"sentiment\"], return_counts=True)\n",
    "    print(f\"CLUSTER {label}: {dict(zip(cluster_classes, cluster_class_counts))}\")"
   ],
   "id": "11f91f9eae01aa52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER -1: {np.int64(-1): np.int64(7318), np.int64(0): np.int64(3771), np.int64(1): np.int64(391), np.int64(2): np.int64(184)}\n",
      "CLUSTER 0: {np.int64(-1): np.int64(260), np.int64(1): np.int64(17), np.int64(2): np.int64(165)}\n",
      "CLUSTER 1: {np.int64(-1): np.int64(385), np.int64(0): np.int64(2), np.int64(1): np.int64(12), np.int64(2): np.int64(212)}\n",
      "CLUSTER 2: {np.int64(-1): np.int64(451), np.int64(0): np.int64(244), np.int64(1): np.int64(6), np.int64(2): np.int64(1)}\n",
      "CLUSTER 3: {np.int64(-1): np.int64(195), np.int64(1): np.int64(2), np.int64(2): np.int64(108)}\n",
      "CLUSTER 4: {np.int64(-1): np.int64(142), np.int64(0): np.int64(90), np.int64(1): np.int64(4)}\n",
      "CLUSTER 5: {np.int64(-1): np.int64(1677), np.int64(0): np.int64(777), np.int64(1): np.int64(138), np.int64(2): np.int64(84)}\n",
      "CLUSTER 6: {np.int64(-1): np.int64(370), np.int64(0): np.int64(234), np.int64(1): np.int64(9), np.int64(2): np.int64(3)}\n",
      "CLUSTER 7: {np.int64(-1): np.int64(89), np.int64(0): np.int64(28), np.int64(1): np.int64(27), np.int64(2): np.int64(7)}\n",
      "CLUSTER 8: {np.int64(-1): np.int64(561), np.int64(0): np.int64(151), np.int64(1): np.int64(120), np.int64(2): np.int64(9)}\n",
      "CLUSTER 9: {np.int64(-1): np.int64(159), np.int64(0): np.int64(89), np.int64(1): np.int64(3)}\n",
      "CLUSTER 10: {np.int64(-1): np.int64(214), np.int64(0): np.int64(98), np.int64(1): np.int64(1)}\n",
      "CLUSTER 11: {np.int64(-1): np.int64(134), np.int64(0): np.int64(1), np.int64(1): np.int64(20), np.int64(2): np.int64(56)}\n",
      "CLUSTER 12: {np.int64(-1): np.int64(3835), np.int64(0): np.int64(2089), np.int64(1): np.int64(107), np.int64(2): np.int64(59)}\n",
      "CLUSTER 13: {np.int64(-1): np.int64(1378), np.int64(0): np.int64(774), np.int64(1): np.int64(26), np.int64(2): np.int64(8)}\n",
      "CLUSTER 14: {np.int64(-1): np.int64(112), np.int64(0): np.int64(48), np.int64(1): np.int64(26), np.int64(2): np.int64(4)}\n",
      "CLUSTER 15: {np.int64(-1): np.int64(299), np.int64(0): np.int64(5), np.int64(1): np.int64(19), np.int64(2): np.int64(149)}\n",
      "CLUSTER 16: {np.int64(-1): np.int64(498), np.int64(0): np.int64(9), np.int64(1): np.int64(51), np.int64(2): np.int64(283)}\n",
      "CLUSTER 17: {np.int64(-1): np.int64(135), np.int64(0): np.int64(83), np.int64(1): np.int64(4), np.int64(2): np.int64(1)}\n",
      "CLUSTER 18: {np.int64(-1): np.int64(334), np.int64(0): np.int64(217), np.int64(1): np.int64(3)}\n",
      "CLUSTER 19: {np.int64(-1): np.int64(5565), np.int64(0): np.int64(3286), np.int64(1): np.int64(12), np.int64(2): np.int64(3)}\n",
      "CLUSTER 20: {np.int64(-1): np.int64(85), np.int64(1): np.int64(5), np.int64(2): np.int64(58)}\n",
      "CLUSTER 21: {np.int64(-1): np.int64(422), np.int64(0): np.int64(275), np.int64(1): np.int64(1)}\n",
      "CLUSTER 22: {np.int64(-1): np.int64(259), np.int64(0): np.int64(2), np.int64(1): np.int64(6), np.int64(2): np.int64(136)}\n",
      "CLUSTER 23: {np.int64(-1): np.int64(201), np.int64(0): np.int64(11), np.int64(1): np.int64(47), np.int64(2): np.int64(80)}\n",
      "CLUSTER 24: {np.int64(-1): np.int64(18800), np.int64(0): np.int64(5633), np.int64(1): np.int64(4707), np.int64(2): np.int64(968)}\n",
      "CLUSTER 25: {np.int64(-1): np.int64(330), np.int64(0): np.int64(240), np.int64(1): np.int64(1)}\n",
      "CLUSTER 26: {np.int64(-1): np.int64(214), np.int64(0): np.int64(150), np.int64(1): np.int64(1), np.int64(2): np.int64(1)}\n",
      "CLUSTER 27: {np.int64(-1): np.int64(3455), np.int64(0): np.int64(1646), np.int64(1): np.int64(293), np.int64(2): np.int64(144)}\n",
      "CLUSTER 28: {np.int64(-1): np.int64(411), np.int64(0): np.int64(218), np.int64(1): np.int64(30), np.int64(2): np.int64(21)}\n",
      "CLUSTER 29: {np.int64(-1): np.int64(97), np.int64(0): np.int64(36), np.int64(1): np.int64(25), np.int64(2): np.int64(1)}\n",
      "CLUSTER 30: {np.int64(-1): np.int64(1415), np.int64(0): np.int64(535), np.int64(1): np.int64(159), np.int64(2): np.int64(137)}\n",
      "CLUSTER 31: {np.int64(-1): np.int64(200), np.int64(0): np.int64(51), np.int64(1): np.int64(44), np.int64(2): np.int64(8)}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T02:45:27.880095Z",
     "start_time": "2025-11-21T02:45:27.862411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def build_labeling_json(concat_ds, cluster_labels, cluster_probabilities, output_path):\n",
    "    \"\"\"\n",
    "    Build labeling JSON for HTML labeling suite.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    concat_ds : HuggingFace Dataset\n",
    "        Must contain columns: tweet_id, time, author, content, sentiment\n",
    "    cluster_labels : Sequence[int]\n",
    "        Cluster assignment per sample\n",
    "    cluster_probabilities : Sequence[float]\n",
    "        Cluster probability per sample\n",
    "    output_path : str\n",
    "        Where to save the output JSON file\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to Python lists for safety\n",
    "    cluster_labels = list(cluster_labels)\n",
    "    cluster_probabilities = list(cluster_probabilities)\n",
    "\n",
    "    # Basic checks\n",
    "    assert len(concat_ds) == len(cluster_labels)\n",
    "    assert len(concat_ds) == len(cluster_probabilities)\n",
    "\n",
    "    # Organize samples by cluster\n",
    "    clusters = defaultdict(list)\n",
    "    for idx, row in enumerate(concat_ds):\n",
    "        cid = int(cluster_labels[idx])\n",
    "        prob = float(cluster_probabilities[idx])\n",
    "\n",
    "        original_label = int(row[\"sentiment\"])\n",
    "        is_unlabeled = original_label == -1\n",
    "\n",
    "        sample = {\n",
    "            \"tweet_id\": row[\"tweet_id\"],\n",
    "            \"content\": row[\"content\"],\n",
    "            \"author\": row.get(\"author\", None),\n",
    "            \"time\": row.get(\"time\", None),\n",
    "\n",
    "            \"cluster_probability\": prob,\n",
    "            \"original_label\": original_label,\n",
    "            \"current_label\": original_label,\n",
    "            \"is_unlabeled\": is_unlabeled,\n",
    "            \"needs_check\": False  # Will fill below\n",
    "        }\n",
    "\n",
    "        clusters[cid].append(sample)\n",
    "\n",
    "    # Build cluster stats + mark minority labels\n",
    "    cluster_blocks = []\n",
    "    for cid, samples in clusters.items():\n",
    "        # Collect stats\n",
    "        label_counts = Counter(s[\"original_label\"] for s in samples if s[\"original_label\"] != -1)\n",
    "        size = len(samples)\n",
    "\n",
    "        # Determine \"dominant\" class for minority detection\n",
    "        if len(label_counts) > 0:\n",
    "            dominant_label, dominant_count = label_counts.most_common(1)[0]\n",
    "        else:\n",
    "            dominant_label, dominant_count = None, 0  # All unlabeled cluster\n",
    "\n",
    "        # Mark minority samples\n",
    "        for s in samples:\n",
    "            orig = s[\"original_label\"]\n",
    "            if orig != -1 and orig != dominant_label:\n",
    "                s[\"needs_check\"] = True\n",
    "\n",
    "        cluster_blocks.append({\n",
    "            \"cluster_id\": cid,\n",
    "            \"stats\": {\n",
    "                \"size\": size,\n",
    "                \"labels\": {str(k): v for k, v in label_counts.items()}\n",
    "            },\n",
    "            \"samples\": samples\n",
    "        })\n",
    "\n",
    "    # Compose final JSON\n",
    "    output_json = {\n",
    "        \"metadata\": {\n",
    "            \"version\": 1,\n",
    "            \"generated_at\": datetime.utcnow().isoformat(),\n",
    "            \"num_clusters\": len(cluster_blocks),\n",
    "            \"num_samples\": len(concat_ds),\n",
    "\n",
    "            \"label_schema\": {\n",
    "                \"-1\": \"Unlabeled\",\n",
    "                \"0\": \"Negative\",\n",
    "                \"1\": \"Neutral\",\n",
    "                \"2\": \"Positive\"\n",
    "            }\n",
    "        },\n",
    "        \"clusters\": cluster_blocks\n",
    "    }\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[OK] Labeling JSON exported to: {output_path}\")\n"
   ],
   "id": "f3472b40e9ca7778",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T02:45:31.454125Z",
     "start_time": "2025-11-21T02:45:28.853275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "build_labeling_json(\n",
    "    concat_ds=concat_ds,\n",
    "    cluster_labels=cluster_labels,\n",
    "    cluster_probabilities=cluster_probabilities,\n",
    "    output_path=\"labeling_data.json\"\n",
    ")"
   ],
   "id": "3c273516349e88aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Labeling JSON exported to: labeling_data.json\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
