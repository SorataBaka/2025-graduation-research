{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34971a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstanfordnlp/sst2\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./datasets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\", cache_dir='./datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dec726",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = dataset[\"train\"]\n",
    "validation_dataset = dataset[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bbf12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_sentence = training_dataset['sentence']\n",
    "training_dataset_label = training_dataset['label']\n",
    "validation_dataset_sentence = validation_dataset['sentence']\n",
    "validation_dataset_label = validation_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "training_dataframe = pd.DataFrame.from_dict({\n",
    "  \"sentence\": training_dataset_sentence,\n",
    "  \"label\": training_dataset_label\n",
    "})\n",
    "validation_dataframe = pd.DataFrame.from_dict({\n",
    "  \"sentence\": validation_dataset_sentence,\n",
    "  \"label\": validation_dataset_label\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "def cleandata(text):\n",
    "  soup = BeautifulSoup(text)\n",
    "  text = soup.getText()\n",
    "  text = unicodedata.normalize(\"NFKC\", text)\n",
    "  text = re.sub(r\"[\\u0000-\\u001F\\u007F]+\", \"\", text)\n",
    "  text.strip()\n",
    "  return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataframe['clean'] = training_dataframe['sentence'].apply(cleandata).tolist()\n",
    "validation_dataframe['clean'] = validation_dataframe['sentence'].apply(cleandata).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=\"./tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e92b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(training_dataframe['clean']))\n",
    "training_encoded = tokenizer(\n",
    "  training_dataframe['clean'].tolist(),\n",
    "  padding=True,\n",
    "  truncation=True,\n",
    "  max_length=128,\n",
    "  return_tensors='tf'\n",
    ")\n",
    "validation_encoded = tokenizer(\n",
    "  validation_dataframe['clean'].tolist(),\n",
    "  padding=True,\n",
    "  truncation=True,\n",
    "  max_length=128,\n",
    "  return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa077ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "print(training_dataframe['sentence'][index])\n",
    "print(training_encoded[\"input_ids\"][index])\n",
    "print(training_encoded['token_type_ids'][index])\n",
    "print(training_encoded['attention_mask'][index])\n",
    "print(tokenizer.decode(training_encoded[\"input_ids\"][index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=\"./model\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 32\n",
    "epochs=4\n",
    "train_data_size = len(training_dataset)\n",
    "steps_per_epoch = train_data_size // batch_size\n",
    "num_train_steps = steps_per_epoch * epochs \n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "  init_lr=2e-5,\n",
    "  num_train_steps= num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  weight_decay_rate=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df794031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "model.compile(\n",
    "  optimizer=optimizer,\n",
    "  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_keras.callbacks import TensorBoard, ModelCheckpoint, BackupAndRestore, RemoteMonitor\n",
    "tensorboard_callbacks = TensorBoard(\n",
    "  log_dir=\"./logs\", histogram_freq=1, write_graph=True,write_images=True,write_steps_per_second=True,update_freq=1\n",
    "   \n",
    ")\n",
    "model_callbacks = ModelCheckpoint(\n",
    "  filepath=\"./assets/checkpoint.keras\",\n",
    "  save_best_only=True,\n",
    "  monitor=\"val_accuracy\",\n",
    "  mode=\"max\"\n",
    ")\n",
    "backup_callbacks = BackupAndRestore(\n",
    "  backup_dir=\"./backup/\",\n",
    "  save_freq=50,\n",
    "  save_before_preemption=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71773013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def batch_to_tf(encodings, labels):\n",
    "    # Convert BatchEncoding to dict of NumPy arrays\n",
    "    encodings['labels'] = tf.convert_to_tensor(labels)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    return dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cd309",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tf = batch_to_tf(training_encoded, training_dataframe[\"label\"])\n",
    "validation_tf = batch_to_tf(validation_encoded, validation_dataframe['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "  training_tf,\n",
    "  validation_data=validation_tf,\n",
    "  epochs=3,\n",
    "  batch_size=32,\n",
    "  callbacks=[backup_callbacks, tensorboard_callbacks]\n",
    ")\n",
    "model.save_pretrained(\"./pretrained/model\")\n",
    "tokenizer.save_pretrained(\"./pretrained/tokenizer\")\n",
    "model.save(\"./pretrained/all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "message = \"initial commit\"\n",
    "REPOSITORY_ID=\"tianharjuno/sst2-bert-training\"\n",
    "upload_folder(\n",
    "  folder_path=\"./pretrained/model\",\n",
    "  repo_id=REPOSITORY_ID,\n",
    "  commit_message=message\n",
    ")\n",
    "upload_folder(\n",
    "  folder_path=\"./pretrained/tokenizer\",\n",
    "  repo_id=REPOSITORY_ID,\n",
    "  commit_message=message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e86dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "test_dataset = dataset[\"test\"]\n",
    "test_df = pd.DataFrame.from_dict({\n",
    "  \"sentence\" : test_dataset['sentence'],\n",
    "  \"label\": test_dataset['label']\n",
    "})\n",
    "test_df['cleaned'] = test_df['sentence'].apply(cleandata).tolist()\n",
    "inputs = tokenizer(\n",
    "  test_df[\"cleaned\"].tolist(),\n",
    "  return_tensors='tf',\n",
    "  padding=True,\n",
    "  truncation=True\n",
    ")\n",
    "test_tf = tf.data.Dataset.from_tensor_slices((\n",
    "  inputs,\n",
    "  test_df[\"label\"])).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "logits = outputs.logits\n",
    "results = tf.argmax(logits, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viewPrediction(index):\n",
    "  print(f'{results[index]}: {test_df[\"sentence\"][index]}')\n",
    "  \n",
    "for i in range(50):\n",
    "  viewPrediction(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer, pipeline\n",
    "\n",
    "load_pretrained_model = TFBertForSequenceClassification.from_pretrained(\"tianharjuno/sst2-bert-training\", cache_dir=\"./model\")\n",
    "load_pretrained_tokenizer = BertTokenizer.from_pretrained(\"tianharjuno/sst2-bert-training\", cache_dir=\"./tokenizer\")\n",
    "nlp = pipeline(\"text-classification\", model=load_pretrained_model, tokenizer=load_pretrained_tokenizer, framework='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381451ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = nlp.predict(\"i would not date you even if you become a princess\")\n",
    "print(pipeline_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996a401",
   "metadata": {},
   "source": [
    "trained tokenize and model are saved to huggingface.co\n",
    "\n",
    "tianharjuno/sst2-bert-training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
