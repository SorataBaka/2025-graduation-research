{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f2281a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\", cache_dir=\"./datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa745208",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY_ID=\"tianharjuno/imdb-bert-training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1598b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = ds[\"train\"]\n",
    "validation_split = ds[\"test\"]\n",
    "test_split = ds[\"unsupervised\"]\n",
    "\n",
    "def printline(index):\n",
    "  print(training_split[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa0a598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = training_split[\"text\"]\n",
    "validation_sentences = validation_split[\"text\"]\n",
    "training_labels = training_split[\"label\"]\n",
    "validation_labels = validation_split['label']\n",
    "\n",
    "sentences = training_sentences + validation_sentences\n",
    "labels = training_labels + validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd55e547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence total: 50000\n",
      "Label total: 50000\n",
      "Training total: 35000\n",
      "Validation total: 15000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "training_sentence, validation_sentence, training_label, validation_label = train_test_split(\n",
    "  sentences, labels, train_size=0.7, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Sentence total: {len(sentences)}\\nLabel total: {len(labels)}\\nTraining total: {len(training_sentence)}\\nValidation total: {len(validation_sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10d6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_df = pd.DataFrame.from_dict({\n",
    "  \"text\": training_sentence,\n",
    "  \"label\": training_label\n",
    "})\n",
    "validation_df = pd.DataFrame.from_dict({\n",
    "  \"text\": validation_sentence,\n",
    "  \"label\": validation_label\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c86df77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def cleanParagraph(text):\n",
    "  soup = BeautifulSoup(text)\n",
    "  innertext = soup.getText()\n",
    "  innertext = re.sub(r'<[^>]+>', '', innertext)\n",
    "  innertext.strip()\n",
    "  return str(innertext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5aff9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0  This is a hilarious film. Burt Reynolds is a N...      1   \n",
      "1  This is the second Animatrix short, and the fi...      1   \n",
      "2  I really wanted to like this, but in the end i...      0   \n",
      "3  By no means is this movie as bad as 'Perfect S...      0   \n",
      "4  I didn't expect much when I first saw the DVD ...      1   \n",
      "\n",
      "                                             cleaned  \n",
      "0  This is a hilarious film. Burt Reynolds is a N...  \n",
      "1  This is the second Animatrix short, and the fi...  \n",
      "2  I really wanted to like this, but in the end i...  \n",
      "3  By no means is this movie as bad as 'Perfect S...  \n",
      "4  I didn't expect much when I first saw the DVD ...  \n",
      "                                                text  label  \\\n",
      "0  I'm afraid I must disagree with Mr. Radcliffe,...      1   \n",
      "1  Homegrown is one of those movies which sort of...      1   \n",
      "2  Michael Keaton has really never been a good ac...      0   \n",
      "3  The film \"Cross Eyed\" by Adam Jones propels th...      1   \n",
      "4  More suspenseful, more subtle, much, much more...      0   \n",
      "\n",
      "                                             cleaned  \n",
      "0  I'm afraid I must disagree with Mr. Radcliffe,...  \n",
      "1  Homegrown is one of those movies which sort of...  \n",
      "2  Michael Keaton has really never been a good ac...  \n",
      "3  The film \"Cross Eyed\" by Adam Jones propels th...  \n",
      "4  More suspenseful, more subtle, much, much more...  \n"
     ]
    }
   ],
   "source": [
    "training_df[\"cleaned\"] = training_df[\"text\"].apply(cleanParagraph).tolist()\n",
    "validation_df[\"cleaned\"] = validation_df[\"text\"].apply(cleanParagraph).tolist()\n",
    "print(training_df.head())\n",
    "print(validation_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2511bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", cache_dir=\"./cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41300de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 23:50:36.605281: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2025-05-03 23:50:36.605310: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-05-03 23:50:36.605315: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-05-03 23:50:36.605332: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-03 23:50:36.605343: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "training_encoded = tokenizer(\n",
    "  training_df['cleaned'].to_list(),\n",
    "  padding=True,\n",
    "  truncation=True,\n",
    "  max_length=256,\n",
    "  verbose=True,\n",
    "  return_tensors=\"tf\"\n",
    ")\n",
    "validation_encoded = tokenizer(\n",
    "  validation_df[\"cleaned\"].to_list(),\n",
    "  padding=True,\n",
    "  truncation=True,\n",
    "  max_length=256,\n",
    "  verbose=True,\n",
    "  return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa415eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEncoded(index):\n",
    "  print(training_df[\"cleaned\"][index])\n",
    "  print(training_encoded[\"input_ids\"][index])\n",
    "  print(tokenizer.decode(training_encoded[\"input_ids\"][index]))\n",
    "  print(training_encoded[\"attention_mask\"][index])\n",
    "  print(training_df[\"label\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02f882fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the second Animatrix short, and the first of them to be what one could call 'artistic'. It contains a lot of references, metaphors and symbols in the dense amount of material, especially with a running time of 9 minutes. I've heard some complaints that this is \"anti-human\", or tries to direct hate towards man, for their \"sins against machine\". I don't think that's true; it merely uses the robots to show us, that as humans, we aren't particularly accepting or open-minded towards anyone different from ourselves. I'd say it does a great job of that. The plot is good... it plays as a historical document, recounting what led to one of the main conflicts in the trilogy. Thus it holds clips from fictional news reports and the like. The voice acting is very good, if there is not a lot of it. The animation is nice, and the use of color, in spite of the usually realistic drawing style, makes it more open to do the smooth transitions and other surreal imagery. This has several bits of strong violence and disturbing visuals, as well as a little nudity. The disc holds a commentary, not in English but subtitled, and worth a listen/read. There is also a well-done and informative making of, based on both parts, so I would advise watching it after seeing the next one, as well. I recommend this to anyone who enjoys the Matrix universe, and/or science fiction in general. 8/10\n",
      "tf.Tensor(\n",
      "[  101  2023  2003  1996  2117  2019  9581 29184  2460  1010  1998  1996\n",
      "  2034  1997  2068  2000  2022  2054  2028  2071  2655  1005  6018  1005\n",
      "  1012  2009  3397  1037  2843  1997  7604  1010 19240  2015  1998  9255\n",
      "  1999  1996  9742  3815  1997  3430  1010  2926  2007  1037  2770  2051\n",
      "  1997  1023  2781  1012  1045  1005  2310  2657  2070 10821  2008  2023\n",
      "  2003  1000  3424  1011  2529  1000  1010  2030  5363  2000  3622  5223\n",
      "  2875  2158  1010  2005  2037  1000 15516  2114  3698  1000  1012  1045\n",
      "  2123  1005  1056  2228  2008  1005  1055  2995  1025  2009  6414  3594\n",
      "  1996 13507  2000  2265  2149  1010  2008  2004  4286  1010  2057  4995\n",
      "  1005  1056  3391 10564  2030  2330  1011 13128  2875  3087  2367  2013\n",
      "  9731  1012  1045  1005  1040  2360  2009  2515  1037  2307  3105  1997\n",
      "  2008  1012  1996  5436  2003  2204  1012  1012  1012  2009  3248  2004\n",
      "  1037  3439  6254  1010 28667 21723  2075  2054  2419  2000  2028  1997\n",
      "  1996  2364  9755  1999  1996 11544  1012  2947  2009  4324 15281  2013\n",
      "  7214  2739  4311  1998  1996  2066  1012  1996  2376  3772  2003  2200\n",
      "  2204  1010  2065  2045  2003  2025  1037  2843  1997  2009  1012  1996\n",
      "  7284  2003  3835  1010  1998  1996  2224  1997  3609  1010  1999  8741\n",
      "  1997  1996  2788 12689  5059  2806  1010  3084  2009  2062  2330  2000\n",
      "  2079  1996  5744 22166  1998  2060 16524 13425  1012  2023  2038  2195\n",
      "  9017  1997  2844  4808  1998 14888 26749  1010  2004  2092  2004  1037\n",
      "  2210 16371 25469  1012  1996  5860  4324  1037  8570  1010  2025  1999\n",
      "  2394  2021  4942   102], shape=(256,), dtype=int32)\n",
      "[CLS] this is the second animatrix short, and the first of them to be what one could call ' artistic '. it contains a lot of references, metaphors and symbols in the dense amount of material, especially with a running time of 9 minutes. i ' ve heard some complaints that this is \" anti - human \", or tries to direct hate towards man, for their \" sins against machine \". i don ' t think that ' s true ; it merely uses the robots to show us, that as humans, we aren ' t particularly accepting or open - minded towards anyone different from ourselves. i ' d say it does a great job of that. the plot is good... it plays as a historical document, recounting what led to one of the main conflicts in the trilogy. thus it holds clips from fictional news reports and the like. the voice acting is very good, if there is not a lot of it. the animation is nice, and the use of color, in spite of the usually realistic drawing style, makes it more open to do the smooth transitions and other surreal imagery. this has several bits of strong violence and disturbing visuals, as well as a little nudity. the disc holds a commentary, not in english but sub [SEP]\n",
      "tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(256,), dtype=int32)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "printEncoded(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04ae65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def batch_to_tf(encodings, labels):\n",
    "    # Convert BatchEncoding to dict of NumPy arrays\n",
    "    encodings['labels'] = tf.convert_to_tensor(labels)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    return dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "  \n",
    "training_tf = batch_to_tf(training_encoded, training_df['label'])\n",
    "validation_tf = batch_to_tf(validation_encoded, validation_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a99666fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e15b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 32\n",
    "epochs=3\n",
    "train_data_size = len(training_df)\n",
    "steps_per_epoch = train_data_size // batch_size\n",
    "num_train_steps = steps_per_epoch * epochs \n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "  init_lr=2e-5,\n",
    "  num_train_steps= num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  weight_decay_rate=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b52f938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "model.compile(\n",
    "  optimizer=optimizer,\n",
    "  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77b28588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_keras.callbacks import TensorBoard, ModelCheckpoint, BackupAndRestore, RemoteMonitor\n",
    "tensorboard_callbacks = TensorBoard(\n",
    "  log_dir=\"./logs\", histogram_freq=1, write_graph=True,write_images=True,write_steps_per_second=True,update_freq=1\n",
    "   \n",
    ")\n",
    "backup_callbacks = BackupAndRestore(\n",
    "  backup_dir=\"./backup\",\n",
    "  save_freq=20,\n",
    "  save_before_preemption=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9b86d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "1094/1094 [==============================] - 1283s 1s/step - loss: 0.0975 - accuracy: 0.9844 - val_loss: 0.0913 - val_accuracy: 0.9721\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  training_tf,\n",
    "  validation_data=training_tf,\n",
    "  epochs=3,\n",
    "  batch_size=32,\n",
    "  callbacks=[backup_callbacks, tensorboard_callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f022a973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./pretrained_imdb/all/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./pretrained_imdb/all/assets\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./pretrained_imdb/model\")\n",
    "tokenizer.save_pretrained(\"./pretrained_imdb/tokenizer\")\n",
    "model.save(\"./pretrained_imdb/all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ad5119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf_model.h5: 100%|██████████| 438M/438M [00:48<00:00, 8.99MB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tianharjuno/imdb-bert-training/commit/7c4859d676f432b0761dba81e44af3dcd512b4c3', commit_message='clean folder', commit_description='', oid='7c4859d676f432b0761dba81e44af3dcd512b4c3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tianharjuno/imdb-bert-training', endpoint='https://huggingface.co', repo_type='model', repo_id='tianharjuno/imdb-bert-training'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "message = \"clean folder\"\n",
    "upload_folder(\n",
    "  folder_path=\"./pretrained_imdb/model\",\n",
    "  repo_id=REPOSITORY_ID,\n",
    "  commit_message=message,\n",
    ")\n",
    "upload_folder(\n",
    "  folder_path=\"./pretrained_imdb/tokenizer\",\n",
    "  repo_id=REPOSITORY_ID,\n",
    "  commit_message=message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e6c2218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 11:46:30.817733: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2025-05-04 11:46:30.817763: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-05-04 11:46:30.817769: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-05-04 11:46:30.817921: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-04 11:46:30.817931: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at pretrained_imdb/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, pipeline\n",
    "trained_model = TFBertForSequenceClassification.from_pretrained(\"pretrained_imdb/model\")\n",
    "trained_tokenizer = BertTokenizer.from_pretrained(\"pretrained_imdb/tokenizer\")\n",
    "nlp = pipeline(task=\"text-classification\", model=trained_model, tokenizer=trained_tokenizer, framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "623a172f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9794296026229858}]\n"
     ]
    }
   ],
   "source": [
    "result = nlp(\"This movie is bad compared to the previous releases.\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
