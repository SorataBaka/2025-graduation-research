{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5e024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/sst2\", cache_dir=\"./datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6745c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET = ds[\"train\"]\n",
    "VALIDATION_DATASET = ds[\"validation\"]\n",
    "TEST_DATASET = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c39a3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 1, 'sentence': 'contains no wit , only labored gags ', 'label': 0}\n",
      "{'idx': 1, 'sentence': 'unflinchingly bleak and desperate ', 'label': 0}\n",
      "{'idx': 1, 'sentence': \"this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation .\", 'label': -1}\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "\n",
    "print(TRAINING_DATASET[idx])\n",
    "print(VALIDATION_DATASET[idx])\n",
    "print(TEST_DATASET[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfbcaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_sentence = TRAINING_DATASET[\"sentence\"]\n",
    "training_labels = TRAINING_DATASET[\"label\"]\n",
    "\n",
    "validation_sentence = VALIDATION_DATASET[\"sentence\"]\n",
    "validation_labels = VALIDATION_DATASET[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e35c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label\n",
      "0                                        much humor       1\n",
      "1                  an overstylized , puréed mélange       0\n",
      "2  the film 's hackneyed message is not helped by...      0\n",
      "3  is very choppy and monosyllabic despite the fact       0\n",
      "4                 more than a whiff of exploitation       0\n",
      "                                            sentence  label\n",
      "0                                , color and cringe       0\n",
      "1  not only of one man 's quest to be president ,...      1\n",
      "2  sends you away a believer again and quite chee...      1\n",
      "3                  this delicate coming-of-age tale       1\n",
      "4  airs just about every cliche in the war movie ...      0\n",
      "                                            sentence  label\n",
      "0             uneasy mishmash of styles and genres .     -1\n",
      "1  this film 's relationship to actual tension is...     -1\n",
      "2  by the end of no such thing the audience , lik...     -1\n",
      "3  director rob marshall went out gunning to make...     -1\n",
      "4  lathan and diggs have considerable personal ch...     -1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "sentence_concat = training_sentence + validation_sentence\n",
    "label_concat = training_labels + validation_labels\n",
    "\n",
    "sentence_training, sentence_validation, label_training, label_validation = train_test_split(sentence_concat, label_concat, test_size=0.15, random_state=100, stratify=label_concat)\n",
    "\n",
    "training_dataframe = pd.DataFrame.from_dict({\"sentence\": sentence_training, \"label\": label_training})\n",
    "validation_dataframe = pd.DataFrame.from_dict({\"sentence\": sentence_validation, \"label\": label_validation})\n",
    "test_dataframe = pd.DataFrame.from_dict({\"sentence\": TEST_DATASET[\"sentence\"], \"label\": TEST_DATASET[\"label\"]})\n",
    "\n",
    "print(training_dataframe.head())\n",
    "print(validation_dataframe.head())\n",
    "print(test_dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d4813bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def clean_text(text):\n",
    "  soup = BeautifulSoup(text)\n",
    "  textcleaned = soup.getText()\n",
    "  text = textcleaned.strip()\n",
    "  return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fbf225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                               much humor\n",
      "1                         an overstylized , puréed mélange\n",
      "2        the film 's hackneyed message is not helped by...\n",
      "3         is very choppy and monosyllabic despite the fact\n",
      "4                        more than a whiff of exploitation\n",
      "                               ...                        \n",
      "57982    what ( frei ) gives us ... is a man who uses t...\n",
      "57983                  could have guessed at the beginning\n",
      "57984    reveals the ways in which a sultry evening or ...\n",
      "57985    is also elevated by it -- the kind of movie th...\n",
      "57986    must be given to the water-camera operating te...\n",
      "Name: cleaned, Length: 57987, dtype: object\n"
     ]
    }
   ],
   "source": [
    "training_dataframe[\"cleaned\"] = training_dataframe[\"sentence\"].apply(clean_text).tolist()\n",
    "validation_dataframe[\"cleaned\"] = validation_dataframe[\"sentence\"].apply(clean_text).tolist()\n",
    "test_dataframe[\"cleaned\"] = test_dataframe[\"sentence\"].apply(clean_text).tolist()\n",
    "\n",
    "print(training_dataframe[\"cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9776bf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "label\n",
      "1    32311\n",
      "0    25676\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(set(training_dataframe['label']))\n",
    "print(training_dataframe['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca82a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f1aad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_training_encoded = tokenizer(\n",
    "  training_dataframe[\"cleaned\"].tolist(),\n",
    "  padding=True,\n",
    "  truncation=True,\n",
    "  return_tensors='tf',\n",
    ")\n",
    "sentence_validation_encoded = tokenizer(\n",
    "  validation_dataframe[\"cleaned\"].tolist(),\n",
    "  padding=True,\n",
    "  truncation=True,\n",
    "  return_tensors=\"tf\",\n",
    ")\n",
    "sentence_test_encoded = tokenizer(\n",
    "  test_dataframe[\"cleaned\"].tolist(),\n",
    "  padding=True,\n",
    "  truncation=True,\n",
    "  return_tensors=\"tf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb672106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exude an air of dignity that 's perfect for the proud warrior that still lingers in the souls of these characters\n",
      "tf.Tensor(\n",
      "[  101  4654 12672  2019  2250  1997 13372  2008  1005  1055  3819  2005\n",
      "  1996  7098  6750  2008  2145 26577  2015  1999  1996  9293  1997  2122\n",
      "  3494   102     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0], shape=(66,), dtype=int32)\n",
      "[CLS] exude an air of dignity that ' s perfect for the proud warrior that still lingers in the souls of these characters [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(66,), dtype=int32)\n",
      "1\n",
      "{'input_ids': <tf.Tensor: shape=(1821, 64), dtype=int32, numpy=\n",
      "array([[  101, 15491, 28616, ...,     0,     0,     0],\n",
      "       [  101,  2023,  2143, ...,     0,     0,     0],\n",
      "       [  101,  2011,  1996, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  4482,  2038, ...,     0,     0,     0],\n",
      "       [  101,  2009,  5836, ...,     0,     0,     0],\n",
      "       [  101, 28190,  1998, ...,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1821, 64), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1821, 64), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "print(training_dataframe[\"cleaned\"][k])\n",
    "print(sentence_training_encoded[\"input_ids\"][k])\n",
    "print(tokenizer.decode(sentence_training_encoded[\"input_ids\"][k]))\n",
    "print(sentence_training_encoded[\"attention_mask\"][k])\n",
    "print(training_dataframe[\"label\"][k])\n",
    "print(sentence_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "145b72db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f31de414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 32\n",
    "epochs=4\n",
    "train_data_size = len(training_dataframe)\n",
    "steps_per_epoch = train_data_size // batch_size\n",
    "num_train_steps = steps_per_epoch * epochs \n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "  init_lr=2e-5,\n",
    "  num_train_steps= num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  weight_decay_rate=0.02\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d818444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eded4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_keras.callbacks import TensorBoard, ModelCheckpoint, BackupAndRestore, RemoteMonitor\n",
    "tensorboard_callbacks = TensorBoard(\n",
    "  log_dir=\"./logs\", histogram_freq=1, write_graph=True,write_images=True,write_steps_per_second=True, update_freq=1\n",
    "   \n",
    ")\n",
    "model_callbacks = ModelCheckpoint(\n",
    "  filepath=\"./assets/checkpoint.keras\",\n",
    "  save_best_only=True,\n",
    "  monitor=\"val_accuracy\",\n",
    "  mode=\"max\"\n",
    ")\n",
    "backup_callbacks = BackupAndRestore(\n",
    "  backup_dir=\"./backup\",\n",
    "  save_freq=1,\n",
    "  save_before_preemption=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44f85d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 546/1813 [========>.....................] - ETA: 33:09 - loss: 0.6995 - accuracy: 0.5430"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43msentence_training_encoded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtraining_dataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentence_validation_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_dataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_callbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackup_callbacks\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./self_training_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./self_training_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1209\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1208\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/engine/training.py:1810\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1808\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1809\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1810\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/callbacks.py:478\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 478\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/callbacks.py:325\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    330\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/callbacks.py:348\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    351\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/callbacks.py:396\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    395\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 396\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/callbacks.py:1953\u001b[0m, in \u001b[0;36mBackupAndRestore.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batches_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_freq:\n\u001b[1;32m   1952\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batches_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1953\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_current_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/callbacks.py:1977\u001b[0m, in \u001b[0;36mBackupAndRestore._backup\u001b[0;34m(self, epoch, batch)\u001b[0m\n\u001b[1;32m   1976\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_backup\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch, batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 1977\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mback_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/distribute/worker_training_state.py:140\u001b[0m, in \u001b[0;36mWorkerTrainingState.back_up\u001b[0;34m(self, epoch, batch)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Back up the current state of training into a checkpoint file.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m  epoch: The current epoch information to be saved.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m  batch: The current batch(step) information to be saved.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Save the model plus CKPT_SAVED_EPOCH and CKPT_SAVED_BATCH variable.\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_checkpoint_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    141\u001b[0m     distributed_file_utils\u001b[38;5;241m.\u001b[39mremove_temp_dirpath(\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_checkpoint_manager\u001b[38;5;241m.\u001b[39mdirectory,\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    144\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint_management.py:851\u001b[0m, in \u001b[0;36mCheckpointManager.save\u001b[0;34m(self, checkpoint_number, check_interval, options)\u001b[0m\n\u001b[1;32m    848\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     options\u001b[38;5;241m.\u001b[39mexperimental_write_callbacks\u001b[38;5;241m.\u001b[39mappend(_record_and_sweep_state)\n\u001b[0;32m--> 851\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_path\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:2369\u001b[0m, in \u001b[0;36mCheckpoint._write\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m   2367\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2368\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;129;01mor\u001b[39;00m checkpoint_options\u001b[38;5;241m.\u001b[39mCheckpointOptions()\n\u001b[0;32m-> 2369\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2370\u001b[0m output \u001b[38;5;241m=\u001b[39m _convert_file_name_tensor_to_string(output)\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;66;03m# Execute callbacks (the only place they are executed; i.e. all entry points\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;66;03m# for callbacks will ultimately be directed to here for execution)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:1356\u001b[0m, in \u001b[0;36mTrackableSaver.save\u001b[0;34m(self, file_prefix, checkpoint_number, session, options)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor_util\u001b[38;5;241m.\u001b[39mis_tensor(file_prefix):\n\u001b[1;32m   1354\u001b[0m   file_io\u001b[38;5;241m.\u001b[39mrecursive_create_dir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(file_prefix))\n\u001b[0;32m-> 1356\u001b[0m save_path, new_feed_additions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_cached_when_graph_building\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_graph_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_feed_additions:\n\u001b[1;32m   1360\u001b[0m   feed_dict\u001b[38;5;241m.\u001b[39mupdate(new_feed_additions)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:1294\u001b[0m, in \u001b[0;36mTrackableSaver._save_cached_when_graph_building\u001b[0;34m(self, file_prefix, object_graph_tensor, options)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_save_object_graph \u001b[38;5;241m!=\u001b[39m graph_proto\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;66;03m# When executing eagerly, we need to re-create SaveableObjects each\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;66;03m# time save() is called so they pick up new Tensors passed to their\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# constructors. That means the Saver needs to be copied with a new\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# var_list.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function()):\n\u001b[1;32m   1292\u001b[0m   saver \u001b[38;5;241m=\u001b[39m functional_saver\u001b[38;5;241m.\u001b[39mMultiDeviceSaver(serialized_tensors,\n\u001b[1;32m   1293\u001b[0m                                             registered_savers)\n\u001b[0;32m-> 1294\u001b[0m   save_op \u001b[38;5;241m=\u001b[39m \u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([save_op]):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tensorflow/python/checkpoint/functional_saver.py:557\u001b[0m, in \u001b[0;36mMultiDeviceSaver.save\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    555\u001b[0m   tf_function_save()\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msave_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tensorflow/python/checkpoint/functional_saver.py:530\u001b[0m, in \u001b[0;36mMultiDeviceSaver.save.<locals>.save_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    527\u001b[0m       shard_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    528\u001b[0m     saved_prefixes\u001b[38;5;241m.\u001b[39mappend(shard_prefix)\n\u001b[1;32m    529\u001b[0m     sharded_saves\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 530\u001b[0m         \u001b[43m_single_shard_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies(sharded_saves):\n\u001b[1;32m    533\u001b[0m   \u001b[38;5;66;03m# Merge on the io_device if specified, otherwise co-locates the merge op\u001b[39;00m\n\u001b[1;32m    534\u001b[0m   \u001b[38;5;66;03m# with the last device used.\u001b[39;00m\n\u001b[1;32m    535\u001b[0m   tensor_device_spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shardable_tensors_by_task\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tensorflow/python/checkpoint/functional_saver.py:102\u001b[0m, in \u001b[0;36m_single_shard_save\u001b[0;34m(file_prefix, shard, task, options)\u001b[0m\n\u001b[1;32m    100\u001b[0m save_device \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mexperimental_io_device \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(tensors) \u001b[38;5;129;01mand\u001b[39;00m task)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(save_device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 102\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_specs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tensorflow/python/ops/gen_io_ops.py:1728\u001b[0m, in \u001b[0;36msave_v2\u001b[0;34m(prefix, tensor_names, shape_and_slices, tensors, name)\u001b[0m\n\u001b[1;32m   1726\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1728\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msave_v2_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m      \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_and_slices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   1732\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tensorflow/python/ops/gen_io_ops.py:1749\u001b[0m, in \u001b[0;36msave_v2_eager_fallback\u001b[0;34m(prefix, tensor_names, shape_and_slices, tensors, name, ctx)\u001b[0m\n\u001b[1;32m   1747\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [prefix, tensor_names, shape_and_slices] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(tensors)\n\u001b[1;32m   1748\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_dtypes)\n\u001b[0;32m-> 1749\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSaveV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  sentence_training_encoded,\n",
    "  training_dataframe[\"label\"],\n",
    "  validation_data=(\n",
    "    sentence_validation_encoded,validation_dataframe[\"label\"]\n",
    "  ),\n",
    "  batch_size=32,\n",
    "  epochs=5,\n",
    "  shuffle=True,\n",
    "  callbacks=[tensorboard_callbacks, model_callbacks, backup_callbacks]\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"./self_training_model\")\n",
    "tokenizer.save_pretrained(\"./self_training_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
