{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5e024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/sst2\", cache_dir=\"./datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6745c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET = ds[\"train\"]\n",
    "VALIDATION_DATASET = ds[\"validation\"]\n",
    "TEST_DATASET = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c39a3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 1, 'sentence': 'contains no wit , only labored gags ', 'label': 0}\n",
      "{'idx': 1, 'sentence': 'unflinchingly bleak and desperate ', 'label': 0}\n",
      "{'idx': 1, 'sentence': \"this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation .\", 'label': -1}\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "\n",
    "print(TRAINING_DATASET[idx])\n",
    "print(VALIDATION_DATASET[idx])\n",
    "print(TEST_DATASET[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfbcaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_sentence = TRAINING_DATASET[\"sentence\"]\n",
    "training_labels = TRAINING_DATASET[\"label\"]\n",
    "\n",
    "validation_sentence = VALIDATION_DATASET[\"sentence\"]\n",
    "validation_labels = VALIDATION_DATASET[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e35c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label\n",
      "0  lies in the ease with which it integrates thou...      1\n",
      "1       is funny , touching , smart and complicated       1\n",
      "2  juliette binoche 's sand is vivacious , but it...      1\n",
      "3         seen it all before in one form or another       0\n",
      "4                                           literal       1\n",
      "                                            sentence  label\n",
      "0                                   the best script       1\n",
      "1                                      vividly back       1\n",
      "2               than most third-rate horror sequels       0\n",
      "3  , byler reveals the ways in which a sultry eve...      1\n",
      "4                    is funny , charming and quirky       1\n",
      "                                            sentence  label\n",
      "0             uneasy mishmash of styles and genres .     -1\n",
      "1  this film 's relationship to actual tension is...     -1\n",
      "2  by the end of no such thing the audience , lik...     -1\n",
      "3  director rob marshall went out gunning to make...     -1\n",
      "4  lathan and diggs have considerable personal ch...     -1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "sentence_concat = training_sentence + validation_sentence\n",
    "label_concat = training_labels + validation_labels\n",
    "\n",
    "sentence_training, sentence_validation, label_training, label_validation = train_test_split(sentence_concat, label_concat, test_size=0.25, random_state=42)\n",
    "\n",
    "training_dataframe = pd.DataFrame.from_dict({\"sentence\": sentence_training, \"label\": label_training})\n",
    "validation_dataframe = pd.DataFrame.from_dict({\"sentence\": sentence_validation, \"label\": label_validation})\n",
    "test_dataframe = pd.DataFrame.from_dict({\"sentence\": TEST_DATASET[\"sentence\"], \"label\": TEST_DATASET[\"label\"]})\n",
    "\n",
    "print(training_dataframe.head())\n",
    "print(validation_dataframe.head())\n",
    "print(test_dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d4813bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "  text = re.sub(r' , ', \", \", text)\n",
    "  text = re.sub(r' . ', \". \", text)\n",
    "  text = re.sub(r'\\. $', \".\", text)\n",
    "  text = re.sub(r' $', \"\", text)\n",
    "  text = re.sub(r\" 's\", \"'s\", text)\n",
    "  text = re.sub(r\" n't\", \"n't\", text)\n",
    "  text = re.sub(r\" 'll\", \"'ll\", text)\n",
    "  text = re.sub(r'^,\\s*', '', text)\n",
    "  text = re.sub(r\"^'s. \", '', text)\n",
    "  text = re.sub(r\"('' || ``)\", '', text)\n",
    "  text = re.sub(r'(?<=\\w)\\.\\s(?=\\w)', ' ', text)\n",
    "  text = re.sub(r',\\.', ' ', text)\n",
    "  text = re.sub(r'\\s*--\\s*', ' ', text)\n",
    "  text = re.sub(r',$', '', text)\n",
    "  text = text.strip()\n",
    "  return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fbf225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataframe[\"cleaned\"] = training_dataframe[\"sentence\"].apply(clean_text).to_list()\n",
    "validation_dataframe[\"cleaned\"] = validation_dataframe[\"sentence\"].apply(clean_text).to_list()\n",
    "test_dataframe[\"cleaned\"] = test_dataframe[\"sentence\"].apply(clean_text).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca82a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f1aad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_training_encoded = tokenizer.batch_encode_plus(training_dataframe[\"cleaned\"].tolist(),\n",
    "                                                        padding=True,\n",
    "                                                        truncation=True,\n",
    "                                                        return_tensors=\"tf\"\n",
    "                                                        )\n",
    "sentence_validation_encoded = tokenizer.batch_encode_plus(validation_dataframe[\"cleaned\"].tolist(),\n",
    "                                                          padding=True,\n",
    "                                                          truncation=True,\n",
    "                                                          return_tensors=\"tf\"\n",
    "                                                          )\n",
    "sentence_test_encoded = tokenizer.batch_encode_plus(test_dataframe[\"cleaned\"].tolist(),\n",
    "                                                    padding=True,\n",
    "                                                    truncation=True,\n",
    "                                                    return_tensors=\"tf\"\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb672106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lies in the ease with which it integrates thoughtfulness and pasta-fagioli comedy.\n",
      "tf.Tensor(\n",
      "[  101  3658  1999  1996  7496  2007  2029  2009 17409  2015 16465  2791\n",
      "  1998 24857  1011  6904 11411  3669  4038  1012   102     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0], shape=(61,), dtype=int32)\n",
      "[CLS] lies in the ease with which it integrates thoughtfulness and pasta - fagioli comedy. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(61,), dtype=int32)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "print(training_dataframe[\"cleaned\"][k])\n",
    "print(sentence_training_encoded[\"input_ids\"][k])\n",
    "print(tokenizer.decode(sentence_training_encoded[\"input_ids\"][k]))\n",
    "print(sentence_training_encoded[\"attention_mask\"][k])\n",
    "print(training_dataframe[\"label\"][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "145b72db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d818444",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x3c7f8fcd0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msparse_categorical_crossentropy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1543\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[0;32m-> 1543\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1555\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m   1556\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1563\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/optimizers/__init__.py:335\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get(\n\u001b[1;32m    331\u001b[0m         config,\n\u001b[1;32m    332\u001b[0m         use_legacy_optimizer\u001b[38;5;241m=\u001b[39muse_legacy_optimizer,\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x3c7f8fcd0>"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44f85d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1691, in train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/metrics/base_metric.py\", line 722, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43m[\u001b[49m\u001b[43msentence_training_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentence_training_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_type_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentence_training_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtraining_dataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msentence_validation_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentence_validation_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_type_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentence_validation_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_dataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./self_training_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./self_training_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1209\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1208\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/rt/3260sfx57rj4s3lg0trkqvgw0000gn/T/__autograph_generated_filejjg7mquh.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1691\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables, tape\u001b[38;5;241m=\u001b[39mtape)\n\u001b[0;32m-> 1691\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;66;03m# Collect metrics to return\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m return_metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1691, in train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/tf_keras/src/metrics/base_metric.py\", line 722, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  [sentence_training_encoded[\"input_ids\"],sentence_training_encoded[\"token_type_ids\"],sentence_training_encoded[\"attention_mask\"]],\n",
    "  training_dataframe[\"label\"],\n",
    "  validation_data=(\n",
    "    [sentence_validation_encoded[\"input_ids\"],sentence_validation_encoded[\"token_type_ids\"],sentence_validation_encoded[\"attention_mask\"]],validation_dataframe[\"label\"]\n",
    "  ),\n",
    "  batch_size=32,\n",
    "  epochs=3,\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"./self_training_model\")\n",
    "tokenizer.save_pretrained(\"./self_training_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b53dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
